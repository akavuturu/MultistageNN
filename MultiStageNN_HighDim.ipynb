{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "from scipy.io import savemat\n",
    "from scipy.fft import fftfreq, fftshift, fftn\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    # Initialize the class\n",
    "    def __init__(self, t_u, x_u, layers, kappa, lt, ut, acts=0):\n",
    "\n",
    "        self.scale = tf.reduce_max(tf.abs(x_u)) / 2\n",
    "        x_u2 = x_u / self.scale\n",
    "        actv = [tf.tanh, tf.sin]\n",
    "\n",
    "        self.t_u = t_u\n",
    "        self.x_u = x_u2\n",
    "        self.datatype = t_u.dtype\n",
    "\n",
    "        self.lt = lt\n",
    "        self.ut = ut\n",
    "\n",
    "        self.layers = layers\n",
    "        self.kappa = kappa\n",
    "\n",
    "        # determine the activation function to use\n",
    "        self.actv = actv[acts]\n",
    "\n",
    "        # Initialize NNs\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # Create a list including all training variables\n",
    "        self.train_variables = self.weights + self.biases\n",
    "        # Key point: anything updates in train_variables will be\n",
    "        #            automatically updated in the original tf.Variable\n",
    "\n",
    "        # define the loss function\n",
    "        self.loss0 = self.scale ** 2\n",
    "        self.loss = []\n",
    "        self.loss_0 = self.loss_NN()\n",
    "\n",
    "        self.optimizer_Adam = tf.optimizers.Adam()\n",
    "\n",
    "    '''\n",
    "    Functions used to establish the initial neural network\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.MPL_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=self.datatype))\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    def MPL_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev, dtype=self.datatype))\n",
    "\n",
    "    def get_params(self):\n",
    "        return (self.weights, self.biases)\n",
    "\n",
    "    '''\n",
    "    Functions used to building the physics-informed contrainst and loss\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def neural_net(self, X):\n",
    "        weights = self.weights\n",
    "        biases = self.biases\n",
    "\n",
    "        num_layers = len(weights) + 1\n",
    "\n",
    "        H = 2.0 * tf.math.divide(\n",
    "                    tf.math.subtract(X, tf.transpose(self.lt)), \n",
    "                    tf.transpose(tf.math.subtract(self.ut, self.lt))) \\\n",
    "            - 1.0\n",
    "\n",
    "        W = weights[0]\n",
    "        b = biases[0]\n",
    "        H = self.actv(tf.add(self.kappa * tf.matmul(H, W), b))\n",
    "\n",
    "        for l in range(1, num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    # calculate the physics-informed loss function\n",
    "    def loss_NN(self):\n",
    "        self.x_pred = self.neural_net(self.t_u)\n",
    "        loss = tf.reduce_mean(tf.square(self.x_u - self.x_pred))\n",
    "        return loss\n",
    "\n",
    "    '''\n",
    "    Functions used to define ADAM optimizers\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    # define the function to apply the ADAM optimizer\n",
    "    def adam_function(self):\n",
    "        @tf.function(reduce_retracing=True)\n",
    "        def f1():\n",
    "            # calculate the loss\n",
    "            loss_norm = self.loss_NN()\n",
    "            loss_value = loss_norm * self.loss0\n",
    "            # store loss value so we can retrieve later\n",
    "            tf.py_function(f1.loss.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "            # print out iteration & loss\n",
    "            f1.iter.assign_add(1)\n",
    "\n",
    "            str_iter = tf.strings.as_string([f1.iter])\n",
    "            str_loss = tf.strings.as_string([loss_value], precision=4, scientific=True)\n",
    "\n",
    "            str_print = tf.strings.join([\"Mode: Adam\", \"Iter: \", str_iter[0],\n",
    "                                         \", loss: \", str_loss[0]])\n",
    "            tf.cond(\n",
    "                f1.iter % 10 == 0,\n",
    "                lambda: tf.print(str_print),\n",
    "                lambda: tf.constant(True)  # return arbitrary for non-printing case\n",
    "            )\n",
    "            return loss_norm\n",
    "\n",
    "        f1.iter = tf.Variable(0)\n",
    "        f1.term = []\n",
    "        f1.loss = []\n",
    "        return f1\n",
    "\n",
    "    def Adam_optimizer(self, nIter):\n",
    "        varlist = self.train_variables\n",
    "        func_adam = self.adam_function()\n",
    "        for it in range(nIter):\n",
    "            tf.keras.optimizers.Adam(func_adam, varlist)\n",
    "            #self.optimizer_Adam.minimize(func_adam, varlist)\n",
    "        return func_adam\n",
    "\n",
    "    '''\n",
    "    Functions used to define L-BFGS optimizers\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    # A factory to create a function required by tfp.optimizer.lbfgs_minimize.\n",
    "    def Lbfgs_function(self, varlist):\n",
    "        # obtain the shapes of all trainable parameters in the model\n",
    "        shapes = tf.shape_n(varlist)\n",
    "        n_tensors = len(shapes)\n",
    "\n",
    "        # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n",
    "        # prepare required information first\n",
    "        count = 0\n",
    "        idx = []  # stitch indices\n",
    "        part = []  # partition indices\n",
    "\n",
    "        for i, shape in enumerate(shapes):\n",
    "            n = np.prod(shape)\n",
    "            idx.append(tf.reshape(tf.range(count, count + n, dtype=tf.int32), shape))\n",
    "            part.extend([i] * n)\n",
    "            count += n\n",
    "\n",
    "        part = tf.constant(part)\n",
    "\n",
    "        def assign_new_model_parameters(params_1d):\n",
    "            # A function updating the model's parameters with a 1D tf.Tensor.\n",
    "            # Sub-function under function of class not need to input self\n",
    "\n",
    "            params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "            for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "                varlist[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "        @tf.function(reduce_retracing=True)\n",
    "        def f2(params_1d):\n",
    "            # A function that can be used by tfp.optimizer.lbfgs_minimize.\n",
    "            # This function is created by function_factory.\n",
    "            # Sub-function under function of class not need to input self\n",
    "\n",
    "            # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters\n",
    "            with tf.GradientTape() as tape:\n",
    "                # update the parameters in the model\n",
    "                # this step is critical for self-defined function for L-BFGS\n",
    "                assign_new_model_parameters(params_1d)\n",
    "                # calculate the loss\n",
    "                loss_norm = self.loss_NN()\n",
    "                loss_value = loss_norm * self.loss0\n",
    "\n",
    "            # calculate gradients and convert to 1D tf.Tensor\n",
    "            grads = tape.gradient(loss_norm, varlist)\n",
    "            grads = tf.dynamic_stitch(idx, grads)\n",
    "\n",
    "            # store loss value so we can retrieve later\n",
    "            tf.py_function(f2.loss.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "            # print out iteration & loss\n",
    "            f2.iter.assign_add(1)\n",
    "\n",
    "            str_iter = tf.strings.as_string([f2.iter])\n",
    "            str_loss = tf.strings.as_string([loss_value], precision=4, scientific=True)\n",
    "\n",
    "            str_print = tf.strings.join([\"Mode: LBFGS\", \"Iter: \", str_iter[0],\n",
    "                                         \", loss: \", str_loss[0]])\n",
    "            tf.cond(\n",
    "                f2.iter % 3000 == 0,\n",
    "                lambda: tf.print(str_print),\n",
    "                lambda: tf.constant(True)  # return arbitrary for non-printing case\n",
    "            )\n",
    "\n",
    "            return loss_value, grads\n",
    "\n",
    "        # store these information as members so we can use them outside the scope\n",
    "        f2.iter = tf.Variable(0)\n",
    "        f2.idx = idx\n",
    "        f2.part = part\n",
    "        f2.shapes = shapes\n",
    "        f2.assign_new_model_parameters = assign_new_model_parameters\n",
    "        f2.loss = []\n",
    "\n",
    "        return f2\n",
    "\n",
    "    # define the function to apply the L-BFGS optimizer\n",
    "    def Lbfgs_optimizer(self, nIter, varlist):\n",
    "\n",
    "        func = self.Lbfgs_function(varlist)\n",
    "\n",
    "        # convert initial model parameters to a 1D tf.Tensor\n",
    "        init_params = tf.dynamic_stitch(func.idx, varlist)\n",
    "\n",
    "        max_nIter = tf.cast(nIter / 3, dtype=tf.int32)\n",
    "\n",
    "        # train the model with L-BFGS solver\n",
    "        results = tfp.optimizer.lbfgs_minimize(\n",
    "            value_and_gradients_function=func, initial_position=init_params,\n",
    "            tolerance=1e-11, max_iterations=max_nIter)\n",
    "\n",
    "        # after training, the final optimized parameters are still in results.position\n",
    "        # so we have to manually put them back to the model\n",
    "        func.assign_new_model_parameters(results.position)\n",
    "\n",
    "        return func\n",
    "\n",
    "    '''\n",
    "    Function used for training the model\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def train(self, nIter, idxOpt):\n",
    "        if idxOpt == 1:\n",
    "            # mode 1: running the Adam optimization\n",
    "            func_adam = self.Adam_optimizer(nIter)\n",
    "            self.loss += func_adam.loss\n",
    "        elif idxOpt == 2:\n",
    "            # mode 2: running the Lbfgs optimization\n",
    "            func_bfgs = self.Lbfgs_optimizer(nIter, self.train_variables)\n",
    "            self.loss += func_bfgs.loss\n",
    "\n",
    "    # @tf.function\n",
    "    def predict(self, t):\n",
    "        x_p = self.neural_net(t) * self.scale\n",
    "        return x_p\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultistageNeuralNetwork:\n",
    "    '''MultistageNeuralNetwork is the multi-stage model used for predicting\n",
    "    high-dimensional function outputs through regression. The network takes as parameters the input dataset,\n",
    "    an n-dimensional square tensor of grid points, a list of labels corresponding to function outputs at each grid point,\n",
    "    a number of stages for the MSNN, a number of hidden layers for each stage, and a number of hidden nodes in each\n",
    "    hidden layer of each stage of the MSNN'''\n",
    "    def __init__(self, x_train, num_stages, num_hidden_layers, num_hidden_nodes):\n",
    "        # MSNN setup\n",
    "        self.dim = x_train.shape[-1]                                 # number of dimensions\n",
    "        self.N = int(round(x_train.shape[0] ** (1/self.dim)))        # number of points per dimension\n",
    "        self.stages = [None] * num_stages                           # Number of stages for MSNN\n",
    "        self.layers = [self.dim] + ([num_hidden_nodes] * num_hidden_layers) + [1] # NN architecture for each stage \n",
    "        self.lt = [tf.math.reduce_min(x_train[:, i]) for i in range(x_train.shape[-1])] # min of each dim\n",
    "        self.ut = [tf.math.reduce_max(x_train[:, i]) for i in range(x_train.shape[-1])] # max of each dim\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def sample(x_train, y_train, step):\n",
    "    #     '''sample returns a subset of evenly-spaced data points from dataset/labels by converting\n",
    "    #     both into a grid format and taking slices of the grid at specified intervals'''\n",
    "    #     dim = x_train.shape[-1]\n",
    "    #     N = int(round(x_train.shape[0] ** (1/dim)))  \n",
    "    #     reshaped_x = tf.reshape(x_train, [N] * dim + [dim])\n",
    "    #     reshaped_y = tf.reshape(y_train, [N] * dim)\n",
    "\n",
    "    #     slices = tuple(slice(0, N, step) for _ in range(dim))\n",
    "\n",
    "    #     sampled_grid = reshaped_x[slices + (slice(None),)]\n",
    "    #     sampled_y = reshaped_y[slices]\n",
    "\n",
    "    #     sampled_points = tf.reshape(sampled_grid, (-1, dim))\n",
    "    #     sampled_y = tf.reshape(sampled_y, (-1, 1))\n",
    "    #     print(f\"Sampled data size: {sampled_points.shape}, Sampled labels size: {sampled_y.shape}\")\n",
    "    #     return sampled_points, sampled_y\n",
    "    \n",
    "\n",
    "    def train(self, x_train, y_train, stage, kappa, iters):\n",
    "        act = 0 if stage == 0 else 1\n",
    "        lt = [tf.cast(tf.math.reduce_min(x_train[:, i]).numpy(), dtype=tf.float64) for i in range(x_train.shape[-1])]\n",
    "        ut = [tf.cast(tf.math.reduce_max(x_train[:, i]).numpy(), dtype=tf.float64) for i in range(x_train.shape[-1])]\n",
    "        self.stages[stage] = NeuralNet(x_train, y_train, self.layers, kappa=kappa, lt=lt, ut=ut, acts=act)\n",
    "        print(f\"Adam Training for stage: {stage}\")\n",
    "        self.stages[stage].train(iters[0], 1)     # mode 1 use Adam\n",
    "        print(f\"L-BFGS Training for stage: {stage}\")\n",
    "        self.stages[stage].train(iters[1], 2)    # mode 2 use L-bfgs\n",
    "    \n",
    "    @staticmethod\n",
    "    def fftn_(x_train, residue):\n",
    "        dim = x_train.shape[-1]\n",
    "        N_train = int(round(x_train.shape[0] ** (1/dim))) \n",
    "        g = residue.numpy()\n",
    "\n",
    "        GG = g.reshape([N_train] * dim)\n",
    "        G = fftn(GG)\n",
    "        G_shifted = fftshift(G)\n",
    "\n",
    "        N = len(G)\n",
    "        # Total time range\n",
    "        total_time_range = 2  # from -1 to 1\n",
    "\n",
    "        # Calculate the sample rate\n",
    "        sample_rate = N / total_time_range\n",
    "\n",
    "        # # Perform FFT\n",
    "        half_N = N // 2\n",
    "        T = 1.0 / sample_rate\n",
    "        idxs = tuple(slice(half_N, N, 1) for _ in range(dim))\n",
    "        G_pos = G_shifted[idxs]\n",
    "        freqs = [fftshift(fftfreq(GG.shape[i], d=T)) for i in range(len(GG.shape))]\n",
    "\n",
    "        # freq_x_pos = freq_x[half_N:]\n",
    "        # freq_y_pos = freq_y[half_N:]\n",
    "        freq_pos = [freqs[i][half_N:] for i in range(len(freqs))]\n",
    "\n",
    "        # Identify the dominant frequency\n",
    "        magnitude_spectrum = np.abs(G_pos)\n",
    "        max_idx = np.unravel_index(np.argmax(magnitude_spectrum), magnitude_spectrum.shape)\n",
    "        dominant_freqs = [freq_pos[i][max_idx[i]] for i in range(len(freq_pos))]\n",
    "        magnitude = magnitude_spectrum[max_idx] / (N ** dim) # normalize magnitude\n",
    "        dominant_freq = max(dominant_freqs)\n",
    "        print(f\"Sample rate = {sample_rate} Hz, Dominant Frequency = {dominant_freq} Hz, Magnitude = {magnitude}\")\n",
    "        kappa_f =  2 * np.pi * dominant_freq if dominant_freq > 0 else 2 * np.pi\n",
    "        print(f\"New Kappa: {kappa_f}\")\n",
    "        return kappa_f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for creating data\n",
    "def create_ds(dims, lo, hi, N):\n",
    "    dimensions = [np.linspace(lo, hi, N)] * dims\n",
    "    mesh = np.meshgrid(*dimensions, indexing='xy')\n",
    "    mesh_points = np.stack([m.flatten() for m in mesh], axis=-1)\n",
    "    mesh_tf = tf.cast(mesh_points, dtype=tf.float64)\n",
    "    return mesh_tf\n",
    "\n",
    "def example_1d_fun(x_train):\n",
    "    y_train = x_train ** 3 / (0.01 + x_train ** 4)\n",
    "    return y_train\n",
    "\n",
    "def example_2d_fun(x_train1, x_train2):\n",
    "    y_train = (np.sin(x_train1 + 1) - 0.5 * np.sin(x_train1))*(1 - x_train2 ** 2)\n",
    "    return y_train\n",
    "\n",
    "def poisson(x_train):\n",
    "    N_f = x_train.shape[0]\n",
    "    dim = x_train.shape[-1]\n",
    "    coeffs = 1 # used to be random from 1 to dim - 1\n",
    "    const_2 = 1\n",
    "    x_radius = 1\n",
    "\n",
    "    xf = x_train.numpy() # used to be random points of size (N_f, args.dim) normalized by sqrt(sum(squares))\n",
    "    x = xf\n",
    "\n",
    "    u1 = x_radius**2 - np.sum(x**2, 1, keepdims=True)\n",
    "    du1_dx = -2 * x\n",
    "    d2u1_dx2 = -2\n",
    "\n",
    "    x1, x2 = x[:, :-1], x[:, 1:]\n",
    "    u2 = coeffs * np.sin(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1))\n",
    "    u2 = np.sum(u2, 1, keepdims=True)\n",
    "    du2_dx_part1 = coeffs * np.cos(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * \\\n",
    "            (1 - x2 * np.sin(x1))\n",
    "    du2_dx_part2 = coeffs * np.cos(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * \\\n",
    "            (-const_2 * np.sin(x2) + np.cos(x1))\n",
    "    du2_dx = np.zeros((N_f, dim))\n",
    "    du2_dx[:, :-1] += du2_dx_part1\n",
    "    du2_dx[:, 1:] += du2_dx_part2\n",
    "    d2u2_dx2_part1 = -coeffs * np.sin(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * (1 - x2 * np.sin(x1))**2 + \\\n",
    "            coeffs * np.cos(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * (- x2 * np.cos(x1))\n",
    "    d2u2_dx2_part2 = -coeffs * np.sin(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * (-const_2 * np.sin(x2) + np.cos(x1))**2 + \\\n",
    "            coeffs * np.cos(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * (-const_2 * np.cos(x2))\n",
    "    d2u2_dx2 = np.zeros((N_f, dim))\n",
    "    d2u2_dx2[:, :-1] += d2u2_dx2_part1\n",
    "    d2u2_dx2[:, 1:] += d2u2_dx2_part2\n",
    "    ff = u1 * d2u2_dx2 + 2 * du1_dx * du2_dx + u2 * d2u1_dx2\n",
    "    ff = np.sum(ff, 1)\n",
    "    u = (u1 * u2).reshape(-1)\n",
    "    ff = ff + np.sin(u)\n",
    "\n",
    "    ff = tf.convert_to_tensor(ff, dtype=tf.float64)  \n",
    "    return ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1501, 1) (1501, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create train and eval data\n",
    "dim = 1\n",
    "N_train = 1501\n",
    "N_eval = 8000\n",
    "x_train = create_ds(dim, -1.02, 1.02, N_train)\n",
    "y_train = tf.reshape(example_1d_fun(x_train), [len(x_train), 1])\n",
    "x_eval = create_ds(dim, -1.02, 1.02, N_eval)\n",
    "y_eval = tf.reshape(example_1d_fun(x_eval), [len(x_eval), 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: (1501, 1), Label size: (1501, 1)\n",
      "******** TRAINING STAGE 0 ********\n",
      "Adam Training for stage: 0\n",
      "L-BFGS Training for stage: 0\n",
      "Mode: LBFGSIter: 3000, loss: 3.8679e-05\n",
      "Mode: LBFGSIter: 6000, loss: 6.6322e-07\n",
      "Mode: LBFGSIter: 9000, loss: 7.3449e-08\n",
      "******** TRAINING STAGE 1 ********\n",
      "Sample rate = 750.5 Hz, Dominant Frequency = 3.0 Hz, Magnitude = 9.512689569010628e-05\n",
      "New Kappa: 18.84955592153876\n",
      "Adam Training for stage: 1\n",
      "L-BFGS Training for stage: 1\n",
      "Mode: LBFGSIter: 3000, loss: 3.3502e-12\n",
      "Mode: LBFGSIter: 6000, loss: 1.0280e-12\n",
      "Mode: LBFGSIter: 9000, loss: 3.0672e-13\n",
      "Mode: LBFGSIter: 12000, loss: 2.2444e-13\n",
      "Mode: LBFGSIter: 15000, loss: 1.5789e-13\n",
      "Mode: LBFGSIter: 18000, loss: 1.1918e-13\n",
      "residue tf.Tensor(\n",
      "[[ 0.0007482 ]\n",
      " [ 0.00071311]\n",
      " [ 0.00067873]\n",
      " ...\n",
      " [-0.00057738]\n",
      " [-0.00060793]\n",
      " [-0.0006391 ]], shape=(1501, 1), dtype=float64)\n",
      "******** TRAINING STAGE 2 ********\n",
      "Sample rate = 2400.5 Hz, Dominant Frequency = 15.0 Hz, Magnitude = 8.083295360445193e-08\n",
      "New Kappa: 94.24777960769379\n",
      "Adam Training for stage: 2\n",
      "L-BFGS Training for stage: 2\n",
      "Mode: LBFGSIter: 3000, loss: 8.3328e-15\n",
      "Mode: LBFGSIter: 6000, loss: 2.9204e-15\n",
      "Mode: LBFGSIter: 9000, loss: 9.2797e-16\n",
      "Mode: LBFGSIter: 12000, loss: 4.7874e-16\n",
      "Mode: LBFGSIter: 15000, loss: 2.1649e-16\n",
      "Mode: LBFGSIter: 18000, loss: 1.4426e-16\n",
      "Mode: LBFGSIter: 21000, loss: 9.7258e-17\n",
      "Mode: LBFGSIter: 24000, loss: 7.5922e-17\n",
      "Mode: LBFGSIter: 27000, loss: 5.1688e-17\n",
      "residue tf.Tensor(\n",
      "[[ 8.56248112e-07]\n",
      " [ 7.19294811e-07]\n",
      " [ 5.92808307e-07]\n",
      " ...\n",
      " [-5.47540649e-07]\n",
      " [-6.82120595e-07]\n",
      " [-8.28642689e-07]], shape=(4801, 1), dtype=float64)\n",
      "******** TRAINING STAGE 3 ********\n",
      "Sample rate = 2400.5 Hz, Dominant Frequency = 36.0 Hz, Magnitude = 9.145593728020321e-10\n",
      "New Kappa: 226.1946710584651\n",
      "Adam Training for stage: 3\n",
      "L-BFGS Training for stage: 3\n",
      "Mode: LBFGSIter: 3000, loss: 5.8872e-18\n",
      "Mode: LBFGSIter: 6000, loss: 2.1068e-18\n",
      "Mode: LBFGSIter: 9000, loss: 1.0789e-18\n",
      "Mode: LBFGSIter: 12000, loss: 7.2664e-19\n",
      "Mode: LBFGSIter: 15000, loss: 5.1076e-19\n",
      "Mode: LBFGSIter: 18000, loss: 3.6745e-19\n",
      "Mode: LBFGSIter: 21000, loss: 2.6947e-19\n",
      "Mode: LBFGSIter: 24000, loss: 2.1193e-19\n",
      "Mode: LBFGSIter: 27000, loss: 1.7678e-19\n",
      "Mode: LBFGSIter: 30000, loss: 1.5286e-19\n",
      "Mode: LBFGSIter: 33000, loss: 1.3369e-19\n",
      "Mode: LBFGSIter: 36000, loss: 1.1966e-19\n",
      "Mode: LBFGSIter: 39000, loss: 1.0559e-19\n",
      "residue tf.Tensor(\n",
      "[[ 9.26579632e-08]\n",
      " [ 5.50754909e-08]\n",
      " [ 2.55031666e-08]\n",
      " ...\n",
      " [-2.63662654e-08]\n",
      " [-5.83012981e-08]\n",
      " [-9.95146764e-08]], shape=(4801, 1), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# Compute Prediction for one dataset (dim = 2)\n",
    "num_stages = 4\n",
    "num_hidden_layers = 3\n",
    "num_hidden_nodes = 20\n",
    "# sample_sizes = list([1, 3, 6, 6, 24, 24, 120, 120][:num_stages])[::-1] \n",
    "sample_sizes = [1501, 1501, 4801, 4801] # sampling schedule - change based on dimensionality of problem\n",
    "# Find indices where the value changes\n",
    "change_indices = [i for i in range(1, len(sample_sizes)) if sample_sizes[i] != sample_sizes[i - 1]]\n",
    "change_indices = [0] + change_indices + [len(sample_sizes)]\n",
    "change_indices = change_indices[1:-1]\n",
    "training_iters = list([(3000, 10000)] + [(5000, 10000*i) for i in range(2, 10)])[:num_stages]\n",
    "\n",
    "print(f\"Data size: {x_train.shape}, Label size: {y_train.shape}\")\n",
    "\n",
    "MSNN = MultistageNeuralNetwork(x_train, num_stages, num_hidden_layers, num_hidden_nodes)\n",
    "\n",
    "print(f\"******** TRAINING STAGE {0} ********\")\n",
    "MSNN.train(x_train, y_train, stage=0, kappa=1, iters=training_iters[0])\n",
    "\n",
    "for i in range(1, num_stages):\n",
    "    print(f\"******** TRAINING STAGE {i} ********\")\n",
    "    if i in change_indices: \n",
    "        x_train = create_ds(dim, -1.02, 1.02, sample_sizes[i])\n",
    "        y_train = tf.reshape(example_1d_fun(x_train), [len(x_train), 1])        \n",
    "\n",
    "    curr_residue = y_train - tf.add_n([MSNN.stages[j].predict(x_train) for j in range(i)])\n",
    "        \n",
    "    kappa = MultistageNeuralNetwork.fftn_(x_train, curr_residue)\n",
    "    MSNN.train(x_train, curr_residue, stage=i, kappa=kappa, iters=training_iters[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1, model2, model3, model4 = MSNN.stages[0], MSNN.stages[1], MSNN.stages[2], MSNN.stages[3]\n",
    "loss = np.array(model1.loss + model2.loss + model3.loss + model4.loss)\n",
    "plt.figure()\n",
    "plt.plot(loss, 'b-')\n",
    "plt.yscale(\"log\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
