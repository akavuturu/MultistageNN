{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akavu\\AppData\\Local\\Temp\\ipykernel_8328\\940848082.py:7: DeprecationWarning: The host_callback APIs are deprecated as of March 20, 2024. The functionality is subsumed by the new JAX external callbacks. See https://github.com/google/jax/issues/20385.\n",
      "  from jax.experimental.host_callback import call\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from jax import random, jit, vjp, grad, vmap\n",
    "import jax.flatten_util as flat_utl\n",
    "from jax.experimental.host_callback import call\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "from pyDOE import lhs\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the neural network weights and biases\n",
    "# NN is a list of tuples containing weight/bias list pairs for each layer of the NN\n",
    "def init_MLP(parent_key, layer_widths):\n",
    "    params = []\n",
    "    keys = random.split(parent_key, num=len(layer_widths) - 1)\n",
    "    # create the weights and biases for the network\n",
    "    for in_dim, out_dim, key in zip(layer_widths[:-1], layer_widths[1:], keys):\n",
    "        weight_key, bias_key = random.split(key)\n",
    "        xavier_stddev = jnp.sqrt(2 / (in_dim + out_dim))\n",
    "        params.append(\n",
    "            [random.truncated_normal(weight_key, -2, 2, shape=(in_dim, out_dim)) * xavier_stddev,\n",
    "             random.truncated_normal(bias_key, -2, 2, shape=(out_dim,)) * xavier_stddev]\n",
    "        )\n",
    "    return params\n",
    "\n",
    "\n",
    "# define the basic formation of neural network\n",
    "def neural_net(params, z, limit, scl, act_s=0):\n",
    "    '''\n",
    "    :param params: weights and biases\n",
    "    :param x: input data [matrix with shape [N, m]]; m is number of inputs)\n",
    "    :param limit: characteristic scale for normalizeation [matrx with shape [2, m]]\n",
    "    :param sgn:  1 for even function and -1 for odd function\n",
    "    :return: neural network output [matrix with shape [N, n]]; n is number of outputs)\n",
    "    '''\n",
    "    lb = limit[0]  # lower bound for each input\n",
    "    ub = limit[1]  # upper bound for each input\n",
    "\n",
    "    # choose the activation function\n",
    "    actv = [jnp.tanh, jnp.sin][act_s]\n",
    "    # normalize the input\n",
    "    H = 2.0 * (z - lb) / (ub - lb) - 1.0\n",
    "    # separate the first, hidden and last layers\n",
    "    first, *hidden, last = params\n",
    "    # calculate the first layers output with right scale\n",
    "    H = actv(jnp.dot(H, first[0]) * scl + first[1])\n",
    "    # calculate the middle layers output\n",
    "    for layer in hidden:\n",
    "        H = jnp.tanh(jnp.dot(H, layer[0]) + layer[1])\n",
    "    # no activation function for last layer \n",
    "    var = jnp.dot(H, last[0]) + last[1]\n",
    "    return var\n",
    "\n",
    "# generate weights and biases for all variables of CLM problem\n",
    "def sol_init_MLP(parent_key, n_hl, n_unit):\n",
    "    '''\n",
    "    :param n_hl: number of hidden layers [int]\n",
    "    :param n_unit: number of units in each layer [int]\n",
    "    '''\n",
    "    layers = [1] + n_hl * [n_unit] + [1]\n",
    "    # generate the random key for each network\n",
    "    keys = random.split(parent_key, 1)\n",
    "    # generate weights and biases for\n",
    "    params_u = init_MLP(keys[0], layers)\n",
    "    return dict(net_u=params_u)\n",
    "\n",
    "\n",
    "# wrapper to create solution function with given domain size\n",
    "def sol_pred_create(limit, scl, act_s=0):\n",
    "    '''\n",
    "    :param limit: domain size of the input\n",
    "    :return: function of the solution (a callable)\n",
    "    '''\n",
    "    def f_u(params, z):\n",
    "        # generate the NN\n",
    "        u = neural_net(params['net_u'], z, limit, scl, act_s)\n",
    "        return u\n",
    "    return f_u\n",
    "\n",
    "def mNN_pred_create(f_u, limit, scl, epsil, act_s=0):\n",
    "    '''\n",
    "    :param f_u: sum of previous stage network\n",
    "    :param limit: domain size of the input\n",
    "    :return: function of the solution (a callable)\n",
    "    '''\n",
    "    def f_comb(params, z):\n",
    "        # generate the NN\n",
    "        u_now = neural_net(params['net_u'], z, limit, scl, act_s)\n",
    "        u = f_u(z) + epsil * u_now\n",
    "        return u\n",
    "    return f_comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower-level PINN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate matrix required for vjp for vector gradient\n",
    "def vgmat(z, n_out, idx=None):\n",
    "    '''\n",
    "    :param n_out: number of output variables\n",
    "    :param idx: indice (list) of the output variable to take the gradient\n",
    "    '''\n",
    "    if idx is None:\n",
    "        idx = range(n_out)\n",
    "    # obtain the number of index\n",
    "    n_idx = len(idx)\n",
    "    # obtain the number of input points\n",
    "    n_pt = z.shape[0]\n",
    "    # determine the shape of the gradient matrix\n",
    "    mat_shape = [n_idx, n_pt, n_out]\n",
    "    # create the zero matrix based on the shape\n",
    "    mat = jnp.zeros(mat_shape)\n",
    "    # choose the associated element in the matrix to 1\n",
    "    for l, ii in zip(range(n_idx), idx):\n",
    "        mat = mat.at[l, :, ii].set(1.)\n",
    "    return mat\n",
    "\n",
    "\n",
    "# vector gradient of the output wrt input\n",
    "def vectgrad(func, z):\n",
    "    # obtain the output and the gradient function\n",
    "    sol, vjp_fn = vjp(func, z)\n",
    "    # determine the mat grad\n",
    "    mat = vgmat(z, sol.shape[1])\n",
    "    # calculate the gradient of each output with respect to each input\n",
    "    grad_sol = vmap(vjp_fn, in_axes=0)(mat)[0]\n",
    "    # calculate the total partial derivative of output with input\n",
    "    n_pd = z.shape[1] * sol.shape[1]\n",
    "    # reshape the derivative of output wrt input\n",
    "    grad_all = grad_sol.transpose(1, 0, 2).reshape(z.shape[0], n_pd)\n",
    "    return grad_all, sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function is du/dx = u + x, u(0) = 0\n",
    "def func(u, x):\n",
    "    return u + x\n",
    "\n",
    "# governing equation for above function\n",
    "def gov_eqn(f_u, x):\n",
    "    # calculate the output and its derivative with original coordinates\n",
    "    u_x, u = vectgrad(f_u, x)\n",
    "    # calculate the residue of the CCF equation\n",
    "    f = u_x - func(u, x)\n",
    "    return f\n",
    "\n",
    "def gov_d3_eqn(f_u, z):\n",
    "    # allocate the value to each variable\n",
    "    fc_res = lambda z: gov_eqn(f_u, z)\n",
    "    # calculate the residue of higher derivative of CCF equation\n",
    "    dfunc = lambda z: vectgrad(fc_res, z)[0]\n",
    "    # calculate the residue of the first and second derivative of CCF equation\n",
    "    d2f, df = vectgrad(dfunc, z)\n",
    "    return df, d2f\n",
    "\n",
    "# used for plotting vs exact solution\n",
    "def fg(x):\n",
    "    u = jnp.exp(x) - x - 1\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the mean squared error\n",
    "def ms_error(diff):\n",
    "    return jnp.mean(jnp.square(diff), axis=0)\n",
    "\n",
    "def loss_create(predf_u, cond, lw, loss_ref):\n",
    "    '''\n",
    "    a function factory to create the loss function based on given info\n",
    "    :param loss_ref: loss value at the initial of the training\n",
    "    :return: a loss function (callable)\n",
    "    '''\n",
    "\n",
    "    # loss function used for the PINN training\n",
    "    def loss_fun(params, data):\n",
    "        # create the function for gradient calculation involves input Z only\n",
    "        f_u = lambda z: predf_u(params, z)\n",
    "        # load the data of normalization condition\n",
    "        z_nm = cond['cond_nm'][0]\n",
    "        u_nm = cond['cond_nm'][1]\n",
    "\n",
    "        # load the position and weight of collocation points\n",
    "        z_col = data['z_col']\n",
    "\n",
    "        # calculate the gradient of phi at origin\n",
    "        u_nm_p = f_u(z_nm)\n",
    "\n",
    "        # calculate the residue of equation\n",
    "        f = gov_eqn(f_u, z_col)\n",
    "        # calculate the residue of first and second derivative\n",
    "        df, d2f = gov_d3_eqn(f_u, z_col)\n",
    "\n",
    "        # calculate the mean squared root error of normalization cond.\n",
    "        norm_err = ms_error(u_nm_p - u_nm)\n",
    "        # calculate the error of far-field exponent cond.\n",
    "        data_err = jnp.hstack([norm_err])\n",
    "\n",
    "        # calculate the mean squared root error of equation\n",
    "        eqn_err_f = ms_error(f)\n",
    "        eqn_err_df = ms_error(df)\n",
    "        eqn_err_d2f = ms_error(d2f)\n",
    "        eqn_err = jnp.hstack([eqn_err_f, eqn_err_df])\n",
    "\n",
    "        # set the weight for each condition and equation\n",
    "        data_weight = jnp.array([1.])\n",
    "        eqn_weight = jnp.array([1., lw[1]])\n",
    "\n",
    "        # calculate the overall data loss and equation loss\n",
    "        loss_data = jnp.sum(data_err * data_weight)\n",
    "        loss_eqn = jnp.sum(eqn_err * eqn_weight)\n",
    "\n",
    "        # calculate the total loss\n",
    "        loss = (loss_data + lw[0] * loss_eqn) / loss_ref\n",
    "        # group the loss of all conditions and equations\n",
    "        loss_info = jnp.hstack([jnp.array([loss, loss_data, loss_eqn]),\n",
    "                                data_err, eqn_err])\n",
    "        return loss, loss_info\n",
    "    return loss_fun\n",
    "\n",
    "\n",
    "# create the Adam minimizer \n",
    "@functools.partial(jit, static_argnames=(\"lossf\", \"opt\"))\n",
    "def adam_minimizer(lossf, params, data, opt, opt_state):\n",
    "    \"\"\"Basic gradient update step based on the opt optimizer.\"\"\"\n",
    "    grads, loss_info = grad(lossf, has_aux=True)(params, data)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, loss_info, opt_state\n",
    "\n",
    "\n",
    "def adam_optimizer(lossf, params, dataf, epoch, lr=1e-3):\n",
    "    # select the Adam as the minimizer\n",
    "    opt_Adam = optax.adam(learning_rate=lr)\n",
    "    # obtain the initial state of the params\n",
    "    opt_state = opt_Adam.init(params)\n",
    "    # pre-allocate the loss varaible\n",
    "    loss_all = []\n",
    "    data = dataf()\n",
    "    nc = jnp.int32(jnp.round(epoch / 5))\n",
    "    nc0 = 2500\n",
    "    # start the training iteration\n",
    "    for step in range(epoch):\n",
    "        # minimize the loss function using Adam\n",
    "        params, loss_info, opt_state = adam_minimizer(lossf, params, data, opt_Adam, opt_state)\n",
    "        # print the loss for every 100 iteration\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step: {step} | Loss: {loss_info[0]:.4e} |\"\n",
    "                  f\" Loss_d: {loss_info[1]:.4e} | Loss_e: {loss_info[2]:.4e}\", file=sys.stderr)\n",
    "            data = dataf()\n",
    "\n",
    "        # saving the loss \n",
    "        loss_all.append(loss_info[0:])\n",
    "\n",
    "        if (step+1) % (2*nc0) == 0:\n",
    "            lossend = np.array(loss_all[-2 * nc0:])[:, 0]\n",
    "            lc1 = lossend[0: nc0]\n",
    "            lc2 = lossend[nc0:]\n",
    "            mm12 = jnp.abs(jnp.mean(lc1) - jnp.mean(lc2))\n",
    "            stdl2 = jnp.std(lc2)\n",
    "            # if the average loss improvement within 'nc' iteration is less than local loss fluctuation (std)\n",
    "            if mm12 / stdl2 < 0.4:\n",
    "                # reduce the learning rate by half\n",
    "                lr = lr / 2\n",
    "                opt_Adam = optax.adam(learning_rate=lr)\n",
    "            print(f\"learning rate for Adam: {lr:.4e} | mean: {mm12:.3e} | std: {stdl2:.3e}\", file=sys.stderr)\n",
    "\n",
    "    # obtain the total loss in the last iterations\n",
    "    lossend = jnp.array(loss_all[-nc:])[:, 0]\n",
    "    # find the minimum loss value\n",
    "    # lmin = jnp.min(lossend)\n",
    "    # optain the last loss value\n",
    "    # llast = lossend[-1]\n",
    "    # guarantee the loss value in last iteration is smaller than anyone before\n",
    "\n",
    "    # while llast > lmin:\n",
    "    #     params, loss_info, opt_state = adam_minimizer(lossf, params, data, opt_Adam, opt_state)\n",
    "    #     llast = loss_info[0]\n",
    "    #     # saving the loss\n",
    "    #     loss_all.append(loss_info[0:])\n",
    "    return params, loss_all\n",
    "\n",
    "\n",
    "# A factory to create a function required by tfp.optimizer.lbfgs_minimize.\n",
    "def lbfgs_function(lossf, init_params, data):\n",
    "    # obtain the 1D parameters and the function that can turn back to the pytree\n",
    "    _, unflat = flat_utl.ravel_pytree(init_params)\n",
    "\n",
    "    def update(params_1d):\n",
    "        # updating the model's parameters from the 1D array\n",
    "        params = unflat(params_1d)\n",
    "        return params\n",
    "\n",
    "    # A function that can be used by tfp.optimizer.lbfgs_minimize.\n",
    "    @jit\n",
    "    def f(params_1d):\n",
    "        # convert the 1d parameters back to pytree format\n",
    "        params = update(params_1d)\n",
    "        # calculate gradients and convert to 1D tf.Tensor\n",
    "        grads, loss_info = grad(lossf, has_aux=True)(params, data)\n",
    "        # convert the grad to 1d arrays\n",
    "        grads_1d = flat_utl.ravel_pytree(grads)[0]\n",
    "        loss_value = loss_info[0]\n",
    "\n",
    "        # # store loss value so we can retrieve later\n",
    "        call(lambda x: f.loss.append(x), loss_info[0:], result_shape=None)\n",
    "        call(lambda x: print(f\" Loss: {x[0]:.4e} |\"\n",
    "                  f\" Loss_d: {x[1]:.4e} | Loss_e: {x[2]:.4e}\"), loss_info)\n",
    "        return loss_value, grads_1d\n",
    "\n",
    "    # store these information as members so we can use them outside the scope\n",
    "    f.update = update\n",
    "    f.loss = []\n",
    "    return f\n",
    "\n",
    "\n",
    "# define the function to apply the L-BFGS optimizer\n",
    "def lbfgs_optimizer(lossf, params, data, epoch):\n",
    "    func_lbfgs = lbfgs_function(lossf, params, data)\n",
    "    # convert initial model parameters to a 1D array\n",
    "    init_params_1d = flat_utl.ravel_pytree(params)[0]\n",
    "    # calculate the effective number of iteration\n",
    "    max_nIter = jnp.int32(epoch / 3)\n",
    "    # train the model with L-BFGS solver\n",
    "    results = tfp.optimizer.lbfgs_minimize(\n",
    "        value_and_gradients_function=func_lbfgs, initial_position=init_params_1d,\n",
    "        tolerance=1e-10, max_iterations=max_nIter)\n",
    "    params = func_lbfgs.update(results.position)\n",
    "    # history = func_lbfgs.loss\n",
    "    num_iter = results.num_objective_evaluations\n",
    "    loss_all = func_lbfgs.loss\n",
    "    print(f\" Total iterations: {num_iter}\")\n",
    "    return params, loss_all\n",
    "\n",
    "\n",
    "\"\"\"Prepare the collocaiton points\"\"\"\n",
    "def data_func_create(N_col, ds):\n",
    "    # define the function that can re-sampling for each calling\n",
    "    def dataf():\n",
    "        # prepare the collocation points\n",
    "        z_cn = (2 * lhs(1, N_col) - 1)\n",
    "        z_col = jnp.sign(z_cn) * jnp.abs(z_cn) ** 1 * ds * 1.01\n",
    "        # add the collocation at the boundary\n",
    "        z_col = jnp.vstack([z_col, jnp.array([-ds, ds])[:, None]])\n",
    "        return dict(z_col=z_col)\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "key = random.PRNGKey(seed)\n",
    "np.random.seed(seed)\n",
    "keys = random.split(key, 4) # create the subkeys\n",
    "\n",
    "# select the size of neural network\n",
    "n_hl = 3\n",
    "n_unit = 20\n",
    "\n",
    "N_col = 201 # number of sampling points\n",
    "ds = 1.\n",
    "lmt = jnp.array([[-ds], [ds]]) # set the size of domain\n",
    "total_time_range = ds * 2 # from -1 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Stage Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 0 | Loss: 1.4459e-01 | Loss_d: 6.6685e-03 | Loss_e: 4.5975e-01\n",
      "Step: 100 | Loss: 4.8714e-02 | Loss_d: 5.3996e-03 | Loss_e: 1.4438e-01\n",
      "Step: 200 | Loss: 1.1697e-02 | Loss_d: 7.2595e-04 | Loss_e: 3.6570e-02\n",
      "Step: 300 | Loss: 4.4644e-04 | Loss_d: 7.9369e-06 | Loss_e: 1.4617e-03\n",
      "Step: 400 | Loss: 1.9222e-04 | Loss_d: 7.1284e-07 | Loss_e: 6.3835e-04\n",
      "Step: 500 | Loss: 1.3356e-04 | Loss_d: 3.1265e-07 | Loss_e: 4.4416e-04\n",
      "Step: 600 | Loss: 1.0087e-04 | Loss_d: 2.4392e-07 | Loss_e: 3.3541e-04\n",
      "Step: 700 | Loss: 7.2785e-05 | Loss_d: 2.6368e-09 | Loss_e: 2.4261e-04\n",
      "Step: 800 | Loss: 5.5612e-05 | Loss_d: 1.9194e-06 | Loss_e: 1.7898e-04\n",
      "Step: 900 | Loss: 3.9673e-05 | Loss_d: 1.0688e-07 | Loss_e: 1.3189e-04\n",
      "Step: 1000 | Loss: 2.9670e-05 | Loss_d: 3.5341e-08 | Loss_e: 9.8782e-05\n",
      "Step: 1100 | Loss: 2.3016e-05 | Loss_d: 1.7640e-07 | Loss_e: 7.6131e-05\n",
      "Step: 1200 | Loss: 1.6368e-05 | Loss_d: 1.2107e-08 | Loss_e: 5.4520e-05\n",
      "Step: 1300 | Loss: 2.8047e-05 | Loss_d: 1.1295e-05 | Loss_e: 5.5839e-05\n",
      "Step: 1400 | Loss: 9.6606e-06 | Loss_d: 9.3698e-09 | Loss_e: 3.2171e-05\n",
      "Step: 1500 | Loss: 1.2006e-04 | Loss_d: 7.7863e-05 | Loss_e: 1.4066e-04\n",
      "Step: 1600 | Loss: 6.0235e-06 | Loss_d: 8.7226e-10 | Loss_e: 2.0075e-05\n",
      "Step: 1700 | Loss: 4.9385e-06 | Loss_d: 3.2913e-09 | Loss_e: 1.6451e-05\n",
      "Step: 1800 | Loss: 4.3645e-06 | Loss_d: 3.1827e-09 | Loss_e: 1.4538e-05\n",
      "Step: 1900 | Loss: 3.8661e-06 | Loss_d: 6.3268e-10 | Loss_e: 1.2885e-05\n",
      "Step: 2000 | Loss: 6.4495e-06 | Loss_d: 1.9672e-06 | Loss_e: 1.4941e-05\n",
      "Step: 2100 | Loss: 3.3273e-06 | Loss_d: 5.2251e-10 | Loss_e: 1.1089e-05\n",
      "Step: 2200 | Loss: 7.1764e-06 | Loss_d: 2.6715e-06 | Loss_e: 1.5016e-05\n",
      "Step: 2300 | Loss: 3.1687e-06 | Loss_d: 3.5385e-09 | Loss_e: 1.0551e-05\n",
      "Step: 2400 | Loss: 3.1817e-06 | Loss_d: 6.5843e-10 | Loss_e: 1.0603e-05\n",
      "Step: 2500 | Loss: 3.0554e-06 | Loss_d: 3.3244e-08 | Loss_e: 1.0074e-05\n",
      "Step: 2600 | Loss: 3.0413e-06 | Loss_d: 7.9936e-11 | Loss_e: 1.0137e-05\n",
      "Step: 2700 | Loss: 2.1428e-05 | Loss_d: 1.2368e-05 | Loss_e: 3.0199e-05\n",
      "Step: 2800 | Loss: 2.9006e-06 | Loss_d: 5.5956e-11 | Loss_e: 9.6685e-06\n",
      "Step: 2900 | Loss: 2.8094e-06 | Loss_d: 1.8674e-09 | Loss_e: 9.3583e-06\n",
      "Step: 3000 | Loss: 2.8904e-06 | Loss_d: 1.0367e-07 | Loss_e: 9.2891e-06\n",
      "Step: 3100 | Loss: 2.7375e-06 | Loss_d: 1.3440e-10 | Loss_e: 9.1246e-06\n",
      "Step: 3200 | Loss: 7.1507e-06 | Loss_d: 2.9335e-06 | Loss_e: 1.4057e-05\n",
      "Step: 3300 | Loss: 2.7365e-06 | Loss_d: 6.5690e-10 | Loss_e: 9.1194e-06\n",
      "Step: 3400 | Loss: 3.0514e-05 | Loss_d: 1.8685e-05 | Loss_e: 3.9431e-05\n",
      "Step: 3500 | Loss: 2.5621e-06 | Loss_d: 1.3509e-10 | Loss_e: 8.5397e-06\n",
      "Step: 3600 | Loss: 2.6094e-06 | Loss_d: 9.8489e-09 | Loss_e: 8.6651e-06\n",
      "Step: 3700 | Loss: 2.6311e-06 | Loss_d: 7.1464e-10 | Loss_e: 8.7681e-06\n",
      "Step: 3800 | Loss: 2.4633e-06 | Loss_d: 1.7509e-10 | Loss_e: 8.2104e-06\n",
      "Step: 3900 | Loss: 2.9781e-06 | Loss_d: 2.4684e-07 | Loss_e: 9.1041e-06\n",
      "Step: 4000 | Loss: 2.4406e-06 | Loss_d: 2.3102e-10 | Loss_e: 8.1346e-06\n",
      "Step: 4100 | Loss: 7.0499e-06 | Loss_d: 3.1282e-06 | Loss_e: 1.3072e-05\n",
      "Step: 4200 | Loss: 2.4898e-06 | Loss_d: 5.3346e-10 | Loss_e: 8.2974e-06\n",
      "Step: 4300 | Loss: 5.3292e-06 | Loss_d: 1.9726e-06 | Loss_e: 1.1189e-05\n",
      "Step: 4400 | Loss: 2.3432e-06 | Loss_d: 1.3977e-08 | Loss_e: 7.7642e-06\n",
      "Step: 4500 | Loss: 2.2576e-06 | Loss_d: 1.2790e-13 | Loss_e: 7.5254e-06\n",
      "Step: 4600 | Loss: 2.6053e-06 | Loss_d: 2.0313e-07 | Loss_e: 8.0074e-06\n",
      "Step: 4700 | Loss: 2.2886e-06 | Loss_d: 1.2691e-10 | Loss_e: 7.6281e-06\n",
      "Step: 4800 | Loss: 5.3077e-05 | Loss_d: 3.4763e-05 | Loss_e: 6.1046e-05\n",
      "Step: 4900 | Loss: 2.1964e-06 | Loss_d: 5.6090e-09 | Loss_e: 7.3027e-06\n",
      "learning rate for Adam: 1.0000e-03 | mean: 4.510e-03 | std: 3.511e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\akavu\\Desktop\\Summer Research\\.venv\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\jax\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\akavu\\Desktop\\Summer Research\\.venv\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\jax\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n",
      " Loss: 2.1457e-06 | Loss_d: 8.9252e-11 | Loss_e: 7.1521e-06\n",
      " Loss: 2.1481e-06 | Loss_d: 1.4168e-09 | Loss_e: 7.1557e-06\n",
      " Loss: 2.1456e-06 | Loss_d: 2.9878e-12 | Loss_e: 7.1521e-06\n",
      " Loss: 2.1455e-06 | Loss_d: 3.2742e-11 | Loss_e: 7.1517e-06\n",
      " Loss: 2.1455e-06 | Loss_d: 3.0727e-11 | Loss_e: 7.1517e-06\n",
      " Loss: 2.1455e-06 | Loss_d: 3.3084e-11 | Loss_e: 7.1517e-06\n",
      " Loss: 2.1454e-06 | Loss_d: 3.5527e-11 | Loss_e: 7.1512e-06\n",
      " Loss: 2.1451e-06 | Loss_d: 6.8642e-11 | Loss_e: 7.1500e-06\n",
      " Loss: 2.1467e-06 | Loss_d: 3.6722e-10 | Loss_e: 7.1545e-06\n",
      " Loss: 2.1449e-06 | Loss_d: 1.1511e-10 | Loss_e: 7.1494e-06\n",
      " Loss: 2.1447e-06 | Loss_d: 7.8874e-11 | Loss_e: 7.1486e-06\n",
      " Loss: 2.1437e-06 | Loss_d: 5.1301e-12 | Loss_e: 7.1458e-06\n",
      " Loss: 2.1461e-06 | Loss_d: 8.9708e-10 | Loss_e: 7.1505e-06\n",
      " Loss: 2.1433e-06 | Loss_d: 5.2446e-11 | Loss_e: 7.1440e-06\n",
      " Loss: 2.1417e-06 | Loss_d: 5.5068e-11 | Loss_e: 7.1388e-06\n",
      " Loss: 2.1393e-06 | Loss_d: 6.5711e-11 | Loss_e: 7.1307e-06\n",
      " Loss: 2.1393e-06 | Loss_d: 6.4269e-11 | Loss_e: 7.1306e-06\n",
      " Loss: 2.1365e-06 | Loss_d: 2.9098e-11 | Loss_e: 7.1214e-06\n",
      " Loss: 2.1289e-06 | Loss_d: 3.2402e-11 | Loss_e: 7.0962e-06\n",
      " Loss: 2.1802e-06 | Loss_d: 3.5812e-09 | Loss_e: 7.2554e-06\n",
      " Loss: 2.1271e-06 | Loss_d: 1.9620e-10 | Loss_e: 7.0897e-06\n",
      " Loss: 2.1237e-06 | Loss_d: 1.7117e-10 | Loss_e: 7.0784e-06\n",
      " Loss: 2.1109e-06 | Loss_d: 5.9580e-11 | Loss_e: 7.0361e-06\n",
      " Loss: 2.0701e-06 | Loss_d: 3.2188e-10 | Loss_e: 6.8993e-06\n",
      " Loss: 2.3989e-06 | Loss_d: 1.8900e-08 | Loss_e: 7.9332e-06\n",
      " Loss: 2.0636e-06 | Loss_d: 1.1563e-09 | Loss_e: 6.8749e-06\n",
      " Loss: 2.0590e-06 | Loss_d: 2.2172e-11 | Loss_e: 6.8631e-06\n",
      " Loss: 2.1132e-06 | Loss_d: 2.5270e-08 | Loss_e: 6.9598e-06\n",
      " Loss: 2.0589e-06 | Loss_d: 9.5554e-11 | Loss_e: 6.8625e-06\n",
      " Loss: 2.0588e-06 | Loss_d: 1.0880e-10 | Loss_e: 6.8623e-06\n",
      " Loss: 2.0587e-06 | Loss_d: 1.5444e-10 | Loss_e: 6.8617e-06\n",
      " Loss: 2.0601e-06 | Loss_d: 4.2901e-10 | Loss_e: 6.8655e-06\n",
      " Loss: 2.0586e-06 | Loss_d: 1.7195e-10 | Loss_e: 6.8616e-06\n",
      " Loss: 2.0573e-06 | Loss_d: 1.5223e-10 | Loss_e: 6.8571e-06\n",
      " Loss: 2.0518e-06 | Loss_d: 9.2663e-11 | Loss_e: 6.8392e-06\n",
      " Loss: 2.0255e-06 | Loss_d: 1.1543e-11 | Loss_e: 6.7516e-06\n",
      " Loss: 1.9144e-06 | Loss_d: 4.8260e-09 | Loss_e: 6.3651e-06\n",
      " Loss: 1.9953e-06 | Loss_d: 1.6065e-07 | Loss_e: 6.1153e-06\n",
      " Loss: 1.8061e-06 | Loss_d: 3.8233e-08 | Loss_e: 5.8928e-06\n",
      " Loss: 1.5781e-06 | Loss_d: 1.5452e-08 | Loss_e: 5.2087e-06\n",
      " Loss: 2.5328e-06 | Loss_d: 4.6197e-08 | Loss_e: 8.2887e-06\n",
      " Loss: 1.4949e-06 | Loss_d: 6.1294e-09 | Loss_e: 4.9626e-06\n",
      " Loss: 1.2593e-06 | Loss_d: 2.9549e-09 | Loss_e: 4.1878e-06\n",
      " Loss: 1.5883e-06 | Loss_d: 4.4992e-07 | Loss_e: 3.7945e-06\n",
      " Loss: 1.1724e-06 | Loss_d: 3.6928e-08 | Loss_e: 3.7848e-06\n",
      " Loss: 1.3404e-06 | Loss_d: 1.1244e-08 | Loss_e: 4.4306e-06\n",
      " Loss: 1.0997e-06 | Loss_d: 2.9468e-08 | Loss_e: 3.5674e-06\n",
      " Loss: 9.6720e-07 | Loss_d: 9.6781e-09 | Loss_e: 3.1917e-06\n",
      " Loss: 9.4464e-07 | Loss_d: 1.1511e-12 | Loss_e: 3.1488e-06\n",
      " Loss: 8.6254e-07 | Loss_d: 7.0181e-09 | Loss_e: 2.8517e-06\n",
      " Loss: 1.0011e-06 | Loss_d: 1.7703e-07 | Loss_e: 2.7470e-06\n",
      " Loss: 8.4299e-07 | Loss_d: 3.9538e-08 | Loss_e: 2.6782e-06\n",
      " Loss: 9.6227e-07 | Loss_d: 2.6226e-08 | Loss_e: 3.1202e-06\n",
      " Loss: 7.8960e-07 | Loss_d: 3.6357e-08 | Loss_e: 2.5108e-06\n",
      " Loss: 7.7876e-07 | Loss_d: 6.1778e-10 | Loss_e: 2.5938e-06\n",
      " Loss: 7.0205e-07 | Loss_d: 1.2503e-08 | Loss_e: 2.2985e-06\n",
      " Loss: 6.2728e-07 | Loss_d: 7.7137e-09 | Loss_e: 2.0652e-06\n",
      " Loss: 6.2727e-07 | Loss_d: 7.2599e-09 | Loss_e: 2.0667e-06\n",
      " Loss: 5.9271e-07 | Loss_d: 4.0371e-09 | Loss_e: 1.9623e-06\n",
      " Loss: 5.2463e-07 | Loss_d: 4.1797e-10 | Loss_e: 1.7474e-06\n",
      " Loss: 1.2867e-05 | Loss_d: 1.9835e-07 | Loss_e: 4.2228e-05\n",
      " Loss: 5.2465e-07 | Loss_d: 4.3396e-10 | Loss_e: 1.7474e-06\n",
      " Loss: 6.2637e-07 | Loss_d: 4.2598e-09 | Loss_e: 2.0737e-06\n",
      " Loss: 4.8343e-07 | Loss_d: 1.3547e-09 | Loss_e: 1.6069e-06\n",
      " Loss: 4.3404e-07 | Loss_d: 9.2225e-10 | Loss_e: 1.4437e-06\n",
      " Loss: 3.2880e-06 | Loss_d: 1.0802e-07 | Loss_e: 1.0600e-05\n",
      " Loss: 4.3345e-07 | Loss_d: 1.0747e-09 | Loss_e: 1.4413e-06\n",
      " Loss: 4.1093e-07 | Loss_d: 2.8490e-09 | Loss_e: 1.3603e-06\n",
      " Loss: 4.0850e-07 | Loss_d: 2.0873e-09 | Loss_e: 1.3547e-06\n",
      " Loss: 3.4339e-07 | Loss_d: 7.6927e-09 | Loss_e: 1.1190e-06\n",
      " Loss: 9.3676e-07 | Loss_d: 8.4484e-08 | Loss_e: 2.8409e-06\n",
      " Loss: 3.3276e-07 | Loss_d: 1.0069e-08 | Loss_e: 1.0756e-06\n",
      " Loss: 3.4508e-07 | Loss_d: 5.2660e-12 | Loss_e: 1.1502e-06\n",
      " Loss: 3.0849e-07 | Loss_d: 2.7637e-09 | Loss_e: 1.0191e-06\n",
      " Loss: 2.8018e-07 | Loss_d: 9.9796e-10 | Loss_e: 9.3062e-07\n",
      " Loss: 1.4605e-06 | Loss_d: 2.0575e-09 | Loss_e: 4.8616e-06\n",
      " Loss: 2.7978e-07 | Loss_d: 9.4044e-10 | Loss_e: 9.2948e-07\n",
      " Loss: 2.3393e-07 | Loss_d: 9.4227e-10 | Loss_e: 7.7662e-07\n",
      " Loss: 1.6635e-06 | Loss_d: 2.5038e-09 | Loss_e: 5.5365e-06\n",
      " Loss: 2.3330e-07 | Loss_d: 9.6620e-10 | Loss_e: 7.7445e-07\n",
      " Loss: 2.1360e-07 | Loss_d: 8.0158e-12 | Loss_e: 7.1198e-07\n",
      " Loss: 3.4193e-07 | Loss_d: 7.8082e-09 | Loss_e: 1.1137e-06\n",
      " Loss: 2.1031e-07 | Loss_d: 7.0632e-11 | Loss_e: 7.0081e-07\n",
      " Loss: 2.3316e-07 | Loss_d: 1.6597e-09 | Loss_e: 7.7168e-07\n",
      " Loss: 2.0340e-07 | Loss_d: 3.7410e-10 | Loss_e: 6.7676e-07\n",
      " Loss: 2.0124e-07 | Loss_d: 9.8489e-11 | Loss_e: 6.7048e-07\n",
      " Loss: 1.9267e-07 | Loss_d: 1.0147e-10 | Loss_e: 6.4189e-07\n",
      " Loss: 2.6619e-07 | Loss_d: 3.6854e-09 | Loss_e: 8.7503e-07\n",
      " Loss: 1.8936e-07 | Loss_d: 2.5517e-10 | Loss_e: 6.3033e-07\n",
      " Loss: 2.3897e-07 | Loss_d: 1.0744e-08 | Loss_e: 7.6074e-07\n",
      " Loss: 1.8568e-07 | Loss_d: 1.9984e-13 | Loss_e: 6.1892e-07\n",
      " Loss: 1.8481e-07 | Loss_d: 1.3718e-10 | Loss_e: 6.1557e-07\n",
      " Loss: 1.8002e-07 | Loss_d: 2.6276e-11 | Loss_e: 5.9997e-07\n",
      " Loss: 1.7195e-07 | Loss_d: 1.0818e-10 | Loss_e: 5.7279e-07\n",
      " Loss: 4.6345e-07 | Loss_d: 5.8162e-09 | Loss_e: 1.5254e-06\n",
      " Loss: 1.7182e-07 | Loss_d: 1.2423e-10 | Loss_e: 5.7231e-07\n",
      " Loss: 1.6029e-07 | Loss_d: 8.6459e-11 | Loss_e: 5.3400e-07\n",
      " Loss: 1.9748e-07 | Loss_d: 8.4640e-09 | Loss_e: 6.3004e-07\n",
      " Loss: 1.5824e-07 | Loss_d: 7.6324e-10 | Loss_e: 5.2491e-07\n",
      " Loss: 1.5353e-07 | Loss_d: 2.6965e-10 | Loss_e: 5.1086e-07\n",
      " Loss: 1.5352e-07 | Loss_d: 2.4949e-10 | Loss_e: 5.1091e-07\n",
      " Loss: 1.5182e-07 | Loss_d: 1.9620e-10 | Loss_e: 5.0541e-07\n",
      " Loss: 1.5563e-07 | Loss_d: 3.6601e-11 | Loss_e: 5.1866e-07\n",
      " Loss: 1.5118e-07 | Loss_d: 1.3579e-10 | Loss_e: 5.0347e-07\n",
      " Loss: 1.4730e-07 | Loss_d: 1.5370e-10 | Loss_e: 4.9048e-07\n",
      " Loss: 1.4727e-07 | Loss_d: 9.0949e-11 | Loss_e: 4.9059e-07\n",
      " Loss: 1.4399e-07 | Loss_d: 3.6040e-10 | Loss_e: 4.7877e-07\n",
      " Loss: 1.7754e-07 | Loss_d: 3.2878e-09 | Loss_e: 5.8085e-07\n",
      " Loss: 1.4340e-07 | Loss_d: 4.7331e-10 | Loss_e: 4.7642e-07\n",
      " Loss: 1.3745e-07 | Loss_d: 5.6851e-11 | Loss_e: 4.5798e-07\n",
      " Loss: 1.3740e-07 | Loss_d: 7.6771e-11 | Loss_e: 4.5775e-07\n",
      " Loss: 1.3545e-07 | Loss_d: 1.0880e-10 | Loss_e: 4.5114e-07\n",
      " Loss: 1.3496e-07 | Loss_d: 1.0267e-10 | Loss_e: 4.4952e-07\n",
      " Loss: 1.3363e-07 | Loss_d: 1.4211e-14 | Loss_e: 4.4544e-07\n",
      " Loss: 1.3309e-07 | Loss_d: 2.2737e-11 | Loss_e: 4.4357e-07\n",
      " Loss: 1.3072e-07 | Loss_d: 4.4964e-11 | Loss_e: 4.3559e-07\n",
      " Loss: 2.3811e-07 | Loss_d: 3.0959e-09 | Loss_e: 7.8339e-07\n",
      " Loss: 1.3071e-07 | Loss_d: 5.1586e-11 | Loss_e: 4.3551e-07\n",
      " Loss: 1.2781e-07 | Loss_d: 1.9536e-10 | Loss_e: 4.2539e-07\n",
      " Loss: 1.3275e-07 | Loss_d: 1.7884e-09 | Loss_e: 4.3654e-07\n",
      " Loss: 1.2717e-07 | Loss_d: 5.4037e-10 | Loss_e: 4.2209e-07\n",
      " Loss: 1.2954e-07 | Loss_d: 2.6890e-11 | Loss_e: 4.3171e-07\n",
      " Loss: 1.2550e-07 | Loss_d: 2.4574e-10 | Loss_e: 4.1753e-07\n",
      " Loss: 1.2257e-07 | Loss_d: 5.5011e-10 | Loss_e: 4.0672e-07\n",
      " Loss: 1.2232e-07 | Loss_d: 1.6270e-10 | Loss_e: 4.0720e-07\n",
      " Loss: 1.2096e-07 | Loss_d: 3.0085e-10 | Loss_e: 4.0221e-07\n",
      " Loss: 1.3616e-07 | Loss_d: 1.1931e-09 | Loss_e: 4.4989e-07\n",
      " Loss: 1.2094e-07 | Loss_d: 3.1974e-10 | Loss_e: 4.0205e-07\n",
      " Loss: 1.2413e-07 | Loss_d: 5.3622e-10 | Loss_e: 4.1198e-07\n",
      " Loss: 1.2015e-07 | Loss_d: 6.5229e-11 | Loss_e: 4.0029e-07\n",
      " Loss: 1.1839e-07 | Loss_d: 1.3509e-12 | Loss_e: 3.9463e-07\n",
      " Loss: 2.5161e-07 | Loss_d: 5.6276e-10 | Loss_e: 8.3681e-07\n",
      " Loss: 1.1839e-07 | Loss_d: 9.6723e-13 | Loss_e: 3.9462e-07\n",
      " Loss: 1.1586e-07 | Loss_d: 1.2357e-10 | Loss_e: 3.8580e-07\n",
      " Loss: 1.2057e-07 | Loss_d: 3.7764e-09 | Loss_e: 3.8932e-07\n",
      " Loss: 1.1472e-07 | Loss_d: 5.5291e-10 | Loss_e: 3.8057e-07\n",
      " Loss: 1.1249e-07 | Loss_d: 8.3710e-11 | Loss_e: 3.7469e-07\n",
      " Loss: 1.1329e-07 | Loss_d: 2.4269e-09 | Loss_e: 3.6956e-07\n",
      " Loss: 1.1154e-07 | Loss_d: 3.9751e-10 | Loss_e: 3.7047e-07\n",
      " Loss: 2.1534e-07 | Loss_d: 2.2229e-09 | Loss_e: 7.1037e-07\n",
      " Loss: 1.1122e-07 | Loss_d: 4.1919e-10 | Loss_e: 3.6935e-07\n",
      " Loss: 1.1478e-07 | Loss_d: 5.9721e-12 | Loss_e: 3.8258e-07\n",
      " Loss: 1.0968e-07 | Loss_d: 2.3283e-10 | Loss_e: 3.6483e-07\n",
      " Loss: 1.0988e-07 | Loss_d: 6.1434e-11 | Loss_e: 3.6606e-07\n",
      " Loss: 1.0850e-07 | Loss_d: 4.4964e-11 | Loss_e: 3.6151e-07\n",
      " Loss: 1.0754e-07 | Loss_d: 2.4201e-10 | Loss_e: 3.5767e-07\n",
      " Loss: 1.2377e-07 | Loss_d: 1.1256e-08 | Loss_e: 3.7505e-07\n",
      " Loss: 1.0754e-07 | Loss_d: 2.4201e-10 | Loss_e: 3.5767e-07\n",
      " Loss: 1.0707e-07 | Loss_d: 2.1763e-10 | Loss_e: 3.5618e-07\n",
      " Loss: 1.1111e-07 | Loss_d: 1.0880e-10 | Loss_e: 3.7000e-07\n",
      " Loss: 1.0699e-07 | Loss_d: 2.1238e-10 | Loss_e: 3.5594e-07\n",
      " Loss: 1.0489e-07 | Loss_d: 4.6171e-11 | Loss_e: 3.4947e-07\n",
      " Loss: 1.1193e-07 | Loss_d: 7.4036e-10 | Loss_e: 3.7063e-07\n",
      " Loss: 1.0411e-07 | Loss_d: 1.9620e-12 | Loss_e: 3.4704e-07\n",
      " Loss: 1.0300e-07 | Loss_d: 6.4748e-13 | Loss_e: 3.4334e-07\n",
      " Loss: 1.0981e-07 | Loss_d: 1.0880e-12 | Loss_e: 3.6604e-07\n",
      " Loss: 1.0296e-07 | Loss_d: 9.0949e-13 | Loss_e: 3.4321e-07\n",
      " Loss: 1.0461e-07 | Loss_d: 0.0000e+00 | Loss_e: 3.4871e-07\n",
      " Loss: 1.0244e-07 | Loss_d: 3.5527e-13 | Loss_e: 3.4146e-07\n",
      " Loss: 1.0227e-07 | Loss_d: 6.9633e-13 | Loss_e: 3.4091e-07\n",
      " Loss: 1.0221e-07 | Loss_d: 1.4211e-14 | Loss_e: 3.4070e-07\n",
      " Loss: 1.0217e-07 | Loss_d: 7.9936e-13 | Loss_e: 3.4056e-07\n",
      " Loss: 1.0243e-07 | Loss_d: 1.0943e-11 | Loss_e: 3.4140e-07\n",
      " Loss: 1.0216e-07 | Loss_d: 2.0464e-12 | Loss_e: 3.4053e-07\n",
      " Loss: 1.0214e-07 | Loss_d: 5.6843e-12 | Loss_e: 3.4044e-07\n",
      " Loss: 1.0214e-07 | Loss_d: 3.9543e-11 | Loss_e: 3.4032e-07\n",
      " Loss: 1.0212e-07 | Loss_d: 2.0251e-11 | Loss_e: 3.4033e-07\n",
      " Loss: 1.0207e-07 | Loss_d: 1.9455e-11 | Loss_e: 3.4017e-07\n",
      " Loss: 1.0191e-07 | Loss_d: 6.8781e-12 | Loss_e: 3.3968e-07\n",
      " Loss: 1.0269e-07 | Loss_d: 4.1823e-11 | Loss_e: 3.4216e-07\n",
      " Loss: 1.0184e-07 | Loss_d: 1.0267e-12 | Loss_e: 3.3946e-07\n",
      " Loss: 1.0191e-07 | Loss_d: 2.0251e-11 | Loss_e: 3.3965e-07\n",
      " Loss: 1.0179e-07 | Loss_d: 1.4930e-12 | Loss_e: 3.3930e-07\n",
      " Loss: 1.0173e-07 | Loss_d: 3.9169e-13 | Loss_e: 3.3910e-07\n",
      " Loss: 1.0163e-07 | Loss_d: 2.2737e-13 | Loss_e: 3.3876e-07\n",
      " Loss: 1.0163e-07 | Loss_d: 1.9984e-13 | Loss_e: 3.3877e-07\n",
      " Loss: 1.0161e-07 | Loss_d: 1.0267e-10 | Loss_e: 3.3836e-07\n",
      " Loss: 1.0155e-07 | Loss_d: 2.7200e-11 | Loss_e: 3.3841e-07\n",
      " Loss: 1.0135e-07 | Loss_d: 1.1746e-11 | Loss_e: 3.3780e-07\n",
      " Loss: 1.0086e-07 | Loss_d: 1.7160e-11 | Loss_e: 3.3614e-07\n",
      " Loss: 1.0326e-07 | Loss_d: 1.9036e-09 | Loss_e: 3.3787e-07\n",
      " Loss: 1.0079e-07 | Loss_d: 1.4353e-10 | Loss_e: 3.3550e-07\n",
      " Loss: 9.9340e-08 | Loss_d: 1.1383e-10 | Loss_e: 3.3075e-07\n",
      " Loss: 1.1378e-07 | Loss_d: 5.4187e-11 | Loss_e: 3.7909e-07\n",
      " Loss: 9.8919e-08 | Loss_d: 1.0942e-10 | Loss_e: 3.2936e-07\n",
      " Loss: 9.8654e-07 | Loss_d: 2.1238e-10 | Loss_e: 3.2878e-06\n",
      " Loss: 9.8846e-08 | Loss_d: 1.0388e-10 | Loss_e: 3.2914e-07\n",
      " Loss: 1.0021e-07 | Loss_d: 6.5711e-11 | Loss_e: 3.3380e-07\n",
      " Loss: 9.8278e-08 | Loss_d: 9.5554e-11 | Loss_e: 3.2727e-07\n",
      " Loss: 9.6934e-08 | Loss_d: 2.8777e-11 | Loss_e: 3.2302e-07\n",
      " Loss: 9.6872e-08 | Loss_d: 5.5431e-12 | Loss_e: 3.2289e-07\n",
      " Loss: 9.6616e-08 | Loss_d: 5.7302e-11 | Loss_e: 3.2186e-07\n",
      " Loss: 9.6557e-08 | Loss_d: 4.0675e-11 | Loss_e: 3.2172e-07\n",
      " Loss: 9.6427e-08 | Loss_d: 7.9936e-15 | Loss_e: 3.2142e-07\n",
      " Loss: 9.7564e-08 | Loss_d: 5.3760e-10 | Loss_e: 3.2342e-07\n",
      " Loss: 9.6421e-08 | Loss_d: 2.0464e-12 | Loss_e: 3.2139e-07\n",
      " Loss: 9.6366e-08 | Loss_d: 4.1069e-12 | Loss_e: 3.2121e-07\n",
      " Loss: 9.6221e-08 | Loss_d: 1.0943e-11 | Loss_e: 3.2070e-07\n",
      " Loss: 9.7005e-08 | Loss_d: 1.3302e-10 | Loss_e: 3.2291e-07\n",
      " Loss: 9.6166e-08 | Loss_d: 2.0251e-11 | Loss_e: 3.2048e-07\n",
      " Loss: 9.7102e-08 | Loss_d: 3.9918e-11 | Loss_e: 3.2354e-07\n",
      " Loss: 9.6135e-08 | Loss_d: 7.0353e-12 | Loss_e: 3.2043e-07\n",
      " Loss: 9.6062e-08 | Loss_d: 5.6843e-14 | Loss_e: 3.2021e-07\n",
      " Loss: 9.6412e-08 | Loss_d: 8.6459e-11 | Loss_e: 3.2108e-07\n",
      " Loss: 9.6054e-08 | Loss_d: 2.4016e-12 | Loss_e: 3.2017e-07\n",
      " Loss: 9.6041e-08 | Loss_d: 1.0747e-13 | Loss_e: 3.2014e-07\n",
      " Loss: 9.6218e-08 | Loss_d: 1.5242e-11 | Loss_e: 3.2068e-07\n",
      " Loss: 9.6035e-08 | Loss_d: 8.8818e-14 | Loss_e: 3.2012e-07\n",
      " Loss: 9.6030e-08 | Loss_d: 2.2204e-14 | Loss_e: 3.2010e-07\n",
      " Loss: 9.6094e-08 | Loss_d: 6.2670e-12 | Loss_e: 3.2029e-07\n",
      " Loss: 9.6031e-08 | Loss_d: 2.8777e-13 | Loss_e: 3.2010e-07\n",
      " Loss: 9.6048e-08 | Loss_d: 2.2204e-12 | Loss_e: 3.2015e-07\n",
      " Loss: 9.6029e-08 | Loss_d: 2.2204e-14 | Loss_e: 3.2010e-07\n",
      " Loss: 9.6028e-08 | Loss_d: 1.0747e-13 | Loss_e: 3.2009e-07\n",
      " Loss: 9.6013e-08 | Loss_d: 2.5668e-13 | Loss_e: 3.2004e-07\n",
      " Loss: 9.6255e-08 | Loss_d: 4.3521e-12 | Loss_e: 3.2083e-07\n",
      " Loss: 9.6012e-08 | Loss_d: 1.0747e-13 | Loss_e: 3.2004e-07\n",
      " Loss: 9.5993e-08 | Loss_d: 1.4930e-12 | Loss_e: 3.1997e-07\n",
      " Loss: 9.5964e-08 | Loss_d: 2.2737e-11 | Loss_e: 3.1981e-07\n",
      " Loss: 9.6683e-08 | Loss_d: 4.0228e-10 | Loss_e: 3.2094e-07\n",
      " Loss: 9.5960e-08 | Loss_d: 1.9455e-11 | Loss_e: 3.1980e-07\n",
      " Loss: 9.5939e-08 | Loss_d: 1.4780e-11 | Loss_e: 3.1975e-07\n",
      " Loss: 9.5944e-08 | Loss_d: 1.0267e-12 | Loss_e: 3.1981e-07\n",
      " Loss: 9.5925e-08 | Loss_d: 7.1942e-12 | Loss_e: 3.1973e-07\n",
      " Loss: 9.5799e-08 | Loss_d: 2.7853e-12 | Loss_e: 3.1932e-07\n",
      " Loss: 9.5381e-08 | Loss_d: 6.5690e-12 | Loss_e: 3.1792e-07\n",
      " Loss: 9.4833e-08 | Loss_d: 5.1843e-10 | Loss_e: 3.1438e-07\n",
      " Loss: 9.4734e-08 | Loss_d: 2.8857e-10 | Loss_e: 3.1482e-07\n",
      " Loss: 9.2692e-08 | Loss_d: 2.2116e-10 | Loss_e: 3.0823e-07\n",
      " Loss: 8.6189e-08 | Loss_d: 1.3220e-11 | Loss_e: 2.8725e-07\n",
      " Loss: 2.5129e-07 | Loss_d: 2.1203e-08 | Loss_e: 7.6695e-07\n",
      " Loss: 8.5273e-08 | Loss_d: 7.4696e-13 | Loss_e: 2.8424e-07\n",
      " Loss: 8.4688e-08 | Loss_d: 2.1615e-11 | Loss_e: 2.8222e-07\n",
      " Loss: 7.9005e-08 | Loss_d: 4.3521e-12 | Loss_e: 2.6333e-07\n",
      " Loss: 8.0247e-08 | Loss_d: 1.2641e-09 | Loss_e: 2.6328e-07\n",
      " Loss: 7.7689e-08 | Loss_d: 8.6459e-11 | Loss_e: 2.5868e-07\n",
      " Loss: 8.9875e-08 | Loss_d: 1.6622e-09 | Loss_e: 2.9404e-07\n",
      " Loss: 7.7177e-08 | Loss_d: 1.4069e-10 | Loss_e: 2.5679e-07\n",
      " Loss: 8.2027e-08 | Loss_d: 7.0353e-10 | Loss_e: 2.7108e-07\n",
      " Loss: 7.6538e-08 | Loss_d: 1.6118e-10 | Loss_e: 2.5459e-07\n",
      " Loss: 7.8071e-08 | Loss_d: 4.9164e-10 | Loss_e: 2.5860e-07\n",
      " Loss: 7.5890e-08 | Loss_d: 1.9620e-10 | Loss_e: 2.5231e-07\n",
      " Loss: 7.5439e-08 | Loss_d: 9.6137e-11 | Loss_e: 2.5114e-07\n",
      " Loss: 7.5133e-08 | Loss_d: 1.4282e-10 | Loss_e: 2.4997e-07\n",
      " Loss: 7.4428e-08 | Loss_d: 3.0070e-11 | Loss_e: 2.4799e-07\n",
      " Loss: 9.1236e-08 | Loss_d: 1.6428e-11 | Loss_e: 3.0406e-07\n",
      " Loss: 7.4429e-08 | Loss_d: 2.8458e-11 | Loss_e: 2.4800e-07\n",
      " Loss: 7.5049e-08 | Loss_d: 1.1787e-09 | Loss_e: 2.4624e-07\n",
      " Loss: 7.3785e-08 | Loss_d: 1.3234e-10 | Loss_e: 2.4551e-07\n",
      " Loss: 7.3259e-08 | Loss_d: 8.5906e-11 | Loss_e: 2.4391e-07\n",
      " Loss: 7.6582e-08 | Loss_d: 9.6723e-13 | Loss_e: 2.5527e-07\n",
      " Loss: 7.3207e-08 | Loss_d: 7.4182e-11 | Loss_e: 2.4377e-07\n",
      " Loss: 7.5678e-08 | Loss_d: 9.1863e-10 | Loss_e: 2.4920e-07\n",
      " Loss: 7.2561e-08 | Loss_d: 2.2204e-12 | Loss_e: 2.4186e-07\n",
      " Loss: 7.1580e-08 | Loss_d: 6.9633e-11 | Loss_e: 2.3837e-07\n",
      " Loss: 1.0196e-07 | Loss_d: 2.2144e-09 | Loss_e: 3.3248e-07\n",
      " Loss: 7.1543e-08 | Loss_d: 7.9936e-11 | Loss_e: 2.3821e-07\n",
      " Loss: 7.0177e-08 | Loss_d: 2.1151e-10 | Loss_e: 2.3322e-07\n",
      " Loss: 8.2854e-08 | Loss_d: 2.6245e-09 | Loss_e: 2.6743e-07\n",
      " Loss: 6.9982e-08 | Loss_d: 2.6381e-10 | Loss_e: 2.3239e-07\n",
      " Loss: 6.9321e-08 | Loss_d: 4.6985e-11 | Loss_e: 2.3091e-07\n",
      " Loss: 6.9102e-08 | Loss_d: 1.8794e-12 | Loss_e: 2.3033e-07\n",
      " Loss: 6.8581e-08 | Loss_d: 3.4142e-12 | Loss_e: 2.2859e-07\n",
      " Loss: 7.0140e-08 | Loss_d: 9.3237e-11 | Loss_e: 2.3349e-07\n",
      " Loss: 6.8473e-08 | Loss_d: 1.3878e-11 | Loss_e: 2.2820e-07\n",
      " Loss: 7.1157e-08 | Loss_d: 2.6867e-10 | Loss_e: 2.3629e-07\n",
      " Loss: 6.8101e-08 | Loss_d: 1.9455e-11 | Loss_e: 2.2694e-07\n",
      " Loss: 7.1309e-08 | Loss_d: 1.6576e-10 | Loss_e: 2.3714e-07\n",
      " Loss: 6.7757e-08 | Loss_d: 1.8674e-11 | Loss_e: 2.2579e-07\n",
      " Loss: 6.8064e-08 | Loss_d: 4.0675e-11 | Loss_e: 2.2675e-07\n",
      " Loss: 6.7319e-08 | Loss_d: 1.6670e-11 | Loss_e: 2.2434e-07\n",
      " Loss: 6.6861e-08 | Loss_d: 1.7195e-12 | Loss_e: 2.2286e-07\n",
      " Loss: 6.6862e-08 | Loss_d: 2.1325e-12 | Loss_e: 2.2287e-07\n",
      " Loss: 6.6694e-08 | Loss_d: 3.5883e-11 | Loss_e: 2.2219e-07\n",
      " Loss: 6.6672e-08 | Loss_d: 2.4770e-11 | Loss_e: 2.2216e-07\n",
      " Loss: 6.6376e-08 | Loss_d: 1.6914e-11 | Loss_e: 2.2120e-07\n",
      " Loss: 6.6364e-08 | Loss_d: 1.2577e-11 | Loss_e: 2.2117e-07\n",
      " Loss: 6.6002e-08 | Loss_d: 7.9936e-15 | Loss_e: 2.2001e-07\n",
      " Loss: 7.5083e-08 | Loss_d: 8.8996e-10 | Loss_e: 2.4731e-07\n",
      " Loss: 6.5982e-08 | Loss_d: 4.2988e-13 | Loss_e: 2.1994e-07\n",
      " Loss: 6.5657e-08 | Loss_d: 1.9718e-11 | Loss_e: 2.1879e-07\n",
      " Loss: 7.8193e-08 | Loss_d: 1.2875e-09 | Loss_e: 2.5635e-07\n",
      " Loss: 6.5654e-08 | Loss_d: 2.0791e-11 | Loss_e: 2.1878e-07\n",
      " Loss: 6.5383e-08 | Loss_d: 1.7160e-11 | Loss_e: 2.1789e-07\n",
      " Loss: 6.5383e-08 | Loss_d: 1.9718e-11 | Loss_e: 2.1788e-07\n",
      " Loss: 6.5231e-08 | Loss_d: 5.6843e-14 | Loss_e: 2.1743e-07\n",
      " Loss: 6.5228e-08 | Loss_d: 7.4696e-13 | Loss_e: 2.1743e-07\n",
      " Loss: 6.5324e-08 | Loss_d: 1.6961e-10 | Loss_e: 2.1718e-07\n",
      " Loss: 6.5161e-08 | Loss_d: 2.6276e-11 | Loss_e: 2.1712e-07\n",
      " Loss: 6.5054e-08 | Loss_d: 1.0360e-11 | Loss_e: 2.1681e-07\n",
      " Loss: 6.5373e-08 | Loss_d: 1.3220e-11 | Loss_e: 2.1787e-07\n",
      " Loss: 6.5030e-08 | Loss_d: 2.5899e-12 | Loss_e: 2.1676e-07\n",
      " Loss: 6.4871e-08 | Loss_d: 2.7853e-12 | Loss_e: 2.1623e-07\n",
      " Loss: 7.0312e-08 | Loss_d: 1.5297e-10 | Loss_e: 2.3386e-07\n",
      " Loss: 6.4868e-08 | Loss_d: 3.8689e-12 | Loss_e: 2.1621e-07\n",
      " Loss: 6.4675e-08 | Loss_d: 9.0603e-12 | Loss_e: 2.1555e-07\n",
      " Loss: 6.7207e-08 | Loss_d: 2.6381e-10 | Loss_e: 2.2315e-07\n",
      " Loss: 6.4657e-08 | Loss_d: 1.5476e-11 | Loss_e: 2.1547e-07\n",
      " Loss: 6.4285e-08 | Loss_d: 7.6819e-12 | Loss_e: 2.1426e-07\n",
      " Loss: 6.5437e-08 | Loss_d: 3.9169e-13 | Loss_e: 2.1812e-07\n",
      " Loss: 6.4210e-08 | Loss_d: 1.8794e-12 | Loss_e: 2.1403e-07\n",
      " Loss: 6.5573e-08 | Loss_d: 3.5527e-11 | Loss_e: 2.1846e-07\n",
      " Loss: 6.3917e-08 | Loss_d: 1.5010e-13 | Loss_e: 2.1306e-07\n",
      " Loss: 1.1033e-07 | Loss_d: 7.6159e-10 | Loss_e: 3.6522e-07\n",
      " Loss: 6.3825e-08 | Loss_d: 1.7408e-13 | Loss_e: 2.1275e-07\n",
      " Loss: 7.8499e-08 | Loss_d: 2.6965e-10 | Loss_e: 2.6076e-07\n",
      " Loss: 6.4298e-08 | Loss_d: 2.5068e-11 | Loss_e: 2.1424e-07\n",
      " Loss: 6.3494e-08 | Loss_d: 2.5668e-13 | Loss_e: 2.1165e-07\n",
      " Loss: 6.6922e-08 | Loss_d: 1.1341e-11 | Loss_e: 2.2304e-07\n",
      " Loss: 6.3000e-08 | Loss_d: 6.4748e-13 | Loss_e: 2.1000e-07\n",
      " Loss: 6.1954e-08 | Loss_d: 8.7050e-12 | Loss_e: 2.0648e-07\n",
      " Loss: 6.1692e-08 | Loss_d: 2.9878e-12 | Loss_e: 2.0563e-07\n",
      " Loss: 6.0523e-08 | Loss_d: 5.0310e-11 | Loss_e: 2.0157e-07\n",
      " Loss: 6.0511e-08 | Loss_d: 3.9543e-11 | Loss_e: 2.0157e-07\n",
      " Loss: 5.9409e-08 | Loss_d: 4.2210e-11 | Loss_e: 1.9789e-07\n",
      " Loss: 5.9404e-08 | Loss_d: 3.5883e-11 | Loss_e: 1.9789e-07\n",
      " Loss: 5.8683e-08 | Loss_d: 1.7160e-11 | Loss_e: 1.9555e-07\n",
      " Loss: 5.8681e-08 | Loss_d: 1.5010e-11 | Loss_e: 1.9555e-07\n",
      " Loss: 6.7826e-08 | Loss_d: 5.4980e-09 | Loss_e: 2.0776e-07\n",
      " Loss: 5.8524e-08 | Loss_d: 1.1543e-11 | Loss_e: 1.9504e-07\n",
      " Loss: 5.8374e-08 | Loss_d: 4.9050e-11 | Loss_e: 1.9442e-07\n",
      " Loss: 5.9986e-08 | Loss_d: 4.1069e-10 | Loss_e: 1.9858e-07\n",
      " Loss: 5.8354e-08 | Loss_d: 5.5068e-11 | Loss_e: 1.9433e-07\n",
      " Loss: 5.8218e-08 | Loss_d: 6.3793e-11 | Loss_e: 1.9385e-07\n",
      " Loss: 5.7972e-08 | Loss_d: 6.0041e-11 | Loss_e: 1.9304e-07\n",
      " Loss: 5.7975e-08 | Loss_d: 6.0041e-11 | Loss_e: 1.9305e-07\n",
      " Loss: 5.7584e-08 | Loss_d: 1.5010e-11 | Loss_e: 1.9189e-07\n",
      " Loss: 5.7227e-08 | Loss_d: 1.5223e-10 | Loss_e: 1.9025e-07\n",
      " Loss: 5.7098e-08 | Loss_d: 2.8777e-11 | Loss_e: 1.9023e-07\n",
      " Loss: 5.6893e-08 | Loss_d: 1.7909e-11 | Loss_e: 1.8958e-07\n",
      " Loss: 5.6820e-08 | Loss_d: 2.1064e-11 | Loss_e: 1.8933e-07\n",
      " Loss: 5.6878e-08 | Loss_d: 7.1637e-11 | Loss_e: 1.8935e-07\n",
      " Loss: 5.6597e-08 | Loss_d: 2.2737e-13 | Loss_e: 1.8866e-07\n",
      " Loss: 5.6229e-08 | Loss_d: 1.7195e-12 | Loss_e: 1.8742e-07\n",
      " Loss: 7.6476e-08 | Loss_d: 4.3521e-12 | Loss_e: 2.5491e-07\n",
      " Loss: 5.6225e-08 | Loss_d: 1.2825e-12 | Loss_e: 1.8741e-07\n",
      " Loss: 5.5510e-08 | Loss_d: 9.2406e-12 | Loss_e: 1.8500e-07\n",
      " Loss: 5.5810e-08 | Loss_d: 4.4565e-11 | Loss_e: 1.8589e-07\n",
      " Loss: 5.5052e-08 | Loss_d: 2.1064e-11 | Loss_e: 1.8344e-07\n",
      " Loss: 5.4796e-08 | Loss_d: 1.2825e-12 | Loss_e: 1.8265e-07\n",
      " Loss: 5.7038e-08 | Loss_d: 1.7039e-10 | Loss_e: 1.8956e-07\n",
      " Loss: 5.4777e-08 | Loss_d: 2.2737e-13 | Loss_e: 1.8259e-07\n",
      " Loss: 5.4786e-08 | Loss_d: 4.8637e-12 | Loss_e: 1.8260e-07\n",
      " Loss: 5.4748e-08 | Loss_d: 6.9633e-13 | Loss_e: 1.8249e-07\n",
      " Loss: 5.4692e-08 | Loss_d: 1.0747e-11 | Loss_e: 1.8227e-07\n",
      " Loss: 5.4794e-08 | Loss_d: 1.2691e-10 | Loss_e: 1.8222e-07\n",
      " Loss: 5.4658e-08 | Loss_d: 2.8777e-11 | Loss_e: 1.8210e-07\n",
      " Loss: 5.4586e-08 | Loss_d: 8.8818e-12 | Loss_e: 1.8192e-07\n",
      " Loss: 5.4887e-08 | Loss_d: 3.8426e-11 | Loss_e: 1.8283e-07\n",
      " Loss: 5.4569e-08 | Loss_d: 2.0464e-12 | Loss_e: 1.8189e-07\n",
      " Loss: 5.4549e-08 | Loss_d: 1.7195e-12 | Loss_e: 1.8182e-07\n",
      " Loss: 5.4767e-08 | Loss_d: 3.1974e-14 | Loss_e: 1.8256e-07\n",
      " Loss: 5.4546e-08 | Loss_d: 1.5667e-12 | Loss_e: 1.8182e-07\n",
      " Loss: 5.4504e-08 | Loss_d: 1.8794e-12 | Loss_e: 1.8167e-07\n",
      " Loss: 5.4597e-08 | Loss_d: 1.5667e-12 | Loss_e: 1.8198e-07\n",
      " Loss: 5.4490e-08 | Loss_d: 2.0464e-12 | Loss_e: 1.8163e-07\n",
      " Loss: 5.4464e-08 | Loss_d: 3.6380e-12 | Loss_e: 1.8153e-07\n",
      " Loss: 5.4460e-08 | Loss_d: 1.5667e-12 | Loss_e: 1.8153e-07\n",
      " Loss: 5.4441e-08 | Loss_d: 3.5527e-15 | Loss_e: 1.8147e-07\n",
      " Loss: 5.4731e-08 | Loss_d: 4.1439e-11 | Loss_e: 1.8230e-07\n",
      " Loss: 5.4441e-08 | Loss_d: 4.2988e-13 | Loss_e: 1.8147e-07\n",
      " Loss: 5.4423e-08 | Loss_d: 3.1974e-14 | Loss_e: 1.8141e-07\n",
      " Loss: 5.4364e-08 | Loss_d: 1.2790e-13 | Loss_e: 1.8121e-07\n",
      " Loss: 5.4662e-08 | Loss_d: 1.1543e-11 | Loss_e: 1.8217e-07\n",
      " Loss: 5.4346e-08 | Loss_d: 1.0267e-12 | Loss_e: 1.8115e-07\n",
      " Loss: 5.4235e-08 | Loss_d: 1.2790e-13 | Loss_e: 1.8078e-07\n",
      " Loss: 5.3922e-08 | Loss_d: 3.1974e-12 | Loss_e: 1.7973e-07\n",
      " Loss: 6.7961e-08 | Loss_d: 6.0892e-10 | Loss_e: 2.2451e-07\n",
      " Loss: 5.3908e-08 | Loss_d: 5.6843e-12 | Loss_e: 1.7967e-07\n",
      " Loss: 5.3531e-08 | Loss_d: 9.2406e-12 | Loss_e: 1.7841e-07\n",
      " Loss: 5.3532e-08 | Loss_d: 7.5175e-12 | Loss_e: 1.7841e-07\n",
      " Loss: 9.4262e-08 | Loss_d: 1.6361e-08 | Loss_e: 2.5967e-07\n",
      " Loss: 5.3314e-08 | Loss_d: 1.3371e-10 | Loss_e: 1.7727e-07\n",
      " Loss: 5.2927e-08 | Loss_d: 9.6065e-12 | Loss_e: 1.7639e-07\n",
      " Loss: 5.2905e-08 | Loss_d: 1.9984e-11 | Loss_e: 1.7628e-07\n",
      " Loss: 5.2413e-08 | Loss_d: 2.2204e-12 | Loss_e: 1.7470e-07\n",
      " Loss: 7.3196e-08 | Loss_d: 1.1543e-11 | Loss_e: 2.4395e-07\n",
      " Loss: 5.2399e-08 | Loss_d: 1.2825e-12 | Loss_e: 1.7466e-07\n",
      " Loss: 5.2140e-08 | Loss_d: 1.1951e-11 | Loss_e: 1.7376e-07\n",
      " Loss: 5.2118e-08 | Loss_d: 7.8479e-12 | Loss_e: 1.7370e-07\n",
      " Loss: 5.1485e-08 | Loss_d: 1.2367e-11 | Loss_e: 1.7158e-07\n",
      " Loss: 5.5111e-08 | Loss_d: 4.7072e-10 | Loss_e: 1.8213e-07\n",
      " Loss: 5.1425e-08 | Loss_d: 2.5668e-11 | Loss_e: 1.7133e-07\n",
      " Loss: 5.2098e-08 | Loss_d: 1.6914e-11 | Loss_e: 1.7360e-07\n",
      " Loss: 5.1239e-08 | Loss_d: 1.4552e-11 | Loss_e: 1.7075e-07\n",
      " Loss: 5.2651e-08 | Loss_d: 7.8146e-10 | Loss_e: 1.7290e-07\n",
      " Loss: 5.0875e-08 | Loss_d: 5.6843e-12 | Loss_e: 1.6956e-07\n",
      " Loss: 5.0472e-08 | Loss_d: 5.0310e-11 | Loss_e: 1.6807e-07\n",
      " Loss: 7.0331e-08 | Loss_d: 2.1228e-09 | Loss_e: 2.2736e-07\n",
      " Loss: 5.0461e-08 | Loss_d: 4.7805e-11 | Loss_e: 1.6804e-07\n",
      " Loss: 5.0699e-08 | Loss_d: 1.6118e-10 | Loss_e: 1.6846e-07\n",
      " Loss: 5.0309e-08 | Loss_d: 8.1005e-11 | Loss_e: 1.6743e-07\n",
      " Loss: 4.9895e-08 | Loss_d: 8.8818e-12 | Loss_e: 1.6629e-07\n",
      " Loss: 5.5675e-08 | Loss_d: 1.3371e-10 | Loss_e: 1.8514e-07\n",
      " Loss: 4.9883e-08 | Loss_d: 3.8689e-12 | Loss_e: 1.6626e-07\n",
      " Loss: 4.9515e-08 | Loss_d: 5.6843e-12 | Loss_e: 1.6503e-07\n",
      " Loss: 5.0224e-08 | Loss_d: 3.5527e-11 | Loss_e: 1.6729e-07\n",
      " Loss: 4.9440e-08 | Loss_d: 6.5690e-12 | Loss_e: 1.6478e-07\n",
      " Loss: 1.2298e-07 | Loss_d: 7.4523e-10 | Loss_e: 4.0744e-07\n",
      " Loss: 4.9393e-08 | Loss_d: 8.1855e-12 | Loss_e: 1.6462e-07\n",
      " Loss: 4.9899e-08 | Loss_d: 4.3773e-11 | Loss_e: 1.6618e-07\n",
      " Loss: 4.9205e-08 | Loss_d: 9.6065e-12 | Loss_e: 1.6399e-07\n",
      " Loss: 4.9955e-08 | Loss_d: 3.0727e-11 | Loss_e: 1.6641e-07\n",
      " Loss: 4.8899e-08 | Loss_d: 1.2825e-12 | Loss_e: 1.6299e-07\n",
      " Loss: 6.0742e-08 | Loss_d: 5.6135e-09 | Loss_e: 1.8376e-07\n",
      " Loss: 4.8827e-08 | Loss_d: 2.3309e-11 | Loss_e: 1.6268e-07\n",
      " Loss: 4.8475e-08 | Loss_d: 2.4016e-12 | Loss_e: 1.6158e-07\n",
      " Loss: 4.8460e-08 | Loss_d: 8.8818e-14 | Loss_e: 1.6153e-07\n",
      " Loss: 4.8305e-08 | Loss_d: 9.0949e-13 | Loss_e: 1.6101e-07\n",
      " Loss: 4.8243e-08 | Loss_d: 4.3773e-11 | Loss_e: 1.6066e-07\n",
      " Loss: 4.8176e-08 | Loss_d: 1.8417e-11 | Loss_e: 1.6052e-07\n",
      " Loss: 4.9271e-08 | Loss_d: 2.8655e-10 | Loss_e: 1.6328e-07\n",
      " Loss: 4.7991e-08 | Loss_d: 3.4120e-11 | Loss_e: 1.5986e-07\n",
      " Loss: 9.5482e-08 | Loss_d: 1.1422e-09 | Loss_e: 3.1446e-07\n",
      " Loss: 4.7949e-08 | Loss_d: 3.7691e-11 | Loss_e: 1.5971e-07\n",
      " Loss: 5.4411e-08 | Loss_d: 3.6836e-10 | Loss_e: 1.8014e-07\n",
      " Loss: 4.7861e-08 | Loss_d: 4.3773e-11 | Loss_e: 1.5939e-07\n",
      " Loss: 5.2729e-08 | Loss_d: 2.0124e-10 | Loss_e: 1.7509e-07\n",
      " Loss: 4.7752e-08 | Loss_d: 4.2988e-11 | Loss_e: 1.5903e-07\n",
      " Loss: 4.9091e-08 | Loss_d: 1.3097e-10 | Loss_e: 1.6320e-07\n",
      " Loss: 4.7577e-08 | Loss_d: 5.0310e-11 | Loss_e: 1.5842e-07\n",
      " Loss: 4.8500e-08 | Loss_d: 1.6884e-10 | Loss_e: 1.6110e-07\n",
      " Loss: 4.7389e-08 | Loss_d: 2.2204e-12 | Loss_e: 1.5796e-07\n",
      " Loss: 4.7252e-08 | Loss_d: 1.0360e-11 | Loss_e: 1.5747e-07\n",
      " Loss: 4.7227e-08 | Loss_d: 7.1942e-12 | Loss_e: 1.5740e-07\n",
      " Loss: 4.7061e-08 | Loss_d: 9.0949e-13 | Loss_e: 1.5687e-07\n",
      " Loss: 4.8582e-08 | Loss_d: 3.0070e-11 | Loss_e: 1.6184e-07\n",
      " Loss: 4.7051e-08 | Loss_d: 4.2988e-13 | Loss_e: 1.5683e-07\n",
      " Loss: 4.6784e-08 | Loss_d: 1.1511e-12 | Loss_e: 1.5594e-07\n",
      " Loss: 4.9049e-08 | Loss_d: 2.6890e-11 | Loss_e: 1.6341e-07\n",
      " Loss: 4.6715e-08 | Loss_d: 2.4016e-12 | Loss_e: 1.5571e-07\n",
      " Loss: 4.8199e-08 | Loss_d: 1.5996e-09 | Loss_e: 1.5533e-07\n",
      " Loss: 4.6602e-08 | Loss_d: 6.9633e-11 | Loss_e: 1.5511e-07\n",
      " Loss: 4.6079e-08 | Loss_d: 1.7408e-11 | Loss_e: 1.5354e-07\n",
      " Loss: 6.3312e-08 | Loss_d: 1.0388e-10 | Loss_e: 2.1069e-07\n",
      " Loss: 4.6073e-08 | Loss_d: 1.8417e-11 | Loss_e: 1.5351e-07\n",
      " Loss: 4.6090e-08 | Loss_d: 1.3220e-11 | Loss_e: 1.5359e-07\n",
      " Loss: 4.5864e-08 | Loss_d: 1.5476e-11 | Loss_e: 1.5283e-07\n",
      " Loss: 4.5323e-08 | Loss_d: 1.5667e-12 | Loss_e: 1.5107e-07\n",
      " Loss: 1.1053e-07 | Loss_d: 2.0464e-12 | Loss_e: 3.6844e-07\n",
      " Loss: 4.5319e-08 | Loss_d: 1.2825e-12 | Loss_e: 1.5106e-07\n",
      " Loss: 4.5015e-08 | Loss_d: 3.1974e-14 | Loss_e: 1.5005e-07\n",
      " Loss: 4.6001e-08 | Loss_d: 1.1543e-11 | Loss_e: 1.5330e-07\n",
      " Loss: 4.4919e-08 | Loss_d: 1.2825e-12 | Loss_e: 1.4973e-07\n",
      " Loss: 4.4766e-08 | Loss_d: 1.4211e-12 | Loss_e: 1.4921e-07\n",
      " Loss: 4.5615e-08 | Loss_d: 2.8777e-13 | Loss_e: 1.5205e-07\n",
      " Loss: 4.4755e-08 | Loss_d: 1.1511e-12 | Loss_e: 1.4918e-07\n",
      " Loss: 4.5398e-08 | Loss_d: 3.7691e-11 | Loss_e: 1.5120e-07\n",
      " Loss: 4.4629e-08 | Loss_d: 0.0000e+00 | Loss_e: 1.4876e-07\n",
      " Loss: 4.7664e-08 | Loss_d: 4.2988e-11 | Loss_e: 1.5874e-07\n",
      " Loss: 4.4575e-08 | Loss_d: 1.2790e-13 | Loss_e: 1.4858e-07\n",
      " Loss: 4.5054e-08 | Loss_d: 9.2406e-12 | Loss_e: 1.5015e-07\n",
      " Loss: 4.4504e-08 | Loss_d: 2.2737e-13 | Loss_e: 1.4835e-07\n",
      " Loss: 4.4548e-08 | Loss_d: 5.1301e-12 | Loss_e: 1.4848e-07\n",
      " Loss: 4.4432e-08 | Loss_d: 7.9936e-13 | Loss_e: 1.4810e-07\n",
      " Loss: 4.4371e-08 | Loss_d: 6.9633e-13 | Loss_e: 1.4790e-07\n",
      " Loss: 4.4369e-08 | Loss_d: 4.2988e-13 | Loss_e: 1.4790e-07\n",
      " Loss: 4.4331e-08 | Loss_d: 9.6065e-12 | Loss_e: 1.4774e-07\n",
      " Loss: 4.4807e-08 | Loss_d: 4.1311e-10 | Loss_e: 1.4798e-07\n",
      " Loss: 4.4335e-08 | Loss_d: 1.8417e-11 | Loss_e: 1.4772e-07\n",
      " Loss: 4.4266e-08 | Loss_d: 2.9878e-12 | Loss_e: 1.4754e-07\n",
      " Loss: 4.4438e-08 | Loss_d: 9.4392e-11 | Loss_e: 1.4781e-07\n",
      " Loss: 4.4243e-08 | Loss_d: 6.9633e-13 | Loss_e: 1.4747e-07\n",
      " Loss: 4.4236e-08 | Loss_d: 5.6843e-14 | Loss_e: 1.4745e-07\n",
      " Loss: 4.4236e-08 | Loss_d: 1.4211e-14 | Loss_e: 1.4745e-07\n",
      " Loss: 4.4230e-08 | Loss_d: 3.5527e-13 | Loss_e: 1.4743e-07\n",
      " Loss: 4.4265e-08 | Loss_d: 1.6914e-11 | Loss_e: 1.4749e-07\n",
      " Loss: 4.4230e-08 | Loss_d: 1.0267e-12 | Loss_e: 1.4743e-07\n",
      " Loss: 4.4226e-08 | Loss_d: 6.9633e-13 | Loss_e: 1.4742e-07\n",
      " Loss: 4.4219e-08 | Loss_d: 2.8777e-13 | Loss_e: 1.4740e-07\n",
      " Loss: 4.4372e-08 | Loss_d: 8.8818e-12 | Loss_e: 1.4788e-07\n",
      " Loss: 4.4221e-08 | Loss_d: 0.0000e+00 | Loss_e: 1.4740e-07\n",
      " Loss: 4.4214e-08 | Loss_d: 1.2790e-13 | Loss_e: 1.4738e-07\n",
      " Loss: 4.4191e-08 | Loss_d: 4.2988e-13 | Loss_e: 1.4730e-07\n",
      " Loss: 4.4305e-08 | Loss_d: 3.6380e-12 | Loss_e: 1.4767e-07\n",
      " Loss: 4.4182e-08 | Loss_d: 7.9936e-13 | Loss_e: 1.4727e-07\n",
      " Loss: 4.4135e-08 | Loss_d: 3.5527e-13 | Loss_e: 1.4712e-07\n",
      " Loss: 4.4042e-08 | Loss_d: 4.3773e-11 | Loss_e: 1.4666e-07\n",
      " Loss: 4.5917e-08 | Loss_d: 1.3481e-09 | Loss_e: 1.4856e-07\n",
      " Loss: 4.4050e-08 | Loss_d: 6.0041e-11 | Loss_e: 1.4663e-07\n",
      " Loss: 4.4000e-08 | Loss_d: 2.4475e-11 | Loss_e: 1.4658e-07\n",
      " Loss: 4.4201e-08 | Loss_d: 2.3309e-11 | Loss_e: 1.4726e-07\n",
      " Loss: 4.3995e-08 | Loss_d: 1.3657e-11 | Loss_e: 1.4660e-07\n",
      " Loss: 4.3948e-08 | Loss_d: 6.5690e-12 | Loss_e: 1.4647e-07\n",
      " Loss: 4.3868e-08 | Loss_d: 9.0949e-13 | Loss_e: 1.4623e-07\n",
      " Loss: 4.5122e-08 | Loss_d: 3.3049e-10 | Loss_e: 1.4931e-07\n",
      " Loss: 4.3872e-08 | Loss_d: 3.1974e-12 | Loss_e: 1.4623e-07\n",
      " Loss: 4.3691e-08 | Loss_d: 3.5527e-13 | Loss_e: 1.4563e-07\n",
      " Loss: 4.3633e-08 | Loss_d: 1.0147e-10 | Loss_e: 1.4511e-07\n",
      " Loss: 4.3528e-08 | Loss_d: 3.4120e-11 | Loss_e: 1.4498e-07\n",
      " Loss: 4.3491e-08 | Loss_d: 2.1615e-11 | Loss_e: 1.4490e-07\n",
      " Loss: 4.3507e-08 | Loss_d: 6.0041e-13 | Loss_e: 1.4502e-07\n",
      " Loss: 4.3471e-08 | Loss_d: 1.1543e-11 | Loss_e: 1.4486e-07\n",
      " Loss: 4.3211e-08 | Loss_d: 2.2737e-11 | Loss_e: 1.4396e-07\n",
      " Loss: 5.2340e-08 | Loss_d: 2.0293e-10 | Loss_e: 1.7379e-07\n",
      " Loss: 4.3212e-08 | Loss_d: 2.3888e-11 | Loss_e: 1.4396e-07\n",
      " Loss: 4.2815e-08 | Loss_d: 9.6065e-12 | Loss_e: 1.4269e-07\n",
      " Loss: 4.2058e-08 | Loss_d: 2.2204e-12 | Loss_e: 1.4019e-07\n",
      " Loss: 1.4773e-07 | Loss_d: 1.8264e-09 | Loss_e: 4.8635e-07\n",
      " Loss: 4.2051e-08 | Loss_d: 2.5899e-12 | Loss_e: 1.4016e-07\n",
      " Loss: 8.0969e-07 | Loss_d: 5.6045e-09 | Loss_e: 2.6803e-06\n",
      " Loss: 4.2037e-08 | Loss_d: 2.5899e-12 | Loss_e: 1.4011e-07\n",
      " Loss: 4.1385e-08 | Loss_d: 2.8777e-13 | Loss_e: 1.3795e-07\n",
      " Loss: 9.1651e-08 | Loss_d: 2.7853e-12 | Loss_e: 3.0549e-07\n",
      " Loss: 4.1384e-08 | Loss_d: 3.5527e-13 | Loss_e: 1.3795e-07\n",
      " Loss: 4.1290e-08 | Loss_d: 1.5476e-11 | Loss_e: 1.3758e-07\n",
      " Loss: 4.1126e-08 | Loss_d: 7.8479e-12 | Loss_e: 1.3706e-07\n",
      " Loss: 4.0787e-08 | Loss_d: 2.9878e-12 | Loss_e: 1.3595e-07\n",
      " Loss: 4.7497e-08 | Loss_d: 1.5010e-11 | Loss_e: 1.5827e-07\n",
      " Loss: 4.0760e-08 | Loss_d: 2.0464e-12 | Loss_e: 1.3586e-07\n",
      " Loss: 4.0319e-08 | Loss_d: 1.7408e-11 | Loss_e: 1.3434e-07\n",
      " Loss: 4.0308e-08 | Loss_d: 1.6428e-11 | Loss_e: 1.3431e-07\n",
      " Loss: 7.6231e-08 | Loss_d: 2.3156e-08 | Loss_e: 1.7692e-07\n",
      " Loss: 4.0168e-08 | Loss_d: 7.1942e-12 | Loss_e: 1.3387e-07\n",
      " Loss: 4.1376e-08 | Loss_d: 6.3793e-11 | Loss_e: 1.3771e-07\n",
      " Loss: 3.9978e-08 | Loss_d: 1.1543e-11 | Loss_e: 1.3322e-07\n",
      " Loss: 4.2873e-08 | Loss_d: 1.6428e-11 | Loss_e: 1.4286e-07\n",
      " Loss: 3.9753e-08 | Loss_d: 1.7408e-11 | Loss_e: 1.3245e-07\n",
      " Loss: 9.8301e-08 | Loss_d: 1.0093e-09 | Loss_e: 3.2430e-07\n",
      " Loss: 3.9708e-08 | Loss_d: 2.1064e-11 | Loss_e: 1.3229e-07\n",
      " Loss: 3.9742e-08 | Loss_d: 2.4016e-12 | Loss_e: 1.3247e-07\n",
      " Loss: 3.9334e-08 | Loss_d: 1.8417e-11 | Loss_e: 1.3105e-07\n",
      " Loss: 3.9040e-08 | Loss_d: 2.7512e-11 | Loss_e: 1.3004e-07\n",
      " Loss: 3.9013e-08 | Loss_d: 2.5068e-11 | Loss_e: 1.2996e-07\n",
      " Loss: 3.8784e-08 | Loss_d: 4.2988e-13 | Loss_e: 1.2928e-07\n",
      " Loss: 3.8784e-08 | Loss_d: 5.1159e-13 | Loss_e: 1.2928e-07\n",
      " Loss: 3.8511e-08 | Loss_d: 3.5527e-15 | Loss_e: 1.2837e-07\n",
      " Loss: 3.7818e-08 | Loss_d: 1.3657e-11 | Loss_e: 1.2602e-07\n",
      " Loss: 5.6970e-08 | Loss_d: 3.1550e-10 | Loss_e: 1.8885e-07\n",
      " Loss: 3.7737e-08 | Loss_d: 1.7408e-11 | Loss_e: 1.2573e-07\n",
      " Loss: 6.7732e-07 | Loss_d: 2.8905e-09 | Loss_e: 2.2481e-06\n",
      " Loss: 3.7715e-08 | Loss_d: 1.8932e-11 | Loss_e: 1.2565e-07\n",
      " Loss: 3.8437e-08 | Loss_d: 2.4475e-11 | Loss_e: 1.2804e-07\n",
      " Loss: 3.7503e-08 | Loss_d: 1.7909e-11 | Loss_e: 1.2495e-07\n",
      " Loss: 4.0272e-08 | Loss_d: 2.1064e-11 | Loss_e: 1.3417e-07\n",
      " Loss: 3.7353e-08 | Loss_d: 1.1543e-11 | Loss_e: 1.2447e-07\n",
      " Loss: 3.7460e-08 | Loss_d: 5.6843e-14 | Loss_e: 1.2487e-07\n",
      " Loss: 3.7165e-08 | Loss_d: 6.2670e-12 | Loss_e: 1.2386e-07\n",
      " Loss: 3.9239e-08 | Loss_d: 1.9824e-09 | Loss_e: 1.2419e-07\n",
      " Loss: 3.7105e-08 | Loss_d: 1.5948e-11 | Loss_e: 1.2363e-07\n",
      " Loss: 3.6905e-08 | Loss_d: 1.0747e-11 | Loss_e: 1.2298e-07\n",
      " Loss: 3.6898e-08 | Loss_d: 7.5175e-12 | Loss_e: 1.2297e-07\n",
      " Loss: 3.6623e-08 | Loss_d: 3.1974e-12 | Loss_e: 1.2207e-07\n",
      " Loss: 3.6796e-08 | Loss_d: 1.6914e-11 | Loss_e: 1.2260e-07\n",
      " Loss: 3.6494e-08 | Loss_d: 3.5527e-13 | Loss_e: 1.2165e-07\n",
      " Loss: 3.6174e-08 | Loss_d: 3.1974e-12 | Loss_e: 1.2057e-07\n",
      " Loss: 3.6091e-08 | Loss_d: 4.1069e-12 | Loss_e: 1.2029e-07\n",
      " Loss: 3.5952e-08 | Loss_d: 3.1974e-12 | Loss_e: 1.1983e-07\n",
      " Loss: 3.5685e-08 | Loss_d: 6.9633e-13 | Loss_e: 1.1895e-07\n",
      " Loss: 3.5363e-08 | Loss_d: 7.9936e-13 | Loss_e: 1.1788e-07\n",
      " Loss: 3.7639e-08 | Loss_d: 3.1974e-14 | Loss_e: 1.2546e-07\n",
      " Loss: 3.5345e-08 | Loss_d: 6.9633e-13 | Loss_e: 1.1781e-07\n",
      " Loss: 3.7363e-08 | Loss_d: 4.5279e-10 | Loss_e: 1.2303e-07\n",
      " Loss: 3.5169e-08 | Loss_d: 2.4016e-12 | Loss_e: 1.1722e-07\n",
      " Loss: 3.4785e-08 | Loss_d: 3.2063e-11 | Loss_e: 1.1584e-07\n",
      " Loss: 3.4769e-08 | Loss_d: 2.1064e-11 | Loss_e: 1.1583e-07\n",
      " Loss: 3.4500e-08 | Loss_d: 2.9420e-11 | Loss_e: 1.1490e-07\n",
      " Loss: 3.8109e-08 | Loss_d: 2.0635e-10 | Loss_e: 1.2634e-07\n",
      " Loss: 3.4501e-08 | Loss_d: 2.6890e-11 | Loss_e: 1.1491e-07\n",
      " Loss: 3.4222e-08 | Loss_d: 1.5667e-12 | Loss_e: 1.1407e-07\n",
      " Loss: 3.4177e-08 | Loss_d: 4.8637e-12 | Loss_e: 1.1391e-07\n",
      " Loss: 3.8818e-08 | Loss_d: 1.4552e-11 | Loss_e: 1.2934e-07\n",
      " Loss: 3.4073e-08 | Loss_d: 5.6843e-12 | Loss_e: 1.1356e-07\n",
      " Loss: 3.4836e-08 | Loss_d: 6.8468e-10 | Loss_e: 1.1384e-07\n",
      " Loss: 3.3766e-08 | Loss_d: 7.8874e-11 | Loss_e: 1.1229e-07\n",
      " Loss: 3.3247e-08 | Loss_d: 6.0041e-13 | Loss_e: 1.1082e-07\n",
      " Loss: 3.9374e-08 | Loss_d: 1.3004e-09 | Loss_e: 1.2691e-07\n",
      " Loss: 3.3250e-08 | Loss_d: 2.0464e-12 | Loss_e: 1.1083e-07\n",
      " Loss: 3.3048e-08 | Loss_d: 1.1543e-11 | Loss_e: 1.1012e-07\n",
      " Loss: 3.3006e-08 | Loss_d: 9.6065e-12 | Loss_e: 1.0999e-07\n",
      " Loss: 3.7335e-08 | Loss_d: 2.9291e-09 | Loss_e: 1.1469e-07\n",
      " Loss: 3.2861e-08 | Loss_d: 4.8637e-12 | Loss_e: 1.0952e-07\n",
      " Loss: 3.2508e-08 | Loss_d: 1.2367e-11 | Loss_e: 1.0832e-07\n",
      " Loss: 3.7468e-08 | Loss_d: 6.8781e-10 | Loss_e: 1.2260e-07\n",
      " Loss: 3.2485e-08 | Loss_d: 1.6428e-11 | Loss_e: 1.0823e-07\n",
      " Loss: 3.2058e-08 | Loss_d: 1.6914e-11 | Loss_e: 1.0680e-07\n",
      " Loss: 3.6524e-08 | Loss_d: 2.4387e-10 | Loss_e: 1.2093e-07\n",
      " Loss: 3.2021e-08 | Loss_d: 1.4552e-11 | Loss_e: 1.0669e-07\n",
      " Loss: 3.1701e-08 | Loss_d: 6.0041e-13 | Loss_e: 1.0567e-07\n",
      " Loss: 3.4191e-08 | Loss_d: 3.1762e-10 | Loss_e: 1.1291e-07\n",
      " Loss: 3.1699e-08 | Loss_d: 5.1301e-12 | Loss_e: 1.0565e-07\n",
      " Loss: 3.4688e-08 | Loss_d: 3.4142e-12 | Loss_e: 1.1562e-07\n",
      " Loss: 3.1615e-08 | Loss_d: 3.6380e-12 | Loss_e: 1.0537e-07\n",
      " Loss: 3.1662e-08 | Loss_d: 2.6276e-11 | Loss_e: 1.0545e-07\n",
      " Loss: 3.1443e-08 | Loss_d: 1.2790e-13 | Loss_e: 1.0481e-07\n",
      " Loss: 3.1355e-08 | Loss_d: 2.7512e-11 | Loss_e: 1.0442e-07\n",
      " Loss: 3.1329e-08 | Loss_d: 9.9796e-12 | Loss_e: 1.0440e-07\n",
      " Loss: 3.1263e-08 | Loss_d: 6.5690e-12 | Loss_e: 1.0419e-07\n",
      " Loss: 3.1661e-08 | Loss_d: 1.1543e-11 | Loss_e: 1.0550e-07\n",
      " Loss: 3.1262e-08 | Loss_d: 1.0747e-11 | Loss_e: 1.0417e-07\n",
      " Loss: 3.2221e-08 | Loss_d: 4.0675e-11 | Loss_e: 1.0727e-07\n",
      " Loss: 3.1252e-08 | Loss_d: 5.4037e-12 | Loss_e: 1.0415e-07\n",
      " Loss: 3.1197e-08 | Loss_d: 5.9721e-12 | Loss_e: 1.0397e-07\n",
      " Loss: 3.1450e-08 | Loss_d: 1.7408e-11 | Loss_e: 1.0477e-07\n",
      " Loss: 3.1171e-08 | Loss_d: 4.1069e-12 | Loss_e: 1.0389e-07\n",
      " Loss: 3.1118e-08 | Loss_d: 3.0070e-11 | Loss_e: 1.0363e-07\n",
      " Loss: 3.1118e-08 | Loss_d: 2.6276e-11 | Loss_e: 1.0364e-07\n",
      " Loss: 3.7824e-08 | Loss_d: 4.1978e-09 | Loss_e: 1.1209e-07\n",
      " Loss: 3.1066e-08 | Loss_d: 7.8479e-12 | Loss_e: 1.0353e-07\n",
      " Loss: 3.1023e-08 | Loss_d: 2.8777e-13 | Loss_e: 1.0341e-07\n",
      " Loss: 3.1018e-08 | Loss_d: 1.1511e-12 | Loss_e: 1.0339e-07\n",
      " Loss: 3.0979e-08 | Loss_d: 1.4211e-14 | Loss_e: 1.0326e-07\n",
      " Loss: 3.1230e-08 | Loss_d: 4.3773e-11 | Loss_e: 1.0395e-07\n",
      " Loss: 3.0974e-08 | Loss_d: 7.9936e-13 | Loss_e: 1.0325e-07\n",
      " Loss: 3.1072e-08 | Loss_d: 3.5527e-15 | Loss_e: 1.0357e-07\n",
      " Loss: 3.0953e-08 | Loss_d: 6.0041e-13 | Loss_e: 1.0317e-07\n",
      " Loss: 3.0897e-08 | Loss_d: 1.0747e-11 | Loss_e: 1.0295e-07\n",
      " Loss: 3.0824e-08 | Loss_d: 1.2825e-12 | Loss_e: 1.0274e-07\n",
      " Loss: 3.0457e-08 | Loss_d: 1.4211e-14 | Loss_e: 1.0152e-07\n",
      " Loss: 4.2676e-08 | Loss_d: 1.8468e-10 | Loss_e: 1.4164e-07\n",
      " Loss: 3.0456e-08 | Loss_d: 2.2737e-13 | Loss_e: 1.0152e-07\n",
      " Loss: 3.0674e-08 | Loss_d: 4.4565e-11 | Loss_e: 1.0210e-07\n",
      " Loss: 3.0266e-08 | Loss_d: 2.5899e-12 | Loss_e: 1.0088e-07\n",
      " Loss: 2.9165e-08 | Loss_d: 6.9633e-13 | Loss_e: 9.7215e-08\n",
      " Loss: 2.9123e-08 | Loss_d: 1.4211e-12 | Loss_e: 9.7071e-08\n",
      " Loss: 2.8152e-08 | Loss_d: 1.7039e-10 | Loss_e: 9.3271e-08\n",
      " Loss: 4.8530e-08 | Loss_d: 2.8649e-09 | Loss_e: 1.5222e-07\n",
      " Loss: 2.8154e-08 | Loss_d: 1.8794e-10 | Loss_e: 9.3221e-08\n",
      " Loss: 2.9211e-08 | Loss_d: 1.5519e-10 | Loss_e: 9.6852e-08\n",
      " Loss: 2.7891e-08 | Loss_d: 1.7986e-10 | Loss_e: 9.2371e-08\n",
      " Loss: 2.7346e-08 | Loss_d: 6.5690e-12 | Loss_e: 9.1132e-08\n",
      " Loss: 2.7320e-08 | Loss_d: 3.1974e-12 | Loss_e: 9.1054e-08\n",
      " Loss: 2.7203e-08 | Loss_d: 1.7195e-12 | Loss_e: 9.0671e-08\n",
      " Loss: 2.8144e-08 | Loss_d: 1.6884e-10 | Loss_e: 9.3252e-08\n",
      " Loss: 2.7193e-08 | Loss_d: 4.6043e-12 | Loss_e: 9.0627e-08\n",
      " Loss: 2.7092e-08 | Loss_d: 2.8777e-13 | Loss_e: 9.0307e-08\n",
      " Loss: 2.6915e-08 | Loss_d: 1.1951e-11 | Loss_e: 8.9676e-08\n",
      " Loss: 3.6122e-08 | Loss_d: 5.2015e-11 | Loss_e: 1.2023e-07\n",
      " Loss: 2.6906e-08 | Loss_d: 8.5301e-12 | Loss_e: 8.9657e-08\n",
      " Loss: 2.6551e-08 | Loss_d: 1.7195e-12 | Loss_e: 8.8499e-08\n",
      " Loss: 3.9010e-08 | Loss_d: 7.7813e-10 | Loss_e: 1.2744e-07\n",
      " Loss: 2.6528e-08 | Loss_d: 9.0949e-13 | Loss_e: 8.8424e-08\n",
      " Loss: 2.8898e-08 | Loss_d: 6.3869e-10 | Loss_e: 9.4198e-08\n",
      " Loss: 2.6247e-08 | Loss_d: 2.4016e-12 | Loss_e: 8.7481e-08\n",
      " Loss: 2.7367e-08 | Loss_d: 9.0949e-11 | Loss_e: 9.0919e-08\n",
      " Loss: 2.6132e-08 | Loss_d: 2.7853e-12 | Loss_e: 8.7096e-08\n",
      " Loss: 2.5704e-08 | Loss_d: 5.2015e-11 | Loss_e: 8.5507e-08\n",
      " Loss: 2.5645e-08 | Loss_d: 2.1615e-11 | Loss_e: 8.5410e-08\n",
      " Loss: 2.5409e-08 | Loss_d: 1.1639e-10 | Loss_e: 8.4310e-08\n",
      " Loss: 3.1331e-08 | Loss_d: 2.3888e-09 | Loss_e: 9.6474e-08\n",
      " Loss: 2.5402e-08 | Loss_d: 1.1639e-10 | Loss_e: 8.4287e-08\n",
      " Loss: 2.5590e-08 | Loss_d: 1.6914e-11 | Loss_e: 8.5245e-08\n",
      " Loss: 2.5240e-08 | Loss_d: 6.1902e-11 | Loss_e: 8.3926e-08\n",
      " Loss: 2.5045e-08 | Loss_d: 1.8794e-12 | Loss_e: 8.3477e-08\n",
      " Loss: 2.8568e-08 | Loss_d: 1.7826e-10 | Loss_e: 9.4632e-08\n",
      " Loss: 2.5040e-08 | Loss_d: 1.4211e-12 | Loss_e: 8.3460e-08\n",
      " Loss: 2.4940e-08 | Loss_d: 1.7408e-13 | Loss_e: 8.3133e-08\n",
      " Loss: 2.6894e-08 | Loss_d: 6.0041e-13 | Loss_e: 8.9646e-08\n",
      " Loss: 2.4937e-08 | Loss_d: 2.2737e-13 | Loss_e: 8.3124e-08\n",
      " Loss: 2.5014e-08 | Loss_d: 7.5730e-11 | Loss_e: 8.3129e-08\n",
      " Loss: 2.4909e-08 | Loss_d: 1.5010e-11 | Loss_e: 8.2981e-08\n",
      " Loss: 2.4866e-08 | Loss_d: 7.8479e-12 | Loss_e: 8.2861e-08\n",
      " Loss: 2.4958e-08 | Loss_d: 5.1159e-13 | Loss_e: 8.3193e-08\n",
      " Loss: 2.4854e-08 | Loss_d: 5.4037e-12 | Loss_e: 8.2829e-08\n",
      " Loss: 2.4959e-08 | Loss_d: 7.7819e-11 | Loss_e: 8.2937e-08\n",
      " Loss: 2.4764e-08 | Loss_d: 1.4211e-12 | Loss_e: 8.2541e-08\n",
      " Loss: 2.4700e-08 | Loss_d: 2.4016e-12 | Loss_e: 8.2326e-08\n",
      " Loss: 2.4843e-08 | Loss_d: 1.5010e-11 | Loss_e: 8.2761e-08\n",
      " Loss: 2.4665e-08 | Loss_d: 4.6043e-12 | Loss_e: 8.2203e-08\n",
      " Loss: 2.4555e-08 | Loss_d: 4.2988e-13 | Loss_e: 8.1849e-08\n",
      " Loss: 2.4805e-08 | Loss_d: 1.2423e-10 | Loss_e: 8.2268e-08\n",
      " Loss: 2.4512e-08 | Loss_d: 9.6065e-12 | Loss_e: 8.1674e-08\n",
      " Loss: 2.4345e-08 | Loss_d: 1.7195e-12 | Loss_e: 8.1143e-08\n",
      " Loss: 2.6939e-08 | Loss_d: 1.2423e-10 | Loss_e: 8.9382e-08\n",
      " Loss: 2.4327e-08 | Loss_d: 2.7853e-12 | Loss_e: 8.1080e-08\n",
      " Loss: 2.3952e-08 | Loss_d: 2.2737e-11 | Loss_e: 7.9766e-08\n",
      " Loss: 3.6716e-08 | Loss_d: 3.3152e-09 | Loss_e: 1.1134e-07\n",
      " Loss: 2.3957e-08 | Loss_d: 3.0070e-11 | Loss_e: 7.9755e-08\n",
      " Loss: 2.3847e-08 | Loss_d: 2.1615e-11 | Loss_e: 7.9416e-08\n",
      " Loss: 2.3816e-08 | Loss_d: 2.9420e-11 | Loss_e: 7.9290e-08\n",
      " Loss: 2.3679e-08 | Loss_d: 8.8818e-14 | Loss_e: 7.8929e-08\n",
      " Loss: 2.3605e-08 | Loss_d: 2.0464e-12 | Loss_e: 7.8676e-08\n",
      " Loss: 2.3456e-08 | Loss_d: 2.8777e-13 | Loss_e: 7.8187e-08\n",
      " Loss: 2.8475e-08 | Loss_d: 4.4565e-11 | Loss_e: 9.4769e-08\n",
      " Loss: 2.3455e-08 | Loss_d: 3.5527e-13 | Loss_e: 7.8181e-08\n",
      " Loss: 2.3215e-08 | Loss_d: 2.6890e-11 | Loss_e: 7.7294e-08\n",
      " Loss: 2.5333e-08 | Loss_d: 1.6729e-10 | Loss_e: 8.3887e-08\n",
      " Loss: 2.3210e-08 | Loss_d: 3.8426e-11 | Loss_e: 7.7238e-08\n",
      " Loss: 2.3188e-08 | Loss_d: 2.5899e-12 | Loss_e: 7.7284e-08\n",
      " Loss: 2.3121e-08 | Loss_d: 7.5175e-12 | Loss_e: 7.7044e-08\n",
      " Loss: 2.3035e-08 | Loss_d: 5.6843e-14 | Loss_e: 7.6784e-08\n",
      " Loss: 2.3021e-08 | Loss_d: 1.5667e-12 | Loss_e: 7.6733e-08\n",
      " Loss: 2.2825e-08 | Loss_d: 2.7512e-11 | Loss_e: 7.5992e-08\n",
      " Loss: 2.2790e-08 | Loss_d: 2.5899e-12 | Loss_e: 7.5958e-08\n",
      " Loss: 2.2573e-08 | Loss_d: 3.2063e-11 | Loss_e: 7.5136e-08\n",
      " Loss: 3.7594e-08 | Loss_d: 2.2794e-09 | Loss_e: 1.1771e-07\n",
      " Loss: 2.2569e-08 | Loss_d: 2.9420e-11 | Loss_e: 7.5130e-08\n",
      " Loss: 2.2336e-08 | Loss_d: 7.1942e-12 | Loss_e: 7.4431e-08\n",
      " Loss: 2.2333e-08 | Loss_d: 1.1141e-11 | Loss_e: 7.4406e-08\n",
      " Loss: 2.2037e-08 | Loss_d: 2.5668e-11 | Loss_e: 7.3370e-08\n",
      " Loss: 2.2037e-08 | Loss_d: 1.9455e-11 | Loss_e: 7.3390e-08\n",
      " Loss: 2.1788e-08 | Loss_d: 3.1974e-14 | Loss_e: 7.2628e-08\n",
      " Loss: 2.3443e-08 | Loss_d: 3.4120e-11 | Loss_e: 7.8029e-08\n",
      " Loss: 2.1760e-08 | Loss_d: 1.0267e-12 | Loss_e: 7.2530e-08\n",
      " Loss: 2.1703e-08 | Loss_d: 3.6241e-11 | Loss_e: 7.2221e-08\n",
      " Loss: 2.1685e-08 | Loss_d: 1.2790e-11 | Loss_e: 7.2239e-08\n",
      " Loss: 2.1495e-08 | Loss_d: 1.0267e-12 | Loss_e: 7.1648e-08\n",
      " Loss: 2.2668e-08 | Loss_d: 2.5899e-12 | Loss_e: 7.5551e-08\n",
      " Loss: 2.1467e-08 | Loss_d: 5.6843e-14 | Loss_e: 7.1558e-08\n",
      " Loss: 2.2097e-08 | Loss_d: 1.0510e-10 | Loss_e: 7.3306e-08\n",
      " Loss: 2.1458e-08 | Loss_d: 2.4016e-12 | Loss_e: 7.1518e-08\n",
      " Loss: 2.1421e-08 | Loss_d: 6.2670e-12 | Loss_e: 7.1382e-08\n",
      " Loss: 2.1739e-08 | Loss_d: 9.5554e-11 | Loss_e: 7.2146e-08\n",
      " Loss: 2.1414e-08 | Loss_d: 6.2670e-12 | Loss_e: 7.1359e-08\n",
      " Loss: 2.1354e-08 | Loss_d: 6.5690e-12 | Loss_e: 7.1158e-08\n",
      " Loss: 2.1865e-08 | Loss_d: 4.7805e-11 | Loss_e: 7.2724e-08\n",
      " Loss: 2.1346e-08 | Loss_d: 5.9721e-12 | Loss_e: 7.1134e-08\n",
      " Loss: 2.1285e-08 | Loss_d: 1.5948e-11 | Loss_e: 7.0897e-08\n",
      " Loss: 2.1281e-08 | Loss_d: 1.4101e-11 | Loss_e: 7.0891e-08\n",
      " Loss: 2.1184e-08 | Loss_d: 4.0675e-11 | Loss_e: 7.0477e-08\n",
      " Loss: 2.1152e-08 | Loss_d: 8.1855e-12 | Loss_e: 7.0479e-08\n",
      " Loss: 2.1033e-08 | Loss_d: 1.9455e-11 | Loss_e: 7.0044e-08\n",
      " Loss: 2.2374e-08 | Loss_d: 7.5175e-12 | Loss_e: 7.4556e-08\n",
      " Loss: 2.1010e-08 | Loss_d: 1.6914e-11 | Loss_e: 6.9977e-08\n",
      " Loss: 2.0785e-08 | Loss_d: 9.0949e-13 | Loss_e: 6.9279e-08\n",
      " Loss: 2.0708e-08 | Loss_d: 9.2406e-12 | Loss_e: 6.8995e-08\n",
      " Loss: 2.0317e-08 | Loss_d: 1.9455e-11 | Loss_e: 6.7660e-08\n",
      " Loss: 2.0310e-08 | Loss_d: 9.9796e-12 | Loss_e: 6.7666e-08\n",
      " Loss: 2.0693e-08 | Loss_d: 2.6890e-11 | Loss_e: 6.8887e-08\n",
      " Loss: 2.0250e-08 | Loss_d: 3.5527e-13 | Loss_e: 6.7500e-08\n",
      " Loss: 2.0061e-08 | Loss_d: 1.4101e-11 | Loss_e: 6.6823e-08\n",
      " Loss: 2.0056e-08 | Loss_d: 7.1942e-12 | Loss_e: 6.6829e-08\n",
      " Loss: 1.9978e-08 | Loss_d: 6.9633e-13 | Loss_e: 6.6591e-08\n",
      " Loss: 2.1116e-08 | Loss_d: 1.3220e-11 | Loss_e: 7.0343e-08\n",
      " Loss: 1.9980e-08 | Loss_d: 1.2825e-12 | Loss_e: 6.6597e-08\n",
      " Loss: 1.9969e-08 | Loss_d: 5.1159e-13 | Loss_e: 6.6560e-08\n",
      " Loss: 1.9873e-08 | Loss_d: 1.4211e-14 | Loss_e: 6.6243e-08\n",
      " Loss: 3.5867e-07 | Loss_d: 1.0905e-08 | Loss_e: 1.1592e-06\n",
      " Loss: 1.9856e-08 | Loss_d: 2.2737e-13 | Loss_e: 6.6185e-08\n",
      " Loss: 1.9743e-08 | Loss_d: 2.9878e-12 | Loss_e: 6.5800e-08\n",
      " Loss: 1.9546e-08 | Loss_d: 6.8642e-11 | Loss_e: 6.4926e-08\n",
      " Loss: 1.9484e-08 | Loss_d: 3.9918e-11 | Loss_e: 6.4814e-08\n",
      " Loss: 1.9294e-08 | Loss_d: 1.9984e-11 | Loss_e: 6.4247e-08\n",
      " Loss: 1.9294e-08 | Loss_d: 6.2670e-12 | Loss_e: 6.4293e-08\n",
      " Loss: 2.0670e-08 | Loss_d: 1.0747e-11 | Loss_e: 6.8864e-08\n",
      " Loss: 1.9229e-08 | Loss_d: 8.5301e-12 | Loss_e: 6.4067e-08\n",
      " Loss: 1.9067e-08 | Loss_d: 2.5668e-11 | Loss_e: 6.3472e-08\n",
      " Loss: 2.4140e-08 | Loss_d: 7.8814e-10 | Loss_e: 7.7840e-08\n",
      " Loss: 1.9052e-08 | Loss_d: 2.2737e-11 | Loss_e: 6.3432e-08\n",
      " Loss: 1.8974e-08 | Loss_d: 3.4120e-11 | Loss_e: 6.3133e-08\n",
      " Loss: 1.8954e-08 | Loss_d: 2.4475e-11 | Loss_e: 6.3098e-08\n",
      " Loss: 1.8750e-08 | Loss_d: 1.7408e-13 | Loss_e: 6.2498e-08\n",
      " Loss: 2.4512e-08 | Loss_d: 7.9936e-13 | Loss_e: 8.1705e-08\n",
      " Loss: 1.8746e-08 | Loss_d: 1.1511e-12 | Loss_e: 6.2483e-08\n",
      " Loss: 1.8592e-08 | Loss_d: 4.8637e-12 | Loss_e: 6.1957e-08\n",
      " Loss: 2.7459e-08 | Loss_d: 1.7708e-09 | Loss_e: 8.5627e-08\n",
      " Loss: 1.8589e-08 | Loss_d: 4.1069e-12 | Loss_e: 6.1949e-08\n",
      " Loss: 1.8873e-08 | Loss_d: 1.8417e-11 | Loss_e: 6.2848e-08\n",
      " Loss: 1.8506e-08 | Loss_d: 3.5527e-15 | Loss_e: 6.1687e-08\n",
      " Loss: 1.8389e-08 | Loss_d: 6.0041e-13 | Loss_e: 6.1295e-08\n",
      " Loss: 2.0930e-08 | Loss_d: 3.8924e-10 | Loss_e: 6.8470e-08\n",
      " Loss: 1.8374e-08 | Loss_d: 2.4016e-12 | Loss_e: 6.1238e-08\n",
      " Loss: 1.8381e-08 | Loss_d: 3.2063e-11 | Loss_e: 6.1162e-08\n",
      " Loss: 1.8293e-08 | Loss_d: 3.6380e-12 | Loss_e: 6.0966e-08\n",
      " Loss: 1.8108e-08 | Loss_d: 8.5301e-12 | Loss_e: 6.0330e-08\n",
      " Loss: 2.6924e-08 | Loss_d: 1.2367e-09 | Loss_e: 8.5623e-08\n",
      " Loss: 1.8105e-08 | Loss_d: 8.5301e-12 | Loss_e: 6.0321e-08\n",
      " Loss: 2.9472e-08 | Loss_d: 2.2341e-09 | Loss_e: 9.0792e-08\n",
      " Loss: 1.8095e-08 | Loss_d: 1.5667e-12 | Loss_e: 6.0312e-08\n",
      " Loss: 1.7985e-08 | Loss_d: 1.2790e-13 | Loss_e: 5.9950e-08\n",
      " Loss: 1.7986e-08 | Loss_d: 3.5527e-15 | Loss_e: 5.9954e-08\n",
      " Loss: 1.7945e-08 | Loss_d: 1.0880e-10 | Loss_e: 5.9455e-08\n",
      " Loss: 1.7903e-08 | Loss_d: 1.1543e-11 | Loss_e: 5.9638e-08\n",
      " Loss: 1.7902e-08 | Loss_d: 1.0633e-10 | Loss_e: 5.9318e-08\n",
      " Loss: 1.7863e-08 | Loss_d: 3.3427e-11 | Loss_e: 5.9432e-08\n",
      " Loss: 1.7786e-08 | Loss_d: 6.0041e-11 | Loss_e: 5.9088e-08\n",
      " Loss: 2.0159e-08 | Loss_d: 8.7402e-10 | Loss_e: 6.4283e-08\n",
      " Loss: 1.7778e-08 | Loss_d: 5.5511e-11 | Loss_e: 5.9074e-08\n",
      " Loss: 1.7663e-08 | Loss_d: 1.9984e-11 | Loss_e: 5.8810e-08\n",
      " Loss: 1.7784e-08 | Loss_d: 3.1974e-14 | Loss_e: 5.9281e-08\n",
      " Loss: 1.7610e-08 | Loss_d: 6.5690e-12 | Loss_e: 5.8678e-08\n",
      " Loss: 1.7527e-08 | Loss_d: 2.8777e-13 | Loss_e: 5.8423e-08\n",
      " Loss: 1.7466e-08 | Loss_d: 2.2172e-11 | Loss_e: 5.8145e-08\n",
      " Loss: 1.7436e-08 | Loss_d: 9.6065e-12 | Loss_e: 5.8090e-08\n",
      " Loss: 1.7320e-08 | Loss_d: 1.7408e-13 | Loss_e: 5.7734e-08\n",
      " Loss: 1.8645e-08 | Loss_d: 6.6609e-10 | Loss_e: 5.9930e-08\n",
      " Loss: 1.7305e-08 | Loss_d: 1.4211e-14 | Loss_e: 5.7683e-08\n",
      " Loss: 1.8007e-08 | Loss_d: 1.7352e-10 | Loss_e: 5.9445e-08\n",
      " Loss: 1.7276e-08 | Loss_d: 1.2790e-13 | Loss_e: 5.7585e-08\n",
      " Loss: 1.0026e-07 | Loss_d: 1.5179e-08 | Loss_e: 2.8360e-07\n",
      " Loss: 1.7268e-08 | Loss_d: 6.0041e-13 | Loss_e: 5.7557e-08\n",
      " Loss: 1.7103e-08 | Loss_d: 3.9169e-11 | Loss_e: 5.6878e-08\n",
      " Loss: 1.7025e-08 | Loss_d: 1.4211e-14 | Loss_e: 5.6750e-08\n",
      " Loss: 1.6977e-08 | Loss_d: 7.1942e-12 | Loss_e: 5.6566e-08\n",
      " Loss: 1.6859e-08 | Loss_d: 1.5010e-11 | Loss_e: 5.6148e-08\n",
      " Loss: 3.0053e-08 | Loss_d: 1.7309e-09 | Loss_e: 9.4407e-08\n",
      " Loss: 1.6857e-08 | Loss_d: 1.4101e-11 | Loss_e: 5.6142e-08\n",
      " Loss: 1.6833e-08 | Loss_d: 3.9918e-11 | Loss_e: 5.5976e-08\n",
      " Loss: 1.6807e-08 | Loss_d: 1.7408e-11 | Loss_e: 5.5966e-08\n",
      " Loss: 1.6648e-08 | Loss_d: 8.5301e-12 | Loss_e: 5.5466e-08\n",
      " Loss: 1.9123e-08 | Loss_d: 2.7063e-10 | Loss_e: 6.2840e-08\n",
      " Loss: 1.6634e-08 | Loss_d: 5.6843e-12 | Loss_e: 5.5428e-08\n",
      " Loss: 1.6522e-08 | Loss_d: 1.7408e-13 | Loss_e: 5.5074e-08\n",
      " Loss: 1.6573e-08 | Loss_d: 2.1064e-11 | Loss_e: 5.5173e-08\n",
      " Loss: 1.6453e-08 | Loss_d: 1.0360e-11 | Loss_e: 5.4810e-08\n",
      " Loss: 1.6748e-08 | Loss_d: 2.5068e-11 | Loss_e: 5.5743e-08\n",
      " Loss: 1.6443e-08 | Loss_d: 6.2670e-12 | Loss_e: 5.4789e-08\n",
      " Loss: 1.6407e-08 | Loss_d: 1.4211e-14 | Loss_e: 5.4689e-08\n",
      " Loss: 1.8137e-08 | Loss_d: 3.5926e-10 | Loss_e: 5.9259e-08\n",
      " Loss: 1.6404e-08 | Loss_d: 4.2988e-13 | Loss_e: 5.4680e-08\n",
      " Loss: 1.6386e-08 | Loss_d: 1.2790e-13 | Loss_e: 5.4618e-08\n",
      " Loss: 1.6429e-08 | Loss_d: 7.9936e-13 | Loss_e: 5.4759e-08\n",
      " Loss: 1.6377e-08 | Loss_d: 8.8818e-14 | Loss_e: 5.4590e-08\n",
      " Loss: 2.6149e-08 | Loss_d: 3.1974e-14 | Loss_e: 8.7165e-08\n",
      " Loss: 1.6371e-08 | Loss_d: 3.1974e-14 | Loss_e: 5.4571e-08\n",
      " Loss: 1.6367e-08 | Loss_d: 1.5948e-11 | Loss_e: 5.4503e-08\n",
      " Loss: 1.6335e-08 | Loss_d: 2.4016e-12 | Loss_e: 5.4443e-08\n",
      " Loss: 1.6333e-08 | Loss_d: 1.2825e-12 | Loss_e: 5.4441e-08\n",
      " Loss: 1.6327e-08 | Loss_d: 1.4211e-14 | Loss_e: 5.4423e-08\n",
      " Loss: 1.6362e-08 | Loss_d: 2.3309e-11 | Loss_e: 5.4463e-08\n",
      " Loss: 1.6325e-08 | Loss_d: 6.0041e-13 | Loss_e: 5.4415e-08\n",
      " Loss: 1.6312e-08 | Loss_d: 8.8818e-14 | Loss_e: 5.4375e-08\n",
      " Loss: 1.6336e-08 | Loss_d: 6.5690e-12 | Loss_e: 5.4430e-08\n",
      " Loss: 1.6310e-08 | Loss_d: 6.0041e-13 | Loss_e: 5.4364e-08\n",
      " Loss: 1.6295e-08 | Loss_d: 3.1974e-14 | Loss_e: 5.4315e-08\n",
      " Loss: 1.6252e-08 | Loss_d: 1.1511e-12 | Loss_e: 5.4169e-08\n",
      " Loss: 1.6695e-08 | Loss_d: 6.7658e-11 | Loss_e: 5.5424e-08\n",
      " Loss: 1.6246e-08 | Loss_d: 2.4016e-12 | Loss_e: 5.4145e-08\n",
      " Loss: 1.6173e-08 | Loss_d: 1.8794e-12 | Loss_e: 5.3904e-08\n",
      " Loss: 1.6132e-08 | Loss_d: 3.5527e-13 | Loss_e: 5.3773e-08\n",
      " Loss: 1.6101e-08 | Loss_d: 5.1159e-13 | Loss_e: 5.3668e-08\n",
      " Loss: 1.6056e-08 | Loss_d: 3.5527e-13 | Loss_e: 5.3518e-08\n",
      " Loss: 1.6055e-08 | Loss_d: 1.2790e-13 | Loss_e: 5.3516e-08\n",
      " Loss: 1.6039e-08 | Loss_d: 3.1974e-12 | Loss_e: 5.3452e-08\n",
      " Loss: 1.6097e-08 | Loss_d: 2.1615e-11 | Loss_e: 5.3584e-08\n",
      " Loss: 1.6034e-08 | Loss_d: 8.1855e-12 | Loss_e: 5.3418e-08\n",
      " Loss: 1.6026e-08 | Loss_d: 1.8794e-12 | Loss_e: 5.3414e-08\n",
      " Loss: 1.6284e-08 | Loss_d: 5.0310e-11 | Loss_e: 5.4113e-08\n",
      " Loss: 1.6026e-08 | Loss_d: 3.5527e-13 | Loss_e: 5.3420e-08\n",
      " Loss: 1.6016e-08 | Loss_d: 1.0267e-12 | Loss_e: 5.3384e-08\n",
      " Loss: 1.6010e-08 | Loss_d: 3.8689e-12 | Loss_e: 5.3355e-08\n",
      " Loss: 1.6009e-08 | Loss_d: 2.7853e-12 | Loss_e: 5.3356e-08\n",
      " Loss: 1.5971e-08 | Loss_d: 2.2204e-12 | Loss_e: 5.3230e-08\n",
      " Loss: 1.5873e-08 | Loss_d: 4.2988e-13 | Loss_e: 5.2910e-08\n",
      " Loss: 3.6696e-08 | Loss_d: 3.0005e-09 | Loss_e: 1.1232e-07\n",
      " Loss: 1.5870e-08 | Loss_d: 1.7408e-13 | Loss_e: 5.2901e-08\n",
      " Loss: 1.5751e-08 | Loss_d: 4.4565e-11 | Loss_e: 5.2354e-08\n",
      " Loss: 1.5674e-08 | Loss_d: 3.6380e-12 | Loss_e: 5.2234e-08\n",
      " Loss: 1.5686e-08 | Loss_d: 7.5175e-12 | Loss_e: 5.2262e-08\n",
      " Loss: 1.5527e-08 | Loss_d: 4.2988e-13 | Loss_e: 5.1756e-08\n",
      " Loss: 1.5649e-08 | Loss_d: 1.5519e-10 | Loss_e: 5.1645e-08\n",
      " Loss: 1.5400e-08 | Loss_d: 3.4142e-12 | Loss_e: 5.1322e-08\n",
      " Loss: 1.5473e-08 | Loss_d: 9.9082e-11 | Loss_e: 5.1247e-08\n",
      " Loss: 1.5320e-08 | Loss_d: 9.9796e-12 | Loss_e: 5.1033e-08\n",
      " Loss: 1.5262e-08 | Loss_d: 2.1064e-11 | Loss_e: 5.0802e-08\n",
      " Loss: 1.5262e-08 | Loss_d: 1.8417e-11 | Loss_e: 5.0811e-08\n",
      " Loss: 1.5234e-08 | Loss_d: 7.8479e-12 | Loss_e: 5.0753e-08\n",
      " Loss: 1.5217e-08 | Loss_d: 1.0747e-11 | Loss_e: 5.0687e-08\n",
      " Loss: 1.5206e-08 | Loss_d: 2.5899e-12 | Loss_e: 5.0678e-08\n",
      " Loss: 1.5126e-08 | Loss_d: 1.1511e-12 | Loss_e: 5.0415e-08\n",
      " Loss: 1.5123e-08 | Loss_d: 2.7853e-12 | Loss_e: 5.0401e-08\n",
      " Loss: 1.5077e-08 | Loss_d: 2.0464e-12 | Loss_e: 5.0249e-08\n",
      " Loss: 1.5080e-08 | Loss_d: 2.7853e-12 | Loss_e: 5.0257e-08\n",
      " Loss: 1.4974e-08 | Loss_d: 6.0041e-13 | Loss_e: 4.9910e-08\n",
      " Loss: 1.5318e-08 | Loss_d: 1.7826e-10 | Loss_e: 5.0464e-08\n",
      " Loss: 1.4940e-08 | Loss_d: 5.6843e-12 | Loss_e: 4.9780e-08\n",
      " Loss: 1.5012e-08 | Loss_d: 5.6403e-11 | Loss_e: 4.9853e-08\n",
      " Loss: 1.4864e-08 | Loss_d: 1.0267e-12 | Loss_e: 4.9542e-08\n",
      " Loss: 1.4775e-08 | Loss_d: 3.6380e-12 | Loss_e: 4.9238e-08\n",
      " Loss: 1.6386e-08 | Loss_d: 6.2074e-10 | Loss_e: 5.2551e-08\n",
      " Loss: 1.4773e-08 | Loss_d: 5.9721e-12 | Loss_e: 4.9223e-08\n",
      " Loss: 1.4811e-08 | Loss_d: 1.4211e-14 | Loss_e: 4.9371e-08\n",
      " Loss: 1.4746e-08 | Loss_d: 9.0949e-13 | Loss_e: 4.9151e-08\n",
      " Loss: 1.4669e-08 | Loss_d: 1.5010e-11 | Loss_e: 4.8846e-08\n",
      " Loss: 1.6438e-08 | Loss_d: 7.0669e-10 | Loss_e: 5.2437e-08\n",
      " Loss: 1.4658e-08 | Loss_d: 1.7909e-11 | Loss_e: 4.8800e-08\n",
      " Loss: 1.4818e-08 | Loss_d: 1.0510e-10 | Loss_e: 4.9044e-08\n",
      " Loss: 1.4619e-08 | Loss_d: 1.8932e-11 | Loss_e: 4.8666e-08\n",
      " Loss: 1.4540e-08 | Loss_d: 4.2988e-11 | Loss_e: 4.8323e-08\n",
      " Loss: 1.4537e-08 | Loss_d: 3.6241e-11 | Loss_e: 4.8335e-08\n",
      " Loss: 1.4471e-08 | Loss_d: 2.5899e-12 | Loss_e: 4.8229e-08\n",
      " Loss: 1.4967e-08 | Loss_d: 1.9787e-10 | Loss_e: 4.9229e-08\n",
      " Loss: 1.4467e-08 | Loss_d: 2.2737e-13 | Loss_e: 4.8223e-08\n",
      " Loss: 1.4417e-08 | Loss_d: 1.8794e-12 | Loss_e: 4.8050e-08\n",
      " Loss: 1.4419e-08 | Loss_d: 3.6380e-12 | Loss_e: 4.8053e-08\n",
      " Loss: 1.4378e-08 | Loss_d: 2.7853e-12 | Loss_e: 4.7916e-08\n",
      " Loss: 1.4476e-08 | Loss_d: 3.4120e-11 | Loss_e: 4.8139e-08\n",
      " Loss: 1.4358e-08 | Loss_d: 1.5667e-12 | Loss_e: 4.7854e-08\n",
      " Loss: 1.4293e-08 | Loss_d: 3.1974e-12 | Loss_e: 4.7634e-08\n",
      " Loss: 1.5810e-08 | Loss_d: 1.9787e-10 | Loss_e: 5.2039e-08\n",
      " Loss: 1.4289e-08 | Loss_d: 2.7853e-12 | Loss_e: 4.7622e-08\n",
      " Loss: 1.4360e-08 | Loss_d: 1.6914e-11 | Loss_e: 4.7811e-08\n",
      " Loss: 1.4264e-08 | Loss_d: 1.0267e-12 | Loss_e: 4.7543e-08\n",
      " Loss: 1.4232e-08 | Loss_d: 5.6843e-14 | Loss_e: 4.7441e-08\n",
      " Loss: 1.5852e-08 | Loss_d: 3.3702e-10 | Loss_e: 5.1718e-08\n",
      " Loss: 1.4233e-08 | Loss_d: 2.8777e-13 | Loss_e: 4.7442e-08\n",
      " Loss: 1.4215e-08 | Loss_d: 2.7853e-12 | Loss_e: 4.7375e-08\n",
      " Loss: 1.4719e-08 | Loss_d: 9.4392e-11 | Loss_e: 4.8750e-08\n",
      " Loss: 1.4212e-08 | Loss_d: 3.5527e-13 | Loss_e: 4.7373e-08\n",
      " Loss: 1.4245e-08 | Loss_d: 4.6171e-11 | Loss_e: 4.7330e-08\n",
      " Loss: 1.4207e-08 | Loss_d: 1.0360e-11 | Loss_e: 4.7321e-08\n",
      " Loss: 1.4183e-08 | Loss_d: 3.1974e-14 | Loss_e: 4.7277e-08\n",
      " Loss: 1.4329e-08 | Loss_d: 9.4392e-11 | Loss_e: 4.7447e-08\n",
      " Loss: 1.4184e-08 | Loss_d: 3.5527e-13 | Loss_e: 4.7278e-08\n",
      " Loss: 1.4159e-08 | Loss_d: 3.5527e-13 | Loss_e: 4.7196e-08\n",
      " Loss: 1.5072e-08 | Loss_d: 1.1639e-10 | Loss_e: 4.9851e-08\n",
      " Loss: 1.4156e-08 | Loss_d: 1.7408e-13 | Loss_e: 4.7186e-08\n",
      " Loss: 1.4139e-08 | Loss_d: 5.1301e-12 | Loss_e: 4.7114e-08\n",
      " Loss: 1.4136e-08 | Loss_d: 9.0949e-13 | Loss_e: 4.7118e-08\n",
      " Loss: 1.4073e-08 | Loss_d: 1.7408e-13 | Loss_e: 4.6909e-08\n",
      " Loss: 1.5089e-08 | Loss_d: 1.3788e-10 | Loss_e: 4.9838e-08\n",
      " Loss: 1.4071e-08 | Loss_d: 2.8777e-13 | Loss_e: 4.6902e-08\n",
      " Loss: 1.4028e-08 | Loss_d: 4.2988e-13 | Loss_e: 4.6759e-08\n",
      " Loss: 1.4575e-08 | Loss_d: 3.2742e-11 | Loss_e: 4.8473e-08\n",
      " Loss: 1.4027e-08 | Loss_d: 6.9633e-13 | Loss_e: 4.6755e-08\n",
      " Loss: 1.4008e-08 | Loss_d: 3.8689e-12 | Loss_e: 4.6681e-08\n",
      " Loss: 1.4002e-08 | Loss_d: 1.2790e-13 | Loss_e: 4.6672e-08\n",
      " Loss: 1.3980e-08 | Loss_d: 6.8781e-12 | Loss_e: 4.6575e-08\n",
      " Loss: 1.3969e-08 | Loss_d: 1.0267e-12 | Loss_e: 4.6561e-08\n",
      " Loss: 1.3894e-08 | Loss_d: 7.9936e-13 | Loss_e: 4.6311e-08\n",
      " Loss: 1.8899e-08 | Loss_d: 7.6489e-10 | Loss_e: 6.0447e-08\n",
      " Loss: 1.3899e-08 | Loss_d: 1.2825e-12 | Loss_e: 4.6324e-08\n",
      " Loss: 1.3796e-08 | Loss_d: 7.1942e-12 | Loss_e: 4.5961e-08\n",
      " Loss: 1.3796e-08 | Loss_d: 4.1069e-12 | Loss_e: 4.5974e-08\n",
      " Loss: 1.3731e-08 | Loss_d: 9.0949e-13 | Loss_e: 4.5766e-08\n",
      " Loss: 1.7243e-08 | Loss_d: 9.5328e-10 | Loss_e: 5.4299e-08\n",
      " Loss: 1.3732e-08 | Loss_d: 7.9936e-13 | Loss_e: 4.5770e-08\n",
      " Loss: 1.3693e-08 | Loss_d: 3.6380e-12 | Loss_e: 4.5630e-08\n",
      " Loss: 1.4099e-08 | Loss_d: 2.2737e-11 | Loss_e: 4.6920e-08\n",
      " Loss: 1.3689e-08 | Loss_d: 2.4016e-12 | Loss_e: 4.5622e-08\n",
      " Loss: 1.5011e-08 | Loss_d: 2.8857e-10 | Loss_e: 4.9074e-08\n",
      " Loss: 1.3672e-08 | Loss_d: 2.8777e-13 | Loss_e: 4.5571e-08\n",
      " Loss: 1.3609e-08 | Loss_d: 3.5527e-15 | Loss_e: 4.5363e-08\n",
      " Loss: 1.4312e-08 | Loss_d: 6.7658e-11 | Loss_e: 4.7482e-08\n",
      " Loss: 1.3595e-08 | Loss_d: 1.4211e-14 | Loss_e: 4.5315e-08\n",
      " Loss: 1.3442e-08 | Loss_d: 5.4037e-12 | Loss_e: 4.4788e-08\n",
      " Loss: 2.2988e-08 | Loss_d: 5.2934e-10 | Loss_e: 7.4862e-08\n",
      " Loss: 1.3440e-08 | Loss_d: 4.1069e-12 | Loss_e: 4.4785e-08\n",
      " Loss: 1.3442e-08 | Loss_d: 2.1615e-11 | Loss_e: 4.4736e-08\n",
      " Loss: 1.3366e-08 | Loss_d: 1.4101e-11 | Loss_e: 4.4506e-08\n",
      " Loss: 1.3284e-08 | Loss_d: 1.4101e-11 | Loss_e: 4.4232e-08\n",
      " Loss: 1.3280e-08 | Loss_d: 5.9721e-12 | Loss_e: 4.4246e-08\n",
      " Loss: 1.3256e-08 | Loss_d: 3.6380e-12 | Loss_e: 4.4176e-08\n",
      " Loss: 1.3603e-08 | Loss_d: 1.5476e-11 | Loss_e: 4.5293e-08\n",
      " Loss: 1.3255e-08 | Loss_d: 3.4142e-12 | Loss_e: 4.4172e-08\n",
      " Loss: 2.0786e-08 | Loss_d: 1.4371e-09 | Loss_e: 6.4498e-08\n",
      " Loss: 1.3250e-08 | Loss_d: 3.4142e-12 | Loss_e: 4.4155e-08\n",
      " Loss: 1.3184e-08 | Loss_d: 4.1069e-12 | Loss_e: 4.3932e-08\n",
      " Loss: 1.3281e-08 | Loss_d: 1.5667e-10 | Loss_e: 4.3747e-08\n",
      " Loss: 1.3168e-08 | Loss_d: 4.0675e-11 | Loss_e: 4.3758e-08\n",
      " Loss: 1.3136e-08 | Loss_d: 1.5667e-12 | Loss_e: 4.3782e-08\n",
      " Loss: 1.3105e-08 | Loss_d: 2.6890e-11 | Loss_e: 4.3595e-08\n",
      " Loss: 1.3303e-08 | Loss_d: 6.3793e-11 | Loss_e: 4.4129e-08\n",
      " Loss: 1.3039e-08 | Loss_d: 1.0747e-11 | Loss_e: 4.3426e-08\n",
      " Loss: 1.2884e-08 | Loss_d: 3.4820e-11 | Loss_e: 4.2829e-08\n",
      " Loss: 1.2868e-08 | Loss_d: 5.6843e-12 | Loss_e: 4.2876e-08\n",
      " Loss: 1.2766e-08 | Loss_d: 1.1543e-11 | Loss_e: 4.2515e-08\n",
      " Loss: 1.4007e-08 | Loss_d: 6.0041e-11 | Loss_e: 4.6489e-08\n",
      " Loss: 1.2768e-08 | Loss_d: 1.3657e-11 | Loss_e: 4.2515e-08\n",
      " Loss: 1.2731e-08 | Loss_d: 1.5817e-10 | Loss_e: 4.1910e-08\n",
      " Loss: 1.2664e-08 | Loss_d: 4.3773e-11 | Loss_e: 4.2067e-08\n",
      " Loss: 1.2547e-08 | Loss_d: 5.7302e-11 | Loss_e: 4.1632e-08\n",
      " Loss: 1.4428e-08 | Loss_d: 7.6489e-10 | Loss_e: 4.5543e-08\n",
      " Loss: 1.2560e-08 | Loss_d: 8.2082e-11 | Loss_e: 4.1594e-08\n",
      " Loss: 1.2484e-08 | Loss_d: 9.2406e-12 | Loss_e: 4.1583e-08\n",
      " Loss: 1.2471e-08 | Loss_d: 1.7408e-11 | Loss_e: 4.1511e-08\n",
      " Loss: 1.2380e-08 | Loss_d: 2.4475e-11 | Loss_e: 4.1184e-08\n",
      " Loss: 1.6650e-08 | Loss_d: 8.7402e-10 | Loss_e: 5.2587e-08\n",
      " Loss: 1.2373e-08 | Loss_d: 1.6914e-11 | Loss_e: 4.1187e-08\n",
      " Loss: 1.2327e-08 | Loss_d: 2.2204e-12 | Loss_e: 4.1084e-08\n",
      " Loss: 1.2329e-08 | Loss_d: 1.7408e-13 | Loss_e: 4.1095e-08\n",
      " Loss: 1.2313e-08 | Loss_d: 4.2988e-13 | Loss_e: 4.1041e-08\n",
      " Loss: 1.2332e-08 | Loss_d: 3.5527e-13 | Loss_e: 4.1105e-08\n",
      " Loss: 1.2301e-08 | Loss_d: 1.7408e-13 | Loss_e: 4.1002e-08\n",
      " Loss: 1.2242e-08 | Loss_d: 3.5527e-15 | Loss_e: 4.0808e-08\n",
      " Loss: 1.2180e-08 | Loss_d: 3.2063e-11 | Loss_e: 4.0492e-08\n",
      " Loss: 1.2170e-08 | Loss_d: 1.2367e-11 | Loss_e: 4.0526e-08\n",
      " Loss: 1.4353e-08 | Loss_d: 5.2114e-10 | Loss_e: 4.6107e-08\n",
      " Loss: 1.2100e-08 | Loss_d: 1.5476e-11 | Loss_e: 4.0280e-08\n",
      " Loss: 1.1749e-08 | Loss_d: 2.7853e-12 | Loss_e: 3.9155e-08\n",
      " Loss: 1.1706e-08 | Loss_d: 3.6380e-12 | Loss_e: 3.9007e-08\n",
      " Loss: 1.1566e-08 | Loss_d: 2.7853e-12 | Loss_e: 3.8543e-08\n",
      " Loss: 1.3042e-08 | Loss_d: 9.8295e-10 | Loss_e: 4.0198e-08\n",
      " Loss: 1.1539e-08 | Loss_d: 8.8818e-12 | Loss_e: 3.8435e-08\n",
      " Loss: 1.1953e-08 | Loss_d: 4.3521e-12 | Loss_e: 3.9827e-08\n",
      " Loss: 1.1448e-08 | Loss_d: 1.7408e-13 | Loss_e: 3.8161e-08\n",
      " Loss: 1.1406e-08 | Loss_d: 9.6723e-11 | Loss_e: 3.7698e-08\n",
      " Loss: 1.1363e-08 | Loss_d: 1.5948e-11 | Loss_e: 3.7822e-08\n",
      " Loss: 1.1321e-08 | Loss_d: 5.9121e-11 | Loss_e: 3.7540e-08\n",
      " Loss: 1.1317e-08 | Loss_d: 3.9169e-11 | Loss_e: 3.7592e-08\n",
      " Loss: 1.1254e-08 | Loss_d: 2.5668e-11 | Loss_e: 3.7426e-08\n",
      " Loss: 1.1442e-08 | Loss_d: 2.3309e-11 | Loss_e: 3.8062e-08\n",
      " Loss: 1.1226e-08 | Loss_d: 1.7909e-11 | Loss_e: 3.7362e-08\n",
      " Loss: 1.1338e-08 | Loss_d: 8.5301e-12 | Loss_e: 3.7765e-08\n",
      " Loss: 1.1206e-08 | Loss_d: 1.5476e-11 | Loss_e: 3.7302e-08\n",
      " Loss: 1.2583e-08 | Loss_d: 3.2188e-10 | Loss_e: 4.0869e-08\n",
      " Loss: 1.1178e-08 | Loss_d: 2.8777e-13 | Loss_e: 3.7258e-08\n",
      " Loss: 1.1117e-08 | Loss_d: 3.5527e-13 | Loss_e: 3.7055e-08\n",
      " Loss: 1.1167e-08 | Loss_d: 1.3234e-10 | Loss_e: 3.6782e-08\n",
      " Loss: 1.1072e-08 | Loss_d: 8.8818e-12 | Loss_e: 3.6876e-08\n",
      " Loss: 1.1017e-08 | Loss_d: 3.6380e-12 | Loss_e: 3.6710e-08\n",
      " Loss: 1.1059e-08 | Loss_d: 5.5511e-11 | Loss_e: 3.6679e-08\n",
      " Loss: 1.0992e-08 | Loss_d: 7.5175e-12 | Loss_e: 3.6616e-08\n",
      " Loss: 1.1264e-08 | Loss_d: 1.4930e-10 | Loss_e: 3.7048e-08\n",
      " Loss: 1.0985e-08 | Loss_d: 1.1543e-11 | Loss_e: 3.6578e-08\n",
      " Loss: 1.0954e-08 | Loss_d: 1.7195e-12 | Loss_e: 3.6508e-08\n",
      " Loss: 1.0952e-08 | Loss_d: 7.9936e-13 | Loss_e: 3.6503e-08\n",
      " Loss: 1.0945e-08 | Loss_d: 9.0949e-13 | Loss_e: 3.6481e-08\n",
      " Loss: 1.1021e-08 | Loss_d: 2.8777e-13 | Loss_e: 3.6735e-08\n",
      " Loss: 1.0941e-08 | Loss_d: 1.2790e-13 | Loss_e: 3.6471e-08\n",
      " Loss: 1.0963e-08 | Loss_d: 5.4037e-12 | Loss_e: 3.6526e-08\n",
      " Loss: 1.0931e-08 | Loss_d: 1.7408e-13 | Loss_e: 3.6436e-08\n",
      " Loss: 1.0927e-08 | Loss_d: 8.8818e-14 | Loss_e: 3.6422e-08\n",
      " Loss: 1.0927e-08 | Loss_d: 6.9633e-13 | Loss_e: 3.6421e-08\n",
      " Loss: 1.0926e-08 | Loss_d: 1.1511e-12 | Loss_e: 3.6415e-08\n",
      " Loss: 1.0948e-08 | Loss_d: 8.8818e-12 | Loss_e: 3.6463e-08\n",
      " Loss: 1.0925e-08 | Loss_d: 4.2988e-13 | Loss_e: 3.6415e-08\n",
      " Loss: 1.0923e-08 | Loss_d: 1.2790e-13 | Loss_e: 3.6409e-08\n",
      " Loss: 1.0929e-08 | Loss_d: 1.4211e-14 | Loss_e: 3.6429e-08\n",
      " Loss: 1.0923e-08 | Loss_d: 5.6843e-14 | Loss_e: 3.6410e-08\n",
      " Loss: 1.0913e-08 | Loss_d: 6.9633e-13 | Loss_e: 3.6375e-08\n",
      " Loss: 1.0978e-08 | Loss_d: 2.0464e-12 | Loss_e: 3.6586e-08\n",
      " Loss: 1.0912e-08 | Loss_d: 1.4211e-12 | Loss_e: 3.6369e-08\n",
      " Loss: 1.0907e-08 | Loss_d: 1.4211e-14 | Loss_e: 3.6357e-08\n",
      " Loss: 1.0961e-08 | Loss_d: 1.5476e-11 | Loss_e: 3.6484e-08\n",
      " Loss: 1.0907e-08 | Loss_d: 8.8818e-14 | Loss_e: 3.6355e-08\n",
      " Loss: 1.0899e-08 | Loss_d: 4.2988e-13 | Loss_e: 3.6328e-08\n",
      " Loss: 1.0949e-08 | Loss_d: 3.6380e-12 | Loss_e: 3.6483e-08\n",
      " Loss: 1.0898e-08 | Loss_d: 1.2790e-13 | Loss_e: 3.6326e-08\n",
      " Loss: 1.0883e-08 | Loss_d: 1.8794e-12 | Loss_e: 3.6270e-08\n",
      " Loss: 1.0871e-08 | Loss_d: 3.4120e-11 | Loss_e: 3.6121e-08\n",
      " Loss: 1.0869e-08 | Loss_d: 1.4552e-11 | Loss_e: 3.6180e-08\n",
      " Loss: 1.0812e-08 | Loss_d: 2.0520e-11 | Loss_e: 3.5973e-08\n",
      " Loss: 1.0819e-08 | Loss_d: 6.8642e-11 | Loss_e: 3.5835e-08\n",
      " Loss: 1.0768e-08 | Loss_d: 3.7691e-11 | Loss_e: 3.5766e-08\n",
      " Loss: 1.0586e-08 | Loss_d: 8.2082e-11 | Loss_e: 3.5014e-08\n",
      " Loss: 1.3872e-08 | Loss_d: 1.5103e-09 | Loss_e: 4.1206e-08\n",
      " Loss: 1.0573e-08 | Loss_d: 9.5554e-11 | Loss_e: 3.4926e-08\n",
      " Loss: 1.0407e-08 | Loss_d: 1.0633e-10 | Loss_e: 3.4336e-08\n",
      " Loss: 1.1721e-08 | Loss_d: 2.8053e-10 | Loss_e: 3.8136e-08\n",
      " Loss: 1.0362e-08 | Loss_d: 1.0388e-10 | Loss_e: 3.4193e-08\n",
      " Loss: 1.0408e-08 | Loss_d: 3.8689e-12 | Loss_e: 3.4680e-08\n",
      " Loss: 1.0285e-08 | Loss_d: 2.8141e-11 | Loss_e: 3.4188e-08\n",
      " Loss: 1.0172e-08 | Loss_d: 3.0070e-11 | Loss_e: 3.3806e-08\n",
      " Loss: 1.0135e-08 | Loss_d: 1.7408e-11 | Loss_e: 3.3726e-08\n",
      " Loss: 1.3343e-08 | Loss_d: 5.9430e-10 | Loss_e: 4.2496e-08\n",
      " Loss: 1.0127e-08 | Loss_d: 6.5690e-12 | Loss_e: 3.3734e-08\n",
      " Loss: 1.0023e-08 | Loss_d: 1.1141e-11 | Loss_e: 3.3372e-08\n",
      " Loss: 1.2952e-08 | Loss_d: 6.9094e-10 | Loss_e: 4.0869e-08\n",
      " Loss: 1.0023e-08 | Loss_d: 9.2406e-12 | Loss_e: 3.3378e-08\n",
      " Loss: 9.9599e-09 | Loss_d: 1.0267e-12 | Loss_e: 3.3196e-08\n",
      " Loss: 9.9606e-09 | Loss_d: 1.2790e-13 | Loss_e: 3.3201e-08\n",
      " Loss: 9.9411e-09 | Loss_d: 1.2790e-13 | Loss_e: 3.3136e-08\n",
      " Loss: 1.0133e-08 | Loss_d: 1.0747e-11 | Loss_e: 3.3742e-08\n",
      " Loss: 9.9403e-09 | Loss_d: 0.0000e+00 | Loss_e: 3.3134e-08\n",
      " Loss: 9.9609e-09 | Loss_d: 2.7512e-11 | Loss_e: 3.3111e-08\n",
      " Loss: 9.9296e-09 | Loss_d: 4.6043e-12 | Loss_e: 3.3083e-08\n",
      " Loss: 9.9122e-09 | Loss_d: 1.0267e-12 | Loss_e: 3.3037e-08\n",
      " Loss: 9.9098e-09 | Loss_d: 8.8818e-12 | Loss_e: 3.3003e-08\n",
      " Loss: 9.8980e-09 | Loss_d: 1.4211e-12 | Loss_e: 3.2989e-08\n",
      " Loss: 9.8532e-09 | Loss_d: 2.4016e-12 | Loss_e: 3.2836e-08\n",
      " Loss: 9.7514e-09 | Loss_d: 1.4211e-14 | Loss_e: 3.2505e-08\n",
      " Loss: 2.3839e-08 | Loss_d: 3.7691e-09 | Loss_e: 6.6899e-08\n",
      " Loss: 9.7494e-09 | Loss_d: 3.1974e-14 | Loss_e: 3.2498e-08\n",
      " Loss: 9.5836e-09 | Loss_d: 3.5527e-15 | Loss_e: 3.1945e-08\n",
      " Loss: 1.0421e-08 | Loss_d: 4.8374e-10 | Loss_e: 3.3123e-08\n",
      " Loss: 9.5435e-09 | Loss_d: 1.1511e-12 | Loss_e: 3.1808e-08\n",
      " Loss: 1.1481e-08 | Loss_d: 8.5649e-10 | Loss_e: 3.5417e-08\n",
      " Loss: 9.5135e-09 | Loss_d: 4.6043e-12 | Loss_e: 3.1696e-08\n",
      " Loss: 9.3610e-09 | Loss_d: 3.8426e-11 | Loss_e: 3.1075e-08\n",
      " Loss: 9.3478e-09 | Loss_d: 2.2737e-11 | Loss_e: 3.1083e-08\n",
      " Loss: 9.2700e-09 | Loss_d: 5.9121e-11 | Loss_e: 3.0703e-08\n",
      " Loss: 9.2774e-09 | Loss_d: 5.4627e-11 | Loss_e: 3.0743e-08\n",
      " Loss: 9.0589e-09 | Loss_d: 9.2406e-12 | Loss_e: 3.0165e-08\n",
      " Loss: 1.2196e-08 | Loss_d: 1.8794e-10 | Loss_e: 4.0027e-08\n",
      " Loss: 9.0415e-09 | Loss_d: 4.8637e-12 | Loss_e: 3.0122e-08\n",
      " Loss: 8.9023e-09 | Loss_d: 3.6962e-11 | Loss_e: 2.9551e-08\n",
      " Loss: 2.0497e-08 | Loss_d: 4.2988e-09 | Loss_e: 5.3995e-08\n",
      " Loss: 8.8966e-09 | Loss_d: 3.3427e-11 | Loss_e: 2.9544e-08\n",
      " Loss: 8.9158e-09 | Loss_d: 2.5899e-12 | Loss_e: 2.9711e-08\n",
      " Loss: 8.7958e-09 | Loss_d: 6.5690e-12 | Loss_e: 2.9297e-08\n",
      " Loss: 8.6343e-09 | Loss_d: 3.6962e-11 | Loss_e: 2.8658e-08\n",
      " Loss: 8.6430e-09 | Loss_d: 2.4475e-11 | Loss_e: 2.8728e-08\n",
      " Loss: 8.5933e-09 | Loss_d: 2.3888e-11 | Loss_e: 2.8565e-08\n",
      " Loss: 8.5605e-09 | Loss_d: 2.1064e-11 | Loss_e: 2.8465e-08\n",
      " Loss: 9.7671e-09 | Loss_d: 8.8463e-10 | Loss_e: 2.9608e-08\n",
      " Loss: 8.5465e-09 | Loss_d: 1.0267e-12 | Loss_e: 2.8485e-08\n",
      " Loss: 8.4466e-09 | Loss_d: 3.5527e-15 | Loss_e: 2.8155e-08\n",
      " Loss: 8.8830e-09 | Loss_d: 5.2879e-11 | Loss_e: 2.9434e-08\n",
      " Loss: 8.4241e-09 | Loss_d: 9.0949e-13 | Loss_e: 2.8077e-08\n",
      " Loss: 8.4243e-09 | Loss_d: 7.8874e-11 | Loss_e: 2.7818e-08\n",
      " Loss: 8.3499e-09 | Loss_d: 5.6843e-12 | Loss_e: 2.7814e-08\n",
      " Loss: 8.2154e-09 | Loss_d: 7.2649e-11 | Loss_e: 2.7143e-08\n",
      " Loss: 8.1966e-09 | Loss_d: 2.8777e-11 | Loss_e: 2.7226e-08\n",
      " Loss: 8.0782e-09 | Loss_d: 1.4101e-11 | Loss_e: 2.6880e-08\n",
      " Loss: 8.5317e-09 | Loss_d: 6.6681e-11 | Loss_e: 2.8217e-08\n",
      " Loss: 8.0575e-09 | Loss_d: 1.2790e-11 | Loss_e: 2.6816e-08\n",
      " Loss: 8.9383e-09 | Loss_d: 8.8818e-14 | Loss_e: 2.9794e-08\n",
      " Loss: 7.9635e-09 | Loss_d: 3.5527e-13 | Loss_e: 2.6544e-08\n",
      " Loss: 1.7377e-08 | Loss_d: 3.1059e-09 | Loss_e: 4.7571e-08\n",
      " Loss: 7.9480e-09 | Loss_d: 1.5667e-12 | Loss_e: 2.6488e-08\n",
      " Loss: 7.9859e-09 | Loss_d: 9.0949e-11 | Loss_e: 2.6316e-08\n",
      " Loss: 7.8966e-09 | Loss_d: 1.0360e-11 | Loss_e: 2.6287e-08\n",
      " Loss: 7.9984e-09 | Loss_d: 1.1005e-10 | Loss_e: 2.6294e-08\n",
      " Loss: 7.8066e-09 | Loss_d: 1.3657e-11 | Loss_e: 2.5977e-08\n",
      " Loss: 7.6766e-09 | Loss_d: 1.9455e-11 | Loss_e: 2.5524e-08\n",
      " Loss: 7.6837e-09 | Loss_d: 2.3309e-11 | Loss_e: 2.5535e-08\n",
      " Loss: 7.6044e-09 | Loss_d: 3.5527e-15 | Loss_e: 2.5348e-08\n",
      " Loss: 8.7156e-09 | Loss_d: 3.8455e-10 | Loss_e: 2.7770e-08\n",
      " Loss: 7.6066e-09 | Loss_d: 1.4211e-14 | Loss_e: 2.5355e-08\n",
      " Loss: 7.3809e-09 | Loss_d: 5.1301e-12 | Loss_e: 2.4586e-08\n",
      " Loss: 3.1205e-08 | Loss_d: 7.2142e-09 | Loss_e: 7.9969e-08\n",
      " Loss: 7.3762e-09 | Loss_d: 4.3521e-12 | Loss_e: 2.4573e-08\n",
      " Loss: 7.3975e-09 | Loss_d: 1.5667e-10 | Loss_e: 2.4136e-08\n",
      " Loss: 7.3132e-09 | Loss_d: 2.6276e-11 | Loss_e: 2.4290e-08\n",
      " Loss: 7.2030e-09 | Loss_d: 1.1951e-11 | Loss_e: 2.3970e-08\n",
      " Loss: 7.9215e-09 | Loss_d: 1.0027e-10 | Loss_e: 2.6071e-08\n",
      " Loss: 7.1940e-09 | Loss_d: 7.1942e-12 | Loss_e: 2.3956e-08\n",
      " Loss: 7.2697e-09 | Loss_d: 3.6241e-11 | Loss_e: 2.4112e-08\n",
      " Loss: 7.1387e-09 | Loss_d: 6.2670e-12 | Loss_e: 2.3775e-08\n",
      " Loss: 7.0779e-09 | Loss_d: 2.2172e-11 | Loss_e: 2.3519e-08\n",
      " Loss: 7.0614e-09 | Loss_d: 1.5010e-11 | Loss_e: 2.3488e-08\n",
      " Loss: 7.0266e-09 | Loss_d: 3.4142e-12 | Loss_e: 2.3411e-08\n",
      " Loss: 7.0250e-09 | Loss_d: 1.7195e-12 | Loss_e: 2.3411e-08\n",
      " Loss: 6.9880e-09 | Loss_d: 1.4211e-12 | Loss_e: 2.3288e-08\n",
      " Loss: 8.0429e-09 | Loss_d: 1.7352e-10 | Loss_e: 2.6231e-08\n",
      " Loss: 6.9860e-09 | Loss_d: 5.1159e-13 | Loss_e: 2.3285e-08\n",
      " Loss: 6.9436e-09 | Loss_d: 8.8818e-14 | Loss_e: 2.3145e-08\n",
      " Loss: 7.2678e-09 | Loss_d: 1.5370e-10 | Loss_e: 2.3714e-08\n",
      " Loss: 6.9359e-09 | Loss_d: 5.1159e-13 | Loss_e: 2.3118e-08\n",
      " Loss: 7.2638e-09 | Loss_d: 4.7805e-11 | Loss_e: 2.4053e-08\n",
      " Loss: 6.9295e-09 | Loss_d: 4.2988e-13 | Loss_e: 2.3097e-08\n",
      " Loss: 6.9119e-09 | Loss_d: 1.4211e-14 | Loss_e: 2.3040e-08\n",
      " Loss: 6.9320e-09 | Loss_d: 1.5667e-12 | Loss_e: 2.3101e-08\n",
      " Loss: 6.9038e-09 | Loss_d: 1.2790e-13 | Loss_e: 2.3012e-08\n",
      " Loss: 6.9467e-09 | Loss_d: 4.9468e-11 | Loss_e: 2.2991e-08\n",
      " Loss: 6.8600e-09 | Loss_d: 2.8777e-13 | Loss_e: 2.2866e-08\n",
      " Loss: 1.1066e-08 | Loss_d: 1.1062e-09 | Loss_e: 3.3200e-08\n",
      " Loss: 6.8384e-09 | Loss_d: 5.1159e-13 | Loss_e: 2.2793e-08\n",
      " Loss: 7.3338e-09 | Loss_d: 1.4211e-10 | Loss_e: 2.3972e-08\n",
      " Loss: 6.8032e-09 | Loss_d: 5.1159e-13 | Loss_e: 2.2676e-08\n",
      " Loss: 6.7562e-09 | Loss_d: 1.6428e-11 | Loss_e: 2.2466e-08\n",
      " Loss: 6.6975e-09 | Loss_d: 7.9936e-13 | Loss_e: 2.2322e-08\n",
      " Loss: 6.6033e-09 | Loss_d: 1.7408e-11 | Loss_e: 2.1953e-08\n",
      " Loss: 6.5983e-09 | Loss_d: 9.6065e-12 | Loss_e: 2.1962e-08\n",
      " Loss: 6.7199e-09 | Loss_d: 3.7691e-11 | Loss_e: 2.2274e-08\n",
      " Loss: 6.5670e-09 | Loss_d: 3.1974e-12 | Loss_e: 2.1879e-08\n",
      " Loss: 6.5091e-09 | Loss_d: 5.9721e-12 | Loss_e: 2.1677e-08\n",
      " Loss: 7.5597e-09 | Loss_d: 6.2074e-10 | Loss_e: 2.3130e-08\n",
      " Loss: 6.5088e-09 | Loss_d: 8.1855e-12 | Loss_e: 2.1669e-08\n",
      " Loss: 6.4714e-09 | Loss_d: 3.1974e-12 | Loss_e: 2.1561e-08\n",
      " Loss: 6.5597e-09 | Loss_d: 1.1543e-11 | Loss_e: 2.1827e-08\n",
      " Loss: 6.4575e-09 | Loss_d: 1.7408e-13 | Loss_e: 2.1525e-08\n",
      " Loss: 6.3587e-09 | Loss_d: 5.1301e-12 | Loss_e: 2.1178e-08\n",
      " Loss: 1.5309e-08 | Loss_d: 2.9098e-09 | Loss_e: 4.1332e-08\n",
      " Loss: 6.3552e-09 | Loss_d: 4.1069e-12 | Loss_e: 2.1170e-08\n",
      " Loss: 6.4050e-09 | Loss_d: 5.9121e-11 | Loss_e: 2.1153e-08\n",
      " Loss: 6.3273e-09 | Loss_d: 1.1141e-11 | Loss_e: 2.1054e-08\n",
      " Loss: 6.2288e-09 | Loss_d: 1.2825e-12 | Loss_e: 2.0758e-08\n",
      " Loss: 6.2269e-09 | Loss_d: 8.8818e-14 | Loss_e: 2.0756e-08\n",
      " Loss: 6.2240e-09 | Loss_d: 2.2172e-11 | Loss_e: 2.0673e-08\n",
      " Loss: 6.1984e-09 | Loss_d: 8.5301e-12 | Loss_e: 2.0633e-08\n",
      " Loss: 6.0934e-09 | Loss_d: 2.2172e-11 | Loss_e: 2.0237e-08\n",
      " Loss: 7.9130e-09 | Loss_d: 8.4606e-10 | Loss_e: 2.3556e-08\n",
      " Loss: 6.0736e-09 | Loss_d: 1.9984e-11 | Loss_e: 2.0179e-08\n",
      " Loss: 6.0710e-09 | Loss_d: 2.8777e-11 | Loss_e: 2.0141e-08\n",
      " Loss: 5.9751e-09 | Loss_d: 2.9878e-12 | Loss_e: 1.9907e-08\n",
      " Loss: 8.5058e-09 | Loss_d: 1.0865e-09 | Loss_e: 2.4731e-08\n",
      " Loss: 5.9444e-09 | Loss_d: 9.0949e-13 | Loss_e: 1.9811e-08\n",
      " Loss: 5.9628e-09 | Loss_d: 9.3237e-11 | Loss_e: 1.9565e-08\n",
      " Loss: 5.8723e-09 | Loss_d: 4.2988e-13 | Loss_e: 1.9573e-08\n",
      " Loss: 4.6620e-08 | Loss_d: 1.5076e-08 | Loss_e: 1.0515e-07\n",
      " Loss: 5.8641e-09 | Loss_d: 1.1511e-12 | Loss_e: 1.9543e-08\n",
      " Loss: 5.8400e-09 | Loss_d: 5.6403e-11 | Loss_e: 1.9279e-08\n",
      " Loss: 5.7566e-09 | Loss_d: 8.1855e-12 | Loss_e: 1.9161e-08\n",
      " Loss: 5.7566e-09 | Loss_d: 9.2406e-12 | Loss_e: 1.9158e-08\n",
      " Loss: 5.7115e-09 | Loss_d: 1.2367e-11 | Loss_e: 1.8997e-08\n",
      " Loss: 5.6612e-09 | Loss_d: 6.2844e-11 | Loss_e: 1.8661e-08\n",
      " Loss: 5.6429e-09 | Loss_d: 2.7512e-11 | Loss_e: 1.8718e-08\n",
      " Loss: 5.5271e-09 | Loss_d: 4.2210e-11 | Loss_e: 1.8283e-08\n",
      " Loss: 8.3857e-09 | Loss_d: 1.3481e-09 | Loss_e: 2.3459e-08\n",
      " Loss: 5.5259e-09 | Loss_d: 5.3749e-11 | Loss_e: 1.8240e-08\n",
      " Loss: 5.2201e-09 | Loss_d: 2.2172e-11 | Loss_e: 1.7326e-08\n",
      " Loss: 1.7898e-08 | Loss_d: 3.9918e-09 | Loss_e: 4.6353e-08\n",
      " Loss: 5.2088e-09 | Loss_d: 2.3309e-11 | Loss_e: 1.7285e-08\n",
      " Loss: 4.9031e-09 | Loss_d: 3.7691e-11 | Loss_e: 1.6218e-08\n",
      " Loss: 8.1368e-09 | Loss_d: 3.2266e-09 | Loss_e: 1.6367e-08\n",
      " Loss: 4.8504e-09 | Loss_d: 4.1439e-11 | Loss_e: 1.6030e-08\n",
      " Loss: 2.7542e-08 | Loss_d: 9.3583e-09 | Loss_e: 6.0611e-08\n",
      " Loss: 4.8424e-09 | Loss_d: 5.4627e-11 | Loss_e: 1.5959e-08\n",
      " Loss: 5.1411e-09 | Loss_d: 3.8222e-10 | Loss_e: 1.5863e-08\n",
      " Loss: 4.7320e-09 | Loss_d: 7.0632e-11 | Loss_e: 1.5538e-08\n",
      " Loss: 1.6842e-08 | Loss_d: 7.8240e-09 | Loss_e: 3.0060e-08\n",
      " Loss: 4.6995e-09 | Loss_d: 7.2649e-11 | Loss_e: 1.5423e-08\n",
      " Loss: 5.0622e-09 | Loss_d: 7.9821e-10 | Loss_e: 1.4213e-08\n",
      " Loss: 4.6399e-09 | Loss_d: 1.2028e-10 | Loss_e: 1.5065e-08\n",
      " Loss: 4.5907e-09 | Loss_d: 2.7853e-10 | Loss_e: 1.4374e-08\n",
      " Loss: 4.5217e-09 | Loss_d: 1.4930e-10 | Loss_e: 1.4575e-08\n",
      " Loss: 4.2596e-09 | Loss_d: 1.6729e-10 | Loss_e: 1.3641e-08\n",
      " Loss: 1.2877e-08 | Loss_d: 4.0599e-09 | Loss_e: 2.9390e-08\n",
      " Loss: 4.2696e-09 | Loss_d: 1.8958e-10 | Loss_e: 1.3600e-08\n",
      " Loss: 3.8662e-09 | Loss_d: 4.1069e-12 | Loss_e: 1.2874e-08\n",
      " Loss: 4.5133e-09 | Loss_d: 6.8781e-10 | Loss_e: 1.2752e-08\n",
      " Loss: 3.7757e-09 | Loss_d: 1.0388e-10 | Loss_e: 1.2239e-08\n",
      " Loss: 4.0430e-09 | Loss_d: 1.0147e-10 | Loss_e: 1.3138e-08\n",
      " Loss: 3.6361e-09 | Loss_d: 3.4120e-11 | Loss_e: 1.2007e-08\n",
      " Loss: 3.4003e-09 | Loss_d: 5.0310e-11 | Loss_e: 1.1167e-08\n",
      " Loss: 3.3785e-09 | Loss_d: 1.3220e-11 | Loss_e: 1.1218e-08\n",
      " Loss: 3.2694e-09 | Loss_d: 2.4475e-11 | Loss_e: 1.0816e-08\n",
      " Loss: 3.8000e-09 | Loss_d: 3.7065e-10 | Loss_e: 1.1431e-08\n",
      " Loss: 3.2488e-09 | Loss_d: 4.8633e-11 | Loss_e: 1.0667e-08\n",
      " Loss: 3.2318e-09 | Loss_d: 1.5667e-12 | Loss_e: 1.0767e-08\n",
      " Loss: 3.1845e-09 | Loss_d: 1.3657e-11 | Loss_e: 1.0569e-08\n",
      " Loss: 3.1241e-09 | Loss_d: 1.1543e-11 | Loss_e: 1.0375e-08\n",
      " Loss: 3.8258e-09 | Loss_d: 1.4785e-10 | Loss_e: 1.2260e-08\n",
      " Loss: 3.1128e-09 | Loss_d: 9.6065e-12 | Loss_e: 1.0344e-08\n",
      " Loss: 3.4697e-09 | Loss_d: 2.9878e-12 | Loss_e: 1.1556e-08\n",
      " Loss: 3.0951e-09 | Loss_d: 5.4037e-12 | Loss_e: 1.0299e-08\n",
      " Loss: 3.0206e-09 | Loss_d: 2.2737e-13 | Loss_e: 1.0068e-08\n",
      " Loss: 3.5791e-09 | Loss_d: 3.4820e-11 | Loss_e: 1.1814e-08\n",
      " Loss: 3.0053e-09 | Loss_d: 2.2737e-13 | Loss_e: 1.0017e-08\n",
      " Loss: 2.8522e-09 | Loss_d: 1.1951e-11 | Loss_e: 9.4675e-09\n",
      " Loss: 2.7705e-09 | Loss_d: 8.8818e-12 | Loss_e: 9.2055e-09\n",
      " Loss: 2.7014e-09 | Loss_d: 3.6241e-11 | Loss_e: 8.8840e-09\n",
      " Loss: 3.0596e-09 | Loss_d: 1.5519e-10 | Loss_e: 9.6813e-09\n",
      " Loss: 2.6474e-09 | Loss_d: 1.3657e-11 | Loss_e: 8.7792e-09\n",
      " Loss: 2.4808e-09 | Loss_d: 1.1256e-10 | Loss_e: 7.8940e-09\n",
      " Loss: 2.4068e-09 | Loss_d: 7.5175e-12 | Loss_e: 7.9975e-09\n",
      " Loss: 2.3737e-09 | Loss_d: 1.0267e-12 | Loss_e: 7.9089e-09\n",
      " Loss: 2.3711e-09 | Loss_d: 6.0041e-13 | Loss_e: 7.9017e-09\n",
      " Loss: 2.3580e-09 | Loss_d: 1.7408e-11 | Loss_e: 7.8019e-09\n",
      " Loss: 2.3535e-09 | Loss_d: 9.2406e-12 | Loss_e: 7.8142e-09\n",
      " Loss: 2.3334e-09 | Loss_d: 2.5899e-12 | Loss_e: 7.7693e-09\n",
      " Loss: 2.4205e-09 | Loss_d: 4.8637e-12 | Loss_e: 8.0521e-09\n",
      " Loss: 2.3303e-09 | Loss_d: 1.5667e-12 | Loss_e: 7.7625e-09\n",
      " Loss: 2.3015e-09 | Loss_d: 2.9878e-12 | Loss_e: 7.6618e-09\n",
      " Loss: 2.6159e-09 | Loss_d: 1.1256e-10 | Loss_e: 8.3446e-09\n",
      " Loss: 2.2999e-09 | Loss_d: 4.3521e-12 | Loss_e: 7.6520e-09\n",
      " Loss: 2.2971e-09 | Loss_d: 2.9878e-12 | Loss_e: 7.6470e-09\n",
      " Loss: 2.2828e-09 | Loss_d: 5.6843e-14 | Loss_e: 7.6093e-09\n",
      " Loss: 2.2792e-09 | Loss_d: 5.1159e-13 | Loss_e: 7.5957e-09\n",
      " Loss: 2.3022e-09 | Loss_d: 9.2406e-12 | Loss_e: 7.6431e-09\n",
      " Loss: 2.2775e-09 | Loss_d: 1.2790e-13 | Loss_e: 7.5913e-09\n",
      " Loss: 2.2780e-09 | Loss_d: 2.8777e-13 | Loss_e: 7.5925e-09\n",
      " Loss: 2.2785e-09 | Loss_d: 5.1159e-13 | Loss_e: 7.5932e-09\n",
      " Loss: 2.2774e-09 | Loss_d: 6.0041e-13 | Loss_e: 7.5893e-09\n",
      " Loss: 2.2774e-09 | Loss_d: 1.2790e-13 | Loss_e: 7.5909e-09\n",
      " Loss: 2.2775e-09 | Loss_d: 1.4211e-14 | Loss_e: 7.5915e-09\n",
      " Loss: 2.2765e-09 | Loss_d: 8.8818e-14 | Loss_e: 7.5880e-09\n",
      " Loss: 2.2772e-09 | Loss_d: 5.6843e-14 | Loss_e: 7.5905e-09\n",
      " Loss: 2.2759e-09 | Loss_d: 3.1974e-14 | Loss_e: 7.5861e-09\n",
      " Loss: 2.2772e-09 | Loss_d: 8.8818e-14 | Loss_e: 7.5904e-09\n",
      " Loss: 2.2773e-09 | Loss_d: 2.2737e-13 | Loss_e: 7.5901e-09\n",
      " Loss: 2.2770e-09 | Loss_d: 1.7408e-13 | Loss_e: 7.5893e-09\n",
      " Loss: 2.2777e-09 | Loss_d: 1.7408e-13 | Loss_e: 7.5917e-09\n",
      " Loss: 2.2773e-09 | Loss_d: 5.6843e-14 | Loss_e: 7.5908e-09\n",
      " Loss: 2.2772e-09 | Loss_d: 1.4211e-14 | Loss_e: 7.5905e-09\n",
      " Loss: 2.2776e-09 | Loss_d: 1.7408e-13 | Loss_e: 7.5913e-09\n",
      " Loss: 2.2773e-09 | Loss_d: 1.2790e-13 | Loss_e: 7.5905e-09\n",
      " Loss: 2.2776e-09 | Loss_d: 2.2737e-13 | Loss_e: 7.5913e-09\n",
      " Loss: 2.2772e-09 | Loss_d: 1.2790e-13 | Loss_e: 7.5904e-09\n",
      " Loss: 2.2776e-09 | Loss_d: 2.2737e-13 | Loss_e: 7.5913e-09\n",
      " Loss: 2.2773e-09 | Loss_d: 1.2790e-13 | Loss_e: 7.5904e-09\n",
      " Loss: 2.2773e-09 | Loss_d: 1.2790e-13 | Loss_e: 7.5904e-09\n",
      " Loss: 2.2776e-09 | Loss_d: 2.2737e-13 | Loss_e: 7.5913e-09\n",
      " Loss: 2.2776e-09 | Loss_d: 2.2737e-13 | Loss_e: 7.5913e-09\n",
      " Loss: 2.2776e-09 | Loss_d: 2.2737e-13 | Loss_e: 7.5913e-09\n",
      " Loss: 2.2776e-09 | Loss_d: 2.2737e-13 | Loss_e: 7.5913e-09\n",
      " Loss: 2.2776e-09 | Loss_d: 2.2737e-13 | Loss_e: 7.5913e-09\n",
      " Loss: 2.2773e-09 | Loss_d: 1.2790e-13 | Loss_e: 7.5904e-09\n",
      " Loss: 2.2790e-09 | Loss_d: 8.8818e-14 | Loss_e: 7.5964e-09\n",
      " Loss: 2.2759e-09 | Loss_d: 3.5527e-13 | Loss_e: 7.5852e-09\n",
      " Loss: 2.2721e-09 | Loss_d: 5.6843e-14 | Loss_e: 7.5736e-09\n",
      " Loss: 2.2974e-09 | Loss_d: 1.5948e-11 | Loss_e: 7.6050e-09\n",
      " Loss: 2.2749e-09 | Loss_d: 3.4142e-12 | Loss_e: 7.5716e-09\n",
      " Loss: 2.2692e-09 | Loss_d: 1.0267e-12 | Loss_e: 7.5605e-09\n",
      " Loss: 2.2819e-09 | Loss_d: 1.5667e-12 | Loss_e: 7.6011e-09\n",
      " Loss: 2.2684e-09 | Loss_d: 3.1974e-14 | Loss_e: 7.5613e-09\n",
      " Loss: 2.2629e-09 | Loss_d: 5.1159e-13 | Loss_e: 7.5411e-09\n",
      " Loss: 2.2456e-09 | Loss_d: 4.1069e-12 | Loss_e: 7.4716e-09\n",
      " Loss: 2.1976e-09 | Loss_d: 4.0675e-11 | Loss_e: 7.1896e-09\n",
      " Loss: 2.7684e-09 | Loss_d: 2.2737e-11 | Loss_e: 9.1521e-09\n",
      " Loss: 2.1943e-09 | Loss_d: 4.4565e-11 | Loss_e: 7.1658e-09\n",
      " Loss: 2.4254e-09 | Loss_d: 2.0520e-11 | Loss_e: 8.0163e-09\n",
      " Loss: 2.1831e-09 | Loss_d: 4.6985e-11 | Loss_e: 7.1203e-09\n",
      " Loss: 2.1538e-09 | Loss_d: 5.9721e-12 | Loss_e: 7.1593e-09\n",
      " Loss: 2.1512e-09 | Loss_d: 2.6890e-11 | Loss_e: 7.0810e-09\n",
      " Loss: 3.0898e-09 | Loss_d: 3.0500e-10 | Loss_e: 9.2828e-09\n",
      " Loss: 2.1391e-09 | Loss_d: 2.1064e-11 | Loss_e: 7.0601e-09\n",
      " Loss: 2.0855e-09 | Loss_d: 1.9455e-11 | Loss_e: 6.8867e-09\n",
      " Loss: 2.0675e-09 | Loss_d: 7.9936e-13 | Loss_e: 6.8891e-09\n",
      " Loss: 1.9882e-09 | Loss_d: 1.7408e-11 | Loss_e: 6.5691e-09\n",
      " Loss: 3.4187e-09 | Loss_d: 1.3613e-09 | Loss_e: 6.8583e-09\n",
      " Loss: 1.9829e-09 | Loss_d: 1.7909e-11 | Loss_e: 6.5498e-09\n",
      " Loss: 1.9549e-09 | Loss_d: 8.1855e-12 | Loss_e: 6.4890e-09\n",
      " Loss: 2.2436e-09 | Loss_d: 1.8417e-11 | Loss_e: 7.4173e-09\n",
      " Loss: 1.9519e-09 | Loss_d: 3.1974e-12 | Loss_e: 6.4958e-09\n",
      " Loss: 1.9286e-09 | Loss_d: 6.5690e-12 | Loss_e: 6.4068e-09\n",
      " Loss: 1.9293e-09 | Loss_d: 6.5690e-12 | Loss_e: 6.4091e-09\n",
      " Loss: 1.9259e-09 | Loss_d: 6.5690e-12 | Loss_e: 6.3979e-09\n",
      " Loss: 1.9587e-09 | Loss_d: 3.8689e-12 | Loss_e: 6.5162e-09\n",
      " Loss: 1.9269e-09 | Loss_d: 7.1942e-12 | Loss_e: 6.3991e-09\n",
      " Loss: 1.9186e-09 | Loss_d: 2.2204e-12 | Loss_e: 6.3878e-09\n",
      " Loss: 1.9164e-09 | Loss_d: 8.8818e-12 | Loss_e: 6.3585e-09\n",
      " Loss: 1.9100e-09 | Loss_d: 6.0041e-13 | Loss_e: 6.3648e-09\n",
      " Loss: 1.8966e-09 | Loss_d: 3.5527e-15 | Loss_e: 6.3220e-09\n",
      " Loss: 2.3042e-09 | Loss_d: 1.6270e-10 | Loss_e: 7.1383e-09\n",
      " Loss: 1.8973e-09 | Loss_d: 3.5527e-13 | Loss_e: 6.3232e-09\n",
      " Loss: 1.8910e-09 | Loss_d: 5.6843e-14 | Loss_e: 6.3030e-09\n",
      " Loss: 1.8914e-09 | Loss_d: 1.2790e-13 | Loss_e: 6.3043e-09\n",
      " Loss: 1.8901e-09 | Loss_d: 8.8818e-14 | Loss_e: 6.3000e-09\n",
      " Loss: 1.9027e-09 | Loss_d: 5.4037e-12 | Loss_e: 6.3242e-09\n",
      " Loss: 1.8907e-09 | Loss_d: 3.5527e-15 | Loss_e: 6.3023e-09\n",
      " Loss: 1.8897e-09 | Loss_d: 3.5527e-13 | Loss_e: 6.2979e-09\n",
      " Loss: 1.8891e-09 | Loss_d: 1.8794e-12 | Loss_e: 6.2906e-09\n",
      " Loss: 1.8901e-09 | Loss_d: 1.2825e-12 | Loss_e: 6.2959e-09\n",
      " Loss: 1.8866e-09 | Loss_d: 8.8818e-14 | Loss_e: 6.2885e-09\n",
      " Loss: 1.8837e-09 | Loss_d: 6.9633e-13 | Loss_e: 6.2767e-09\n",
      " Loss: 1.8836e-09 | Loss_d: 2.2737e-13 | Loss_e: 6.2779e-09\n",
      " Loss: 1.8720e-09 | Loss_d: 2.2737e-13 | Loss_e: 6.2392e-09\n",
      " Loss: 1.8566e-09 | Loss_d: 1.4211e-14 | Loss_e: 6.1885e-09\n",
      " Loss: 1.8560e-09 | Loss_d: 1.2825e-12 | Loss_e: 6.1825e-09\n",
      " Loss: 1.8408e-09 | Loss_d: 9.0949e-13 | Loss_e: 6.1331e-09\n",
      " Loss: 1.9998e-09 | Loss_d: 1.1005e-10 | Loss_e: 6.2993e-09\n",
      " Loss: 1.8392e-09 | Loss_d: 1.4211e-12 | Loss_e: 6.1259e-09\n",
      " Loss: 1.8400e-09 | Loss_d: 1.2790e-13 | Loss_e: 6.1329e-09\n",
      " Loss: 1.8387e-09 | Loss_d: 6.9633e-13 | Loss_e: 6.1266e-09\n",
      " Loss: 1.8339e-09 | Loss_d: 3.5527e-15 | Loss_e: 6.1128e-09\n",
      " Loss: 1.8340e-09 | Loss_d: 1.2790e-13 | Loss_e: 6.1129e-09\n",
      " Loss: 1.8326e-09 | Loss_d: 1.2825e-12 | Loss_e: 6.1045e-09\n",
      " Loss: 1.8397e-09 | Loss_d: 1.2367e-11 | Loss_e: 6.0912e-09\n",
      " Loss: 1.8329e-09 | Loss_d: 3.1974e-12 | Loss_e: 6.0990e-09\n",
      " Loss: 1.8281e-09 | Loss_d: 2.2737e-13 | Loss_e: 6.0928e-09\n",
      " Loss: 1.8290e-09 | Loss_d: 1.0267e-12 | Loss_e: 6.0934e-09\n",
      " Loss: 1.8267e-09 | Loss_d: 9.0949e-13 | Loss_e: 6.0861e-09\n",
      " Loss: 1.8434e-09 | Loss_d: 3.4142e-12 | Loss_e: 6.1334e-09\n",
      " Loss: 1.8259e-09 | Loss_d: 7.9936e-13 | Loss_e: 6.0838e-09\n",
      " Loss: 1.8238e-09 | Loss_d: 1.4211e-14 | Loss_e: 6.0793e-09\n",
      " Loss: 1.8354e-09 | Loss_d: 2.8777e-13 | Loss_e: 6.1170e-09\n",
      " Loss: 1.8229e-09 | Loss_d: 3.1974e-14 | Loss_e: 6.0762e-09\n",
      " Loss: 1.8214e-09 | Loss_d: 5.6843e-14 | Loss_e: 6.0712e-09\n",
      " Loss: 1.8316e-09 | Loss_d: 1.2825e-12 | Loss_e: 6.1009e-09\n",
      " Loss: 1.8201e-09 | Loss_d: 1.4211e-14 | Loss_e: 6.0669e-09\n",
      " Loss: 1.8160e-09 | Loss_d: 3.5527e-13 | Loss_e: 6.0523e-09\n",
      " Loss: 1.8261e-09 | Loss_d: 1.4552e-11 | Loss_e: 6.0386e-09\n",
      " Loss: 1.8141e-09 | Loss_d: 2.4016e-12 | Loss_e: 6.0388e-09\n",
      " Loss: 1.8060e-09 | Loss_d: 6.0041e-13 | Loss_e: 6.0178e-09\n",
      " Loss: 2.2571e-09 | Loss_d: 1.7039e-10 | Loss_e: 6.9556e-09\n",
      " Loss: 1.8053e-09 | Loss_d: 5.6843e-14 | Loss_e: 6.0175e-09\n",
      " Loss: 1.7917e-09 | Loss_d: 2.8777e-13 | Loss_e: 5.9713e-09\n",
      " Loss: 1.8949e-09 | Loss_d: 1.5010e-11 | Loss_e: 6.2664e-09\n",
      " Loss: 1.7886e-09 | Loss_d: 1.2825e-12 | Loss_e: 5.9577e-09\n",
      " Loss: 1.7728e-09 | Loss_d: 1.2790e-13 | Loss_e: 5.9090e-09\n",
      " Loss: 1.7727e-09 | Loss_d: 5.6843e-14 | Loss_e: 5.9088e-09\n",
      " Loss: 1.7689e-09 | Loss_d: 9.0949e-13 | Loss_e: 5.8932e-09\n",
      " Loss: 1.7930e-09 | Loss_d: 1.1543e-11 | Loss_e: 5.9381e-09\n",
      " Loss: 1.7679e-09 | Loss_d: 1.7195e-12 | Loss_e: 5.8874e-09\n",
      " Loss: 1.7657e-09 | Loss_d: 3.5527e-15 | Loss_e: 5.8858e-09\n",
      " Loss: 1.7660e-09 | Loss_d: 5.6843e-14 | Loss_e: 5.8864e-09\n",
      " Loss: 1.7575e-09 | Loss_d: 3.5527e-13 | Loss_e: 5.8572e-09\n",
      " Loss: 1.7439e-09 | Loss_d: 6.5690e-12 | Loss_e: 5.7910e-09\n",
      " Loss: 1.9853e-09 | Loss_d: 5.4627e-11 | Loss_e: 6.4356e-09\n",
      " Loss: 1.7445e-09 | Loss_d: 9.6065e-12 | Loss_e: 5.7831e-09\n",
      " Loss: 1.6908e-09 | Loss_d: 8.1855e-12 | Loss_e: 5.6086e-09\n",
      " Loss: 1.6157e-09 | Loss_d: 1.4552e-11 | Loss_e: 5.3370e-09\n",
      " Loss: 1.6111e-09 | Loss_d: 1.3220e-11 | Loss_e: 5.3263e-09\n",
      " Loss: 1.5401e-09 | Loss_d: 1.2367e-11 | Loss_e: 5.0924e-09\n",
      " Loss: 1.4556e-09 | Loss_d: 7.9936e-13 | Loss_e: 4.8494e-09\n",
      " Loss: 1.3697e-09 | Loss_d: 1.7408e-13 | Loss_e: 4.5652e-09\n",
      " Loss: 2.1786e-09 | Loss_d: 1.2557e-10 | Loss_e: 6.8435e-09\n",
      " Loss: 1.3645e-09 | Loss_d: 9.0949e-13 | Loss_e: 4.5452e-09\n",
      " Loss: 1.2999e-09 | Loss_d: 2.2737e-13 | Loss_e: 4.3322e-09\n",
      " Loss: 1.2995e-09 | Loss_d: 0.0000e+00 | Loss_e: 4.3318e-09\n",
      " Loss: 1.2042e-09 | Loss_d: 2.0464e-12 | Loss_e: 4.0073e-09\n",
      " Loss: 3.0340e-09 | Loss_d: 7.9485e-10 | Loss_e: 7.4639e-09\n",
      " Loss: 1.2001e-09 | Loss_d: 3.1974e-12 | Loss_e: 3.9898e-09\n",
      " Loss: 1.1864e-09 | Loss_d: 3.1974e-14 | Loss_e: 3.9547e-09\n",
      " Loss: 1.1719e-09 | Loss_d: 2.2204e-12 | Loss_e: 3.8990e-09\n",
      " Loss: 1.1424e-09 | Loss_d: 2.4475e-11 | Loss_e: 3.7266e-09\n",
      " Loss: 1.1393e-09 | Loss_d: 8.5301e-12 | Loss_e: 3.7691e-09\n",
      " Loss: 1.1161e-09 | Loss_d: 2.9420e-11 | Loss_e: 3.6221e-09\n",
      " Loss: 1.1144e-09 | Loss_d: 2.3309e-11 | Loss_e: 3.6370e-09\n",
      " Loss: 1.0549e-09 | Loss_d: 7.1942e-12 | Loss_e: 3.4923e-09\n",
      " Loss: 1.4632e-09 | Loss_d: 1.1130e-10 | Loss_e: 4.5063e-09\n",
      " Loss: 1.0436e-09 | Loss_d: 6.8781e-12 | Loss_e: 3.4557e-09\n",
      " Loss: 1.0088e-09 | Loss_d: 5.1159e-13 | Loss_e: 3.3610e-09\n",
      " Loss: 1.0335e-09 | Loss_d: 4.5365e-11 | Loss_e: 3.2939e-09\n",
      " Loss: 4.2518e-08 | Loss_d: 2.1992e-08 | Loss_e: 6.8422e-08\n",
      " Loss: 1.0277e-09 | Loss_d: 4.0675e-11 | Loss_e: 3.2901e-09\n",
      " Loss: 8.3180e-06 | Loss_d: 4.8356e-06 | Loss_e: 1.1608e-05\n",
      " Loss: 1.0196e-09 | Loss_d: 3.4120e-11 | Loss_e: 3.2851e-09\n",
      " Loss: 1.0175e-09 | Loss_d: 3.6962e-11 | Loss_e: 3.2686e-09\n",
      " Loss: 5.4703e-07 | Loss_d: 2.9868e-07 | Loss_e: 8.2783e-07\n",
      " Loss: 1.0120e-09 | Loss_d: 3.2742e-11 | Loss_e: 3.2642e-09\n",
      " Loss: 9.5786e-10 | Loss_d: 7.5175e-12 | Loss_e: 3.1678e-09\n",
      " Loss: 9.1673e-09 | Loss_d: 4.5044e-09 | Loss_e: 1.5543e-08\n",
      " Loss: 9.5702e-10 | Loss_d: 9.2406e-12 | Loss_e: 3.1592e-09\n",
      " Loss: 1.7198e-09 | Loss_d: 4.4270e-10 | Loss_e: 4.2570e-09\n",
      " Loss: 9.4569e-10 | Loss_d: 4.6043e-12 | Loss_e: 3.1369e-09\n",
      " Loss: 1.2314e-09 | Loss_d: 2.7655e-10 | Loss_e: 3.1827e-09\n",
      " Loss: 9.2710e-10 | Loss_d: 1.4211e-14 | Loss_e: 3.0903e-09\n",
      " Loss: 8.9439e-10 | Loss_d: 7.5175e-12 | Loss_e: 2.9562e-09\n",
      " Loss: 1.4541e-09 | Loss_d: 3.3484e-10 | Loss_e: 3.7308e-09\n",
      " Loss: 8.9383e-10 | Loss_d: 9.2406e-12 | Loss_e: 2.9486e-09\n",
      " Loss: 8.8781e-10 | Loss_d: 2.2204e-12 | Loss_e: 2.9520e-09\n",
      " Loss: 8.8696e-10 | Loss_d: 1.2825e-12 | Loss_e: 2.9523e-09\n",
      " Loss: 8.8686e-10 | Loss_d: 3.8689e-12 | Loss_e: 2.9433e-09\n",
      " Loss: 8.9201e-10 | Loss_d: 9.2406e-12 | Loss_e: 2.9426e-09\n",
      " Loss: 8.8636e-10 | Loss_d: 4.3521e-12 | Loss_e: 2.9400e-09\n",
      " Loss: 8.8387e-10 | Loss_d: 4.1069e-12 | Loss_e: 2.9326e-09\n",
      " Loss: 9.3018e-10 | Loss_d: 5.1159e-13 | Loss_e: 3.0989e-09\n",
      " Loss: 8.8325e-10 | Loss_d: 3.4142e-12 | Loss_e: 2.9328e-09\n",
      " Loss: 1.0180e-09 | Loss_d: 8.8818e-12 | Loss_e: 3.3637e-09\n",
      " Loss: 8.8146e-10 | Loss_d: 2.5899e-12 | Loss_e: 2.9296e-09\n",
      " Loss: 8.7544e-10 | Loss_d: 5.1159e-13 | Loss_e: 2.9164e-09\n",
      " Loss: 8.6508e-10 | Loss_d: 2.0464e-12 | Loss_e: 2.8768e-09\n",
      " Loss: 9.4808e-10 | Loss_d: 8.1855e-12 | Loss_e: 3.1330e-09\n",
      " Loss: 8.6300e-10 | Loss_d: 1.7195e-12 | Loss_e: 2.8709e-09\n",
      " Loss: 8.5684e-10 | Loss_d: 2.4016e-12 | Loss_e: 2.8481e-09\n",
      " Loss: 8.8967e-10 | Loss_d: 1.3657e-11 | Loss_e: 2.9201e-09\n",
      " Loss: 8.5246e-10 | Loss_d: 6.0041e-13 | Loss_e: 2.8395e-09\n",
      " Loss: 8.4748e-10 | Loss_d: 6.0041e-13 | Loss_e: 2.8229e-09\n",
      " Loss: 8.4761e-10 | Loss_d: 2.8777e-13 | Loss_e: 2.8244e-09\n",
      " Loss: 8.4636e-10 | Loss_d: 8.8818e-14 | Loss_e: 2.8209e-09\n",
      " Loss: 8.4735e-10 | Loss_d: 4.2988e-13 | Loss_e: 2.8231e-09\n",
      " Loss: 8.4365e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.8121e-09\n",
      " Loss: 8.9734e-10 | Loss_d: 1.9455e-11 | Loss_e: 2.9263e-09\n",
      " Loss: 8.4372e-10 | Loss_d: 2.8777e-13 | Loss_e: 2.8114e-09\n",
      " Loss: 8.4062e-10 | Loss_d: 8.8818e-14 | Loss_e: 2.8018e-09\n",
      " Loss: 8.3395e-10 | Loss_d: 1.2790e-13 | Loss_e: 2.7794e-09\n",
      " Loss: 9.1346e-10 | Loss_d: 3.4142e-12 | Loss_e: 3.0335e-09\n",
      " Loss: 8.3349e-10 | Loss_d: 5.6843e-14 | Loss_e: 2.7781e-09\n",
      " Loss: 8.2943e-10 | Loss_d: 1.5667e-12 | Loss_e: 2.7596e-09\n",
      " Loss: 8.2910e-10 | Loss_d: 1.0267e-12 | Loss_e: 2.7602e-09\n",
      " Loss: 8.2747e-10 | Loss_d: 4.2988e-13 | Loss_e: 2.7568e-09\n",
      " Loss: 8.3442e-10 | Loss_d: 2.2737e-13 | Loss_e: 2.7806e-09\n",
      " Loss: 8.2646e-10 | Loss_d: 5.6843e-14 | Loss_e: 2.7547e-09\n",
      " Loss: 8.2371e-10 | Loss_d: 6.9633e-13 | Loss_e: 2.7434e-09\n",
      " Loss: 9.1663e-10 | Loss_d: 5.2879e-11 | Loss_e: 2.8792e-09\n",
      " Loss: 8.2361e-10 | Loss_d: 2.8777e-13 | Loss_e: 2.7444e-09\n",
      " Loss: 8.2324e-10 | Loss_d: 2.0464e-12 | Loss_e: 2.7373e-09\n",
      " Loss: 8.4377e-10 | Loss_d: 5.9721e-12 | Loss_e: 2.7927e-09\n",
      " Loss: 8.2112e-10 | Loss_d: 6.9633e-13 | Loss_e: 2.7347e-09\n",
      " Loss: 8.1845e-10 | Loss_d: 2.8777e-13 | Loss_e: 2.7272e-09\n",
      " Loss: 8.1882e-10 | Loss_d: 1.2825e-12 | Loss_e: 2.7251e-09\n",
      " Loss: 8.1545e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7182e-09\n",
      " Loss: 9.3022e-10 | Loss_d: 5.3749e-11 | Loss_e: 2.9216e-09\n",
      " Loss: 8.1594e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7198e-09\n",
      " Loss: 8.1539e-10 | Loss_d: 1.5667e-12 | Loss_e: 2.7128e-09\n",
      " Loss: 8.3396e-10 | Loss_d: 1.0267e-12 | Loss_e: 2.7764e-09\n",
      " Loss: 8.1402e-10 | Loss_d: 1.2790e-13 | Loss_e: 2.7130e-09\n",
      " Loss: 8.1552e-10 | Loss_d: 2.2204e-12 | Loss_e: 2.7110e-09\n",
      " Loss: 8.1351e-10 | Loss_d: 1.1511e-12 | Loss_e: 2.7079e-09\n",
      " Loss: 8.1154e-10 | Loss_d: 1.2790e-13 | Loss_e: 2.7047e-09\n",
      " Loss: 8.2339e-10 | Loss_d: 6.9633e-13 | Loss_e: 2.7423e-09\n",
      " Loss: 8.1244e-10 | Loss_d: 1.7408e-13 | Loss_e: 2.7075e-09\n",
      " Loss: 8.1359e-10 | Loss_d: 1.0267e-12 | Loss_e: 2.7085e-09\n",
      " Loss: 8.1169e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1196e-10 | Loss_d: 5.6843e-14 | Loss_e: 2.7063e-09\n",
      " Loss: 8.1170e-10 | Loss_d: 1.4211e-14 | Loss_e: 2.7056e-09\n",
      " Loss: 8.1161e-10 | Loss_d: 1.2790e-13 | Loss_e: 2.7049e-09\n",
      " Loss: 8.1189e-10 | Loss_d: 4.2988e-13 | Loss_e: 2.7049e-09\n",
      " Loss: 8.1220e-10 | Loss_d: 2.2737e-13 | Loss_e: 2.7066e-09\n",
      " Loss: 8.1195e-10 | Loss_d: 1.2790e-13 | Loss_e: 2.7061e-09\n",
      " Loss: 8.1193e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7064e-09\n",
      " Loss: 8.1220e-10 | Loss_d: 2.8777e-13 | Loss_e: 2.7064e-09\n",
      " Loss: 8.1160e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7052e-09\n",
      " Loss: 8.1192e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7063e-09\n",
      " Loss: 8.1164e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7054e-09\n",
      " Loss: 8.1167e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7056e-09\n",
      " Loss: 8.1165e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1179e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7059e-09\n",
      " Loss: 8.1172e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7057e-09\n",
      " Loss: 8.1171e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7057e-09\n",
      " Loss: 8.1170e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7057e-09\n",
      " Loss: 8.1165e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1166e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1170e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7057e-09\n",
      " Loss: 8.1166e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1166e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1166e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1170e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7057e-09\n",
      " Loss: 8.1170e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7057e-09\n",
      " Loss: 8.1166e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1170e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7057e-09\n",
      " Loss: 8.1244e-10 | Loss_d: 5.1159e-13 | Loss_e: 2.7064e-09\n",
      " Loss: 8.1190e-10 | Loss_d: 2.8777e-13 | Loss_e: 2.7054e-09\n",
      " Loss: 8.1347e-10 | Loss_d: 1.2825e-12 | Loss_e: 2.7073e-09\n",
      " Loss: 8.1220e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7073e-09\n",
      " Loss: 8.1133e-10 | Loss_d: 0.0000e+00 | Loss_e: 2.7044e-09\n",
      " Loss: 8.1194e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7064e-09\n",
      " Loss: 8.1193e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7064e-09\n",
      " Loss: 8.1176e-10 | Loss_d: 5.1159e-13 | Loss_e: 2.7042e-09\n",
      " Loss: 8.1164e-10 | Loss_d: 1.4211e-14 | Loss_e: 2.7054e-09\n",
      " Loss: 8.1170e-10 | Loss_d: 1.4211e-14 | Loss_e: 2.7056e-09\n",
      " Loss: 8.1175e-10 | Loss_d: 1.4211e-14 | Loss_e: 2.7058e-09\n",
      " Loss: 8.1165e-10 | Loss_d: 1.4211e-14 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1184e-10 | Loss_d: 1.4211e-14 | Loss_e: 2.7061e-09\n",
      " Loss: 8.1162e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7054e-09\n",
      " Loss: 8.1191e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7064e-09\n",
      " Loss: 8.1190e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7063e-09\n",
      " Loss: 8.1190e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7062e-09\n",
      " Loss: 8.1173e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7056e-09\n",
      " Loss: 8.1161e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7054e-09\n",
      " Loss: 8.1165e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1173e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7056e-09\n",
      " Loss: 8.1165e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1173e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7056e-09\n",
      " Loss: 8.1165e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1173e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7056e-09\n",
      " Loss: 8.1173e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7056e-09\n",
      " Loss: 8.1165e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1165e-10 | Loss_d: 3.5527e-15 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1173e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7056e-09\n",
      " Loss: 8.4398e-10 | Loss_d: 4.8637e-12 | Loss_e: 2.7971e-09\n",
      " Loss: 8.1264e-10 | Loss_d: 7.9936e-13 | Loss_e: 2.7061e-09\n",
      " Loss: 8.1183e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7060e-09\n",
      " Loss: 8.1223e-10 | Loss_d: 2.8777e-13 | Loss_e: 2.7065e-09\n",
      " Loss: 8.1191e-10 | Loss_d: 8.8818e-14 | Loss_e: 2.7061e-09\n",
      " Loss: 8.1128e-10 | Loss_d: 5.6843e-14 | Loss_e: 2.7041e-09\n",
      " Loss: 8.1239e-10 | Loss_d: 2.2737e-13 | Loss_e: 2.7072e-09\n",
      " Loss: 8.1175e-10 | Loss_d: 8.8818e-14 | Loss_e: 2.7056e-09\n",
      " Loss: 8.1178e-10 | Loss_d: 1.4211e-14 | Loss_e: 2.7059e-09\n",
      " Loss: 8.1182e-10 | Loss_d: 2.2737e-13 | Loss_e: 2.7053e-09\n",
      " Loss: 8.1167e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7054e-09\n",
      " Loss: 8.1153e-10 | Loss_d: 5.6843e-14 | Loss_e: 2.7049e-09\n",
      " Loss: 8.1146e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7048e-09\n",
      " Loss: 8.1165e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7054e-09\n",
      " Loss: 8.1163e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7053e-09\n",
      " Loss: 8.1165e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7054e-09\n",
      " Loss: 8.1168e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1168e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1165e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7054e-09\n",
      " Loss: 8.1168e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1165e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7054e-09\n",
      " Loss: 8.1168e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1168e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1168e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1168e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1168e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1165e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7054e-09\n",
      " Loss: 8.1168e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7055e-09\n",
      " Loss: 8.1165e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7054e-09\n",
      " Loss: 8.1165e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7054e-09\n",
      " Loss: 8.1168e-10 | Loss_d: 3.1974e-14 | Loss_e: 2.7055e-09\n",
      " Total iterations: 1535\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Setting up training variables and collocation points\"\"\"\n",
    "scl=1\n",
    "epoch1 = 5000\n",
    "epoch2 = 20000\n",
    "lw = [0.3, 0.2] # loss weights - lw[0] is gamma from paper, lw[1] ?\n",
    "\n",
    "# initialize the weights and biases of the network\n",
    "trained_params = sol_init_MLP(keys[0], n_hl, n_unit)\n",
    "\n",
    "# prepare the normalization condition\n",
    "z_nm = jnp.array([[0.]])\n",
    "cond_nm = jnp.array([[0.]])\n",
    "cond = dict(cond_nm=[z_nm, cond_nm])\n",
    "\n",
    "# prepare the collocation points to evaluate equation gradient\n",
    "dataf = data_func_create(N_col, ds)\n",
    "# group all the conditions and collocation points\n",
    "data = dataf()\n",
    "\n",
    "pred_u = sol_pred_create(lmt, scl, act_s=0) # create the solution function\n",
    "\n",
    "NN_loss = loss_create(pred_u, cond, lw, loss_ref=1) # calculate the loss function\n",
    "loss0 = NN_loss(trained_params, data)[0]\n",
    "\n",
    "\"\"\"First stage of training\"\"\"\n",
    "lr = 1e-3\n",
    "# training the neural network\n",
    "trained_params, loss1 = adam_optimizer(NN_loss, trained_params, dataf, epoch1, lr=lr)\n",
    "data = dataf()\n",
    "trained_params, loss2 = lbfgs_optimizer(NN_loss, trained_params, data, epoch2)\n",
    "\n",
    "# calculate the equation residue\n",
    "f_u1 = lambda z: pred_u(trained_params, z)\n",
    "fu1_x = lambda z: vectgrad(f_u1, z)[0]\n",
    "\n",
    "# calculate the solution\n",
    "x_star0 = jnp.linspace(-1, 1, 2501)[:, None]\n",
    "x_star = jnp.sign(x_star0) * jnp.abs(x_star0) ** 1 * 1\n",
    "\n",
    "u1_p = f_u1(x_star)\n",
    "u1_xx, u1_x = vectgrad(fu1_x, x_star)\n",
    "\n",
    "f1_p = gov_eqn(f_u1, x_star) # residue of governing equation\n",
    "df1, d2f1 = gov_d3_eqn(f_u1, x_star) # residues of first and second derivatives\n",
    "\n",
    "# generate the last loss\n",
    "loss_all = (loss1+loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Stage Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant Frequency: 1.0 Hz\n",
      "Kappa 1 6.283185307179586\n",
      "Magnitude Prefactor 9.587560969360004e-07\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Finding new scale factor and magnitude prefactor for stage 2\"\"\"\n",
    "g = np.array(f1_p).flatten()\n",
    "N = len(g)\n",
    "sample_rate = N / total_time_range\n",
    "# Perform FFT\n",
    "T = 1.0 / sample_rate\n",
    "yf = fft(g)\n",
    "xf = fftfreq(N, T)[:N//2]\n",
    "dominant_freq_idx = np.argmax(np.abs(yf[:N//2]))\n",
    "dominant_freq = xf[dominant_freq_idx]\n",
    "magnitude = np.abs(yf[dominant_freq_idx]) / N\n",
    "\n",
    "print(f\"Dominant Frequency: {dominant_freq} Hz\")\n",
    "\n",
    "# Calculate kappa for Stage 2\n",
    "if dominant_freq < g.size / (6 * np.pi):\n",
    "    scl2 = 2 * np.pi * dominant_freq\n",
    "    print('Kappa 1', scl2)\n",
    "else:\n",
    "    print('Not enough data: Please Increase')\n",
    "\n",
    "\n",
    "# find magnitude prefactor\n",
    "eps_r1 = np.sqrt(1/len(f1_p) * sum(i ** 2 for i in f1_p.flatten())) # RMS of equation residue\n",
    "u, x = data['z_col'].flatten(), data['z_col'].flatten()\n",
    "f = func(u, x).flatten()\n",
    "b_m = np.gradient(f, u) # partial of governing equation wrt highest order derivative of u in f\n",
    "eps_bm = np.sqrt(1/len(b_m) * sum(i ** 2 for i in b_m))\n",
    "\n",
    "epsil = eps_r1 / (((2 * np.pi * dominant_freq) ** 1) * eps_bm)\n",
    "print(\"Magnitude Prefactor\", epsil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 0 | Loss: 9.7806e-01 | Loss_d: 2.0059e-13 | Loss_e: 1.5445e-10\n",
      "Step: 100 | Loss: 5.2693e-01 | Loss_d: 6.4603e-17 | Loss_e: 8.8609e-11\n",
      "Step: 200 | Loss: 2.8193e-02 | Loss_d: 2.2904e-16 | Loss_e: 4.7297e-12\n",
      "Step: 300 | Loss: 1.8059e-02 | Loss_d: 8.0199e-17 | Loss_e: 3.0329e-12\n",
      "Step: 400 | Loss: 1.3545e-02 | Loss_d: 6.1802e-17 | Loss_e: 2.2747e-12\n",
      "Step: 500 | Loss: 1.0463e-02 | Loss_d: 5.4634e-17 | Loss_e: 1.7569e-12\n",
      "Step: 600 | Loss: 7.4382e-03 | Loss_d: 3.8258e-17 | Loss_e: 1.2489e-12\n",
      "Step: 700 | Loss: 4.8158e-03 | Loss_d: 1.1910e-17 | Loss_e: 8.0927e-13\n",
      "Step: 800 | Loss: 2.1208e-03 | Loss_d: 2.7475e-18 | Loss_e: 3.5651e-13\n",
      "Step: 900 | Loss: 1.0411e-03 | Loss_d: 1.1630e-18 | Loss_e: 1.7503e-13\n",
      "Step: 1000 | Loss: 8.0125e-04 | Loss_d: 6.7316e-19 | Loss_e: 1.3471e-13\n",
      "Step: 1100 | Loss: 8.5743e-04 | Loss_d: 4.7445e-19 | Loss_e: 1.4417e-13\n",
      "Step: 1200 | Loss: 6.8514e-04 | Loss_d: 1.2353e-19 | Loss_e: 1.1521e-13\n",
      "Step: 1300 | Loss: 5.9758e-04 | Loss_d: 3.2227e-21 | Loss_e: 1.0049e-13\n",
      "Step: 1400 | Loss: 6.6595e-04 | Loss_d: 8.6455e-20 | Loss_e: 1.1199e-13\n",
      "Step: 1500 | Loss: 5.4345e-04 | Loss_d: 4.1752e-20 | Loss_e: 9.1388e-14\n",
      "Step: 1600 | Loss: 5.3931e-04 | Loss_d: 7.3617e-19 | Loss_e: 9.0658e-14\n",
      "Step: 1700 | Loss: 6.2879e-04 | Loss_d: 1.1680e-17 | Loss_e: 1.0516e-13\n",
      "Step: 1800 | Loss: 5.7632e-04 | Loss_d: 8.2008e-19 | Loss_e: 9.6877e-14\n",
      "Step: 1900 | Loss: 5.8808e-04 | Loss_d: 2.0933e-18 | Loss_e: 9.8791e-14\n",
      "Step: 2000 | Loss: 5.1583e-04 | Loss_d: 9.9228e-18 | Loss_e: 8.6251e-14\n",
      "Step: 2100 | Loss: 5.8584e-04 | Loss_d: 3.1548e-20 | Loss_e: 9.8518e-14\n",
      "Step: 2200 | Loss: 5.8962e-04 | Loss_d: 5.3943e-18 | Loss_e: 9.8886e-14\n",
      "Step: 2300 | Loss: 6.5939e-04 | Loss_d: 2.8487e-20 | Loss_e: 1.1089e-13\n",
      "Step: 2400 | Loss: 5.4870e-04 | Loss_d: 3.6320e-18 | Loss_e: 9.2092e-14\n",
      "Step: 2500 | Loss: 5.5797e-04 | Loss_d: 2.8015e-18 | Loss_e: 9.3692e-14\n",
      "Step: 2600 | Loss: 5.6789e-04 | Loss_d: 1.6382e-21 | Loss_e: 9.5501e-14\n",
      "Step: 2700 | Loss: 6.2420e-04 | Loss_d: 9.1503e-19 | Loss_e: 1.0492e-13\n",
      "Step: 2800 | Loss: 6.0280e-04 | Loss_d: 2.1001e-20 | Loss_e: 1.0137e-13\n",
      "Step: 2900 | Loss: 5.6771e-04 | Loss_d: 2.2331e-19 | Loss_e: 9.5459e-14\n",
      "Step: 3000 | Loss: 5.3655e-04 | Loss_d: 4.1397e-20 | Loss_e: 9.0228e-14\n",
      "Step: 3100 | Loss: 5.4499e-04 | Loss_d: 1.1161e-17 | Loss_e: 9.1092e-14\n",
      "Step: 3200 | Loss: 5.6213e-04 | Loss_d: 1.0471e-17 | Loss_e: 9.4008e-14\n",
      "Step: 3300 | Loss: 5.4204e-04 | Loss_d: 6.1650e-18 | Loss_e: 9.0845e-14\n",
      "Step: 3400 | Loss: 5.3788e-04 | Loss_d: 7.0099e-19 | Loss_e: 9.0419e-14\n",
      "Step: 3500 | Loss: 5.4723e-04 | Loss_d: 7.1739e-18 | Loss_e: 9.1667e-14\n",
      "Step: 3600 | Loss: 5.0510e-04 | Loss_d: 5.5883e-17 | Loss_e: 8.2147e-14\n",
      "Step: 3700 | Loss: 5.3773e-04 | Loss_d: 2.9907e-18 | Loss_e: 9.0280e-14\n",
      "Step: 3800 | Loss: 6.3357e-04 | Loss_d: 9.9416e-17 | Loss_e: 1.0158e-13\n",
      "Step: 3900 | Loss: 5.0668e-04 | Loss_d: 6.7152e-20 | Loss_e: 8.5204e-14\n",
      "Step: 4000 | Loss: 5.8487e-04 | Loss_d: 6.9398e-17 | Loss_e: 9.4886e-14\n",
      "Step: 4100 | Loss: 5.4323e-04 | Loss_d: 7.5151e-19 | Loss_e: 9.1316e-14\n",
      "Step: 4200 | Loss: 5.8259e-04 | Loss_d: 2.3821e-17 | Loss_e: 9.6781e-14\n",
      "Step: 4300 | Loss: 5.8443e-04 | Loss_d: 2.4606e-18 | Loss_e: 9.8160e-14\n",
      "Step: 4400 | Loss: 5.0529e-04 | Loss_d: 5.2290e-18 | Loss_e: 8.4712e-14\n",
      "Step: 4500 | Loss: 5.9719e-04 | Loss_d: 1.7434e-16 | Loss_e: 9.1711e-14\n",
      "Step: 4600 | Loss: 5.9716e-04 | Loss_d: 1.8321e-19 | Loss_e: 1.0041e-13\n",
      "Step: 4700 | Loss: 5.0830e-04 | Loss_d: 4.5631e-18 | Loss_e: 8.5252e-14\n",
      "Step: 4800 | Loss: 5.4590e-04 | Loss_d: 1.0191e-18 | Loss_e: 9.1752e-14\n",
      "Step: 4900 | Loss: 4.9828e-04 | Loss_d: 2.0486e-18 | Loss_e: 8.3692e-14\n",
      "learning rate for Adam: 1.0000e-03 | mean: 4.200e-02 | std: 3.748e-05\n",
      "Step: 5000 | Loss: 4.9367e-04 | Loss_d: 7.4936e-20 | Loss_e: 8.3016e-14\n",
      "Step: 5100 | Loss: 5.6058e-04 | Loss_d: 3.0170e-18 | Loss_e: 9.4121e-14\n",
      "Step: 5200 | Loss: 4.9782e-04 | Loss_d: 1.2870e-19 | Loss_e: 8.3711e-14\n",
      "Step: 5300 | Loss: 5.3906e-04 | Loss_d: 2.9375e-17 | Loss_e: 8.9184e-14\n",
      "Step: 5400 | Loss: 5.5133e-04 | Loss_d: 2.1344e-18 | Loss_e: 9.2609e-14\n",
      "Step: 5500 | Loss: 5.3782e-04 | Loss_d: 1.3902e-19 | Loss_e: 9.0436e-14\n",
      "Step: 5600 | Loss: 5.3073e-04 | Loss_d: 5.8292e-18 | Loss_e: 8.8960e-14\n",
      "Step: 5700 | Loss: 5.0388e-04 | Loss_d: 1.0736e-17 | Loss_e: 8.4199e-14\n",
      "Step: 5800 | Loss: 5.5073e-04 | Loss_d: 6.8495e-17 | Loss_e: 8.9191e-14\n",
      "Step: 5900 | Loss: 5.9028e-04 | Loss_d: 3.7684e-18 | Loss_e: 9.9077e-14\n",
      "Step: 6000 | Loss: 5.6392e-04 | Loss_d: 9.4146e-20 | Loss_e: 9.4829e-14\n",
      "Step: 6100 | Loss: 6.0234e-04 | Loss_d: 2.5656e-17 | Loss_e: 1.0001e-13\n",
      "Step: 6200 | Loss: 5.1758e-04 | Loss_d: 3.0848e-18 | Loss_e: 8.6886e-14\n",
      "Step: 6300 | Loss: 5.4507e-04 | Loss_d: 1.0561e-19 | Loss_e: 9.1658e-14\n",
      "Step: 6400 | Loss: 5.4294e-04 | Loss_d: 2.4914e-17 | Loss_e: 9.0059e-14\n",
      "Step: 6500 | Loss: 5.1886e-04 | Loss_d: 3.9253e-18 | Loss_e: 8.7060e-14\n",
      "Step: 6600 | Loss: 5.9967e-04 | Loss_d: 1.6397e-17 | Loss_e: 1.0003e-13\n",
      "Step: 6700 | Loss: 5.9506e-04 | Loss_d: 5.4929e-18 | Loss_e: 9.9795e-14\n",
      "Step: 6800 | Loss: 5.4541e-04 | Loss_d: 9.4779e-19 | Loss_e: 9.1674e-14\n",
      "Step: 6900 | Loss: 5.8014e-04 | Loss_d: 1.5313e-16 | Loss_e: 8.9903e-14\n",
      "Step: 7000 | Loss: 5.3472e-04 | Loss_d: 1.3332e-17 | Loss_e: 8.9257e-14\n",
      "Step: 7100 | Loss: 5.5936e-04 | Loss_d: 3.4926e-17 | Loss_e: 9.2320e-14\n",
      "Step: 7200 | Loss: 5.5127e-04 | Loss_d: 4.6730e-19 | Loss_e: 9.2682e-14\n",
      "Step: 7300 | Loss: 5.1865e-04 | Loss_d: 4.4878e-18 | Loss_e: 8.6996e-14\n",
      "Step: 7400 | Loss: 5.9704e-04 | Loss_d: 1.2600e-21 | Loss_e: 1.0040e-13\n",
      "Step: 7500 | Loss: 5.5157e-04 | Loss_d: 3.0293e-17 | Loss_e: 9.1241e-14\n",
      "Step: 7600 | Loss: 5.3208e-04 | Loss_d: 8.0339e-18 | Loss_e: 8.9077e-14\n",
      "Step: 7700 | Loss: 5.9994e-04 | Loss_d: 1.7249e-24 | Loss_e: 1.0089e-13\n",
      "Step: 7800 | Loss: 5.6117e-04 | Loss_d: 3.7474e-17 | Loss_e: 9.2496e-14\n",
      "Step: 7900 | Loss: 5.2932e-04 | Loss_d: 2.9814e-18 | Loss_e: 8.8865e-14\n",
      "Step: 8000 | Loss: 5.7473e-04 | Loss_d: 4.8476e-18 | Loss_e: 9.6408e-14\n",
      "Step: 8100 | Loss: 6.5907e-04 | Loss_d: 2.6827e-16 | Loss_e: 9.7421e-14\n",
      "Step: 8200 | Loss: 5.3963e-04 | Loss_d: 8.3325e-20 | Loss_e: 9.0745e-14\n",
      "Step: 8300 | Loss: 5.8728e-04 | Loss_d: 1.0181e-17 | Loss_e: 9.8253e-14\n",
      "Step: 8400 | Loss: 5.0589e-04 | Loss_d: 4.0773e-18 | Loss_e: 8.4870e-14\n",
      "Step: 8500 | Loss: 6.2211e-04 | Loss_d: 2.8830e-19 | Loss_e: 1.0460e-13\n",
      "Step: 8600 | Loss: 5.5272e-04 | Loss_d: 2.5628e-20 | Loss_e: 9.2948e-14\n",
      "Step: 8700 | Loss: 5.8712e-04 | Loss_d: 3.7964e-19 | Loss_e: 9.8715e-14\n",
      "Step: 8800 | Loss: 5.1709e-04 | Loss_d: 5.0013e-19 | Loss_e: 8.6933e-14\n",
      "Step: 8900 | Loss: 5.5939e-04 | Loss_d: 6.2734e-19 | Loss_e: 9.4041e-14\n",
      "Step: 9000 | Loss: 5.6372e-04 | Loss_d: 7.6775e-17 | Loss_e: 9.0960e-14\n",
      "Step: 9100 | Loss: 5.5027e-04 | Loss_d: 1.3482e-19 | Loss_e: 9.2531e-14\n",
      "Step: 9200 | Loss: 4.9703e-04 | Loss_d: 3.7587e-18 | Loss_e: 8.3397e-14\n",
      "Step: 9300 | Loss: 5.8239e-04 | Loss_d: 1.9863e-18 | Loss_e: 9.7839e-14\n",
      "Step: 9400 | Loss: 5.2738e-04 | Loss_d: 1.6984e-18 | Loss_e: 8.8603e-14\n",
      "Step: 9500 | Loss: 4.7804e-04 | Loss_d: 1.5036e-17 | Loss_e: 7.9639e-14\n",
      "Step: 9600 | Loss: 5.1988e-04 | Loss_d: 8.0635e-19 | Loss_e: 8.7387e-14\n",
      "Step: 9700 | Loss: 5.6918e-04 | Loss_d: 5.0263e-20 | Loss_e: 9.5715e-14\n",
      "Step: 9800 | Loss: 4.6285e-04 | Loss_d: 1.0638e-17 | Loss_e: 7.7305e-14\n",
      "Step: 9900 | Loss: 5.6774e-04 | Loss_d: 5.0933e-20 | Loss_e: 9.5474e-14\n",
      "learning rate for Adam: 5.0000e-04 | mean: 1.009e-06 | std: 4.416e-05\n",
      "Step: 10000 | Loss: 5.0549e-04 | Loss_d: 2.2323e-17 | Loss_e: 8.3890e-14\n",
      "Step: 10100 | Loss: 5.0113e-04 | Loss_d: 1.5331e-23 | Loss_e: 8.4273e-14\n",
      "Step: 10200 | Loss: 5.1350e-04 | Loss_d: 6.5476e-22 | Loss_e: 8.6354e-14\n",
      "Step: 10300 | Loss: 4.8091e-04 | Loss_d: 3.1083e-21 | Loss_e: 8.0873e-14\n",
      "Step: 10400 | Loss: 5.2922e-04 | Loss_d: 5.7179e-25 | Loss_e: 8.8998e-14\n",
      "Step: 10500 | Loss: 5.4932e-04 | Loss_d: 1.5878e-20 | Loss_e: 9.2377e-14\n",
      "Step: 10600 | Loss: 4.9898e-04 | Loss_d: 3.5477e-20 | Loss_e: 8.3911e-14\n",
      "Step: 10700 | Loss: 5.1750e-04 | Loss_d: 2.7344e-20 | Loss_e: 8.7026e-14\n",
      "Step: 10800 | Loss: 4.7614e-04 | Loss_d: 1.6520e-21 | Loss_e: 8.0072e-14\n",
      "Step: 10900 | Loss: 5.1998e-04 | Loss_d: 1.1718e-21 | Loss_e: 8.7444e-14\n",
      "Step: 11000 | Loss: 5.8647e-04 | Loss_d: 6.0849e-21 | Loss_e: 9.8625e-14\n",
      "Step: 11100 | Loss: 5.3274e-04 | Loss_d: 1.9007e-20 | Loss_e: 8.9589e-14\n",
      "Step: 11200 | Loss: 4.7563e-04 | Loss_d: 4.4043e-20 | Loss_e: 7.9983e-14\n",
      "Step: 11300 | Loss: 4.9375e-04 | Loss_d: 5.4620e-21 | Loss_e: 8.3033e-14\n",
      "Step: 11400 | Loss: 5.2070e-04 | Loss_d: 4.0939e-19 | Loss_e: 8.7545e-14\n",
      "Step: 11500 | Loss: 4.7632e-04 | Loss_d: 2.7845e-17 | Loss_e: 7.8710e-14\n",
      "Step: 11600 | Loss: 5.3261e-04 | Loss_d: 1.5238e-18 | Loss_e: 8.9492e-14\n",
      "Step: 11700 | Loss: 5.5784e-04 | Loss_d: 3.1567e-17 | Loss_e: 9.2232e-14\n",
      "Step: 11800 | Loss: 5.4563e-04 | Loss_d: 9.1619e-18 | Loss_e: 9.1300e-14\n",
      "Step: 11900 | Loss: 5.8086e-04 | Loss_d: 4.9610e-18 | Loss_e: 9.7434e-14\n",
      "Step: 12000 | Loss: 4.7248e-04 | Loss_d: 7.6017e-18 | Loss_e: 7.9076e-14\n",
      "Step: 12100 | Loss: 5.1580e-04 | Loss_d: 4.2147e-19 | Loss_e: 8.6720e-14\n",
      "Step: 12200 | Loss: 5.5278e-04 | Loss_d: 1.8844e-18 | Loss_e: 9.2866e-14\n",
      "Step: 12300 | Loss: 5.8035e-04 | Loss_d: 3.2049e-22 | Loss_e: 9.7596e-14\n",
      "Step: 12400 | Loss: 5.3178e-04 | Loss_d: 7.7549e-18 | Loss_e: 8.9041e-14\n",
      "Step: 12500 | Loss: 5.0424e-04 | Loss_d: 2.8693e-18 | Loss_e: 8.4653e-14\n",
      "Step: 12600 | Loss: 5.3360e-04 | Loss_d: 6.1031e-18 | Loss_e: 8.9429e-14\n",
      "Step: 12700 | Loss: 5.5600e-04 | Loss_d: 3.5788e-18 | Loss_e: 9.3322e-14\n",
      "Step: 12800 | Loss: 5.0148e-04 | Loss_d: 1.5925e-19 | Loss_e: 8.4325e-14\n",
      "Step: 12900 | Loss: 4.9853e-04 | Loss_d: 1.6446e-18 | Loss_e: 8.3755e-14\n",
      "Step: 13000 | Loss: 5.4348e-04 | Loss_d: 6.6264e-19 | Loss_e: 9.1363e-14\n",
      "Step: 13100 | Loss: 5.0181e-04 | Loss_d: 7.8224e-23 | Loss_e: 8.4388e-14\n",
      "Step: 13200 | Loss: 5.1387e-04 | Loss_d: 8.4434e-18 | Loss_e: 8.5994e-14\n",
      "Step: 13300 | Loss: 5.0591e-04 | Loss_d: 1.4537e-18 | Loss_e: 8.5004e-14\n",
      "Step: 13400 | Loss: 5.9799e-04 | Loss_d: 5.7873e-19 | Loss_e: 1.0053e-13\n",
      "Step: 13500 | Loss: 6.2829e-04 | Loss_d: 1.6499e-18 | Loss_e: 1.0558e-13\n",
      "Step: 13600 | Loss: 5.9706e-04 | Loss_d: 2.9507e-18 | Loss_e: 1.0026e-13\n",
      "Step: 13700 | Loss: 6.1456e-04 | Loss_d: 2.5777e-20 | Loss_e: 1.0335e-13\n",
      "Step: 13800 | Loss: 5.7333e-04 | Loss_d: 7.6279e-20 | Loss_e: 9.6412e-14\n",
      "Step: 13900 | Loss: 5.3492e-04 | Loss_d: 2.3634e-18 | Loss_e: 8.9838e-14\n",
      "Step: 14000 | Loss: 4.8120e-04 | Loss_d: 9.3225e-19 | Loss_e: 8.0875e-14\n",
      "Step: 14100 | Loss: 5.2354e-04 | Loss_d: 3.2406e-18 | Loss_e: 8.7881e-14\n",
      "Step: 14200 | Loss: 5.4686e-04 | Loss_d: 7.2368e-19 | Loss_e: 9.1929e-14\n",
      "Step: 14300 | Loss: 5.3412e-04 | Loss_d: 4.0597e-18 | Loss_e: 8.9619e-14\n",
      "Step: 14400 | Loss: 4.6624e-04 | Loss_d: 1.2116e-19 | Loss_e: 7.8401e-14\n",
      "Step: 14500 | Loss: 5.1476e-04 | Loss_d: 1.1032e-17 | Loss_e: 8.6014e-14\n",
      "Step: 14600 | Loss: 6.2537e-04 | Loss_d: 5.3543e-18 | Loss_e: 1.0490e-13\n",
      "Step: 14700 | Loss: 5.6133e-04 | Loss_d: 1.3284e-19 | Loss_e: 9.4391e-14\n",
      "Step: 14800 | Loss: 5.6152e-04 | Loss_d: 8.8559e-19 | Loss_e: 9.4385e-14\n",
      "Step: 14900 | Loss: 5.0210e-04 | Loss_d: 1.8041e-18 | Loss_e: 8.4346e-14\n",
      "learning rate for Adam: 5.0000e-04 | mean: 2.073e-05 | std: 4.632e-05\n",
      "Step: 15000 | Loss: 5.1667e-04 | Loss_d: 1.6747e-18 | Loss_e: 8.6804e-14\n",
      "Step: 15100 | Loss: 5.8293e-04 | Loss_d: 2.9798e-21 | Loss_e: 9.8030e-14\n",
      "Step: 15200 | Loss: 6.3307e-04 | Loss_d: 6.1042e-17 | Loss_e: 1.0341e-13\n",
      "Step: 15300 | Loss: 5.3243e-04 | Loss_d: 1.0546e-20 | Loss_e: 8.9537e-14\n",
      "Step: 15400 | Loss: 4.9956e-04 | Loss_d: 2.1446e-19 | Loss_e: 8.4000e-14\n",
      "Step: 15500 | Loss: 5.8942e-04 | Loss_d: 3.3823e-18 | Loss_e: 9.8953e-14\n",
      "Step: 15600 | Loss: 4.9251e-04 | Loss_d: 1.0924e-19 | Loss_e: 8.2819e-14\n",
      "Step: 15700 | Loss: 5.3094e-04 | Loss_d: 2.8560e-18 | Loss_e: 8.9144e-14\n",
      "Step: 15800 | Loss: 5.3326e-04 | Loss_d: 2.6369e-18 | Loss_e: 8.9546e-14\n",
      "Step: 15900 | Loss: 4.8708e-04 | Loss_d: 4.1858e-20 | Loss_e: 8.1910e-14\n",
      "Step: 16000 | Loss: 5.2431e-04 | Loss_d: 3.4384e-18 | Loss_e: 8.7999e-14\n",
      "Step: 16100 | Loss: 6.2049e-04 | Loss_d: 1.4555e-17 | Loss_e: 1.0362e-13\n",
      "Step: 16200 | Loss: 4.9905e-04 | Loss_d: 5.4223e-20 | Loss_e: 8.3921e-14\n",
      "Step: 16300 | Loss: 5.9441e-04 | Loss_d: 7.8907e-18 | Loss_e: 9.9567e-14\n",
      "Step: 16400 | Loss: 6.2868e-04 | Loss_d: 2.0905e-18 | Loss_e: 1.0562e-13\n",
      "Step: 16500 | Loss: 4.9944e-04 | Loss_d: 1.3595e-18 | Loss_e: 8.3921e-14\n",
      "Step: 16600 | Loss: 4.9435e-04 | Loss_d: 6.9191e-18 | Loss_e: 8.2787e-14\n",
      "Step: 16700 | Loss: 5.3740e-04 | Loss_d: 1.8695e-18 | Loss_e: 9.0280e-14\n",
      "Step: 16800 | Loss: 5.6374e-04 | Loss_d: 2.3830e-19 | Loss_e: 9.4790e-14\n",
      "Step: 16900 | Loss: 5.5976e-04 | Loss_d: 6.9771e-18 | Loss_e: 9.3785e-14\n",
      "Step: 17000 | Loss: 5.3409e-04 | Loss_d: 1.1038e-20 | Loss_e: 8.9815e-14\n",
      "Step: 17100 | Loss: 5.3702e-04 | Loss_d: 2.9681e-20 | Loss_e: 9.0308e-14\n",
      "Step: 17200 | Loss: 5.1684e-04 | Loss_d: 7.7389e-20 | Loss_e: 8.6912e-14\n",
      "Step: 17300 | Loss: 5.3235e-04 | Loss_d: 5.2732e-17 | Loss_e: 8.6888e-14\n",
      "Step: 17400 | Loss: 5.7542e-04 | Loss_d: 4.4323e-19 | Loss_e: 9.6745e-14\n",
      "Step: 17500 | Loss: 4.9120e-04 | Loss_d: 3.7147e-18 | Loss_e: 8.2418e-14\n",
      "Step: 17600 | Loss: 5.4392e-04 | Loss_d: 3.3359e-18 | Loss_e: 9.1303e-14\n",
      "Step: 17700 | Loss: 4.9488e-04 | Loss_d: 2.6792e-19 | Loss_e: 8.3209e-14\n",
      "Step: 17800 | Loss: 5.7058e-04 | Loss_d: 8.7722e-18 | Loss_e: 9.5515e-14\n",
      "Step: 17900 | Loss: 5.8774e-04 | Loss_d: 9.5642e-20 | Loss_e: 9.8835e-14\n",
      "Step: 18000 | Loss: 5.9212e-04 | Loss_d: 8.8562e-19 | Loss_e: 9.9531e-14\n",
      "Step: 18100 | Loss: 5.2738e-04 | Loss_d: 7.9268e-19 | Loss_e: 8.8649e-14\n",
      "Step: 18200 | Loss: 5.3125e-04 | Loss_d: 5.5789e-18 | Loss_e: 8.9061e-14\n",
      "Step: 18300 | Loss: 5.1469e-04 | Loss_d: 4.1855e-18 | Loss_e: 8.6345e-14\n",
      "Step: 18400 | Loss: 5.0249e-04 | Loss_d: 4.4636e-17 | Loss_e: 8.2270e-14\n",
      "Step: 18500 | Loss: 5.6769e-04 | Loss_d: 1.6186e-19 | Loss_e: 9.5460e-14\n",
      "Step: 18600 | Loss: 5.8580e-04 | Loss_d: 2.4249e-18 | Loss_e: 9.8391e-14\n",
      "Step: 18700 | Loss: 4.7962e-04 | Loss_d: 9.2814e-19 | Loss_e: 8.0610e-14\n",
      "Step: 18800 | Loss: 5.1718e-04 | Loss_d: 1.6828e-18 | Loss_e: 8.6888e-14\n",
      "Step: 18900 | Loss: 5.5467e-04 | Loss_d: 5.1291e-18 | Loss_e: 9.3021e-14\n",
      "Step: 19000 | Loss: 5.3878e-04 | Loss_d: 8.4423e-19 | Loss_e: 9.0563e-14\n",
      "Step: 19100 | Loss: 5.4968e-04 | Loss_d: 6.2059e-19 | Loss_e: 9.2407e-14\n",
      "Step: 19200 | Loss: 5.9515e-04 | Loss_d: 1.2676e-17 | Loss_e: 9.9451e-14\n",
      "Step: 19300 | Loss: 5.0797e-04 | Loss_d: 3.3327e-18 | Loss_e: 8.5258e-14\n",
      "Step: 19400 | Loss: 4.9412e-04 | Loss_d: 1.0686e-22 | Loss_e: 8.3096e-14\n",
      "Step: 19500 | Loss: 5.0514e-04 | Loss_d: 2.4025e-17 | Loss_e: 8.3747e-14\n",
      "Step: 19600 | Loss: 4.8764e-04 | Loss_d: 4.0907e-18 | Loss_e: 8.1801e-14\n",
      "Step: 19700 | Loss: 5.1365e-04 | Loss_d: 4.9813e-18 | Loss_e: 8.6131e-14\n",
      "Step: 19800 | Loss: 4.4995e-04 | Loss_d: 2.5278e-19 | Loss_e: 7.5655e-14\n",
      "Step: 19900 | Loss: 5.2183e-04 | Loss_d: 6.7907e-19 | Loss_e: 8.7721e-14\n",
      "learning rate for Adam: 2.5000e-04 | mean: 1.243e-05 | std: 3.911e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss: 5.3455e-04 | Loss_d: 4.3262e-19 | Loss_e: 8.9872e-14\n",
      " Loss: 2.0362e-02 | Loss_d: 6.1521e-16 | Loss_e: 3.3935e-12\n",
      " Loss: 5.1244e-04 | Loss_d: 8.4044e-20 | Loss_e: 8.6172e-14\n",
      " Loss: 4.9331e-04 | Loss_d: 7.4078e-21 | Loss_e: 8.2958e-14\n",
      " Loss: 5.6447e-04 | Loss_d: 2.5134e-18 | Loss_e: 9.4799e-14\n",
      " Loss: 4.8252e-04 | Loss_d: 1.9928e-19 | Loss_e: 8.1134e-14\n",
      " Loss: 4.6063e-04 | Loss_d: 6.3435e-19 | Loss_e: 7.7431e-14\n",
      " Loss: 4.4746e-04 | Loss_d: 4.7973e-18 | Loss_e: 7.5008e-14\n",
      " Loss: 4.4709e-04 | Loss_d: 4.5854e-18 | Loss_e: 7.4957e-14\n",
      " Loss: 4.5142e-04 | Loss_d: 2.2761e-17 | Loss_e: 7.4776e-14\n",
      " Loss: 4.4222e-04 | Loss_d: 2.0325e-18 | Loss_e: 7.4265e-14\n",
      " Loss: 4.4135e-04 | Loss_d: 3.9100e-22 | Loss_e: 7.4220e-14\n",
      " Loss: 4.6235e-04 | Loss_d: 3.1411e-17 | Loss_e: 7.6182e-14\n",
      " Loss: 4.4194e-04 | Loss_d: 2.1033e-19 | Loss_e: 7.4310e-14\n",
      " Loss: 4.3575e-04 | Loss_d: 4.5904e-19 | Loss_e: 7.3256e-14\n",
      " Loss: 4.3681e-04 | Loss_d: 2.4078e-18 | Loss_e: 7.3338e-14\n",
      " Loss: 4.3548e-04 | Loss_d: 1.7103e-18 | Loss_e: 7.3149e-14\n",
      " Loss: 4.3711e-04 | Loss_d: 8.7227e-19 | Loss_e: 7.3464e-14\n",
      " Loss: 4.3623e-04 | Loss_d: 1.2566e-18 | Loss_e: 7.3298e-14\n",
      " Loss: 4.3441e-04 | Loss_d: 1.4743e-18 | Loss_e: 7.2981e-14\n",
      " Loss: 4.3668e-04 | Loss_d: 1.3631e-18 | Loss_e: 7.3367e-14\n",
      " Loss: 4.3593e-04 | Loss_d: 1.4191e-18 | Loss_e: 7.3239e-14\n",
      " Loss: 4.3420e-04 | Loss_d: 1.4462e-18 | Loss_e: 7.2946e-14\n",
      " Loss: 4.3416e-04 | Loss_d: 1.4332e-18 | Loss_e: 7.2940e-14\n",
      " Loss: 4.3477e-04 | Loss_d: 1.4257e-18 | Loss_e: 7.3042e-14\n",
      " Loss: 4.3586e-04 | Loss_d: 1.4224e-18 | Loss_e: 7.3226e-14\n",
      " Loss: 4.3476e-04 | Loss_d: 1.4235e-18 | Loss_e: 7.3042e-14\n",
      " Loss: 4.3586e-04 | Loss_d: 1.4234e-18 | Loss_e: 7.3226e-14\n",
      " Loss: 4.3476e-04 | Loss_d: 1.4233e-18 | Loss_e: 7.3042e-14\n",
      " Loss: 4.3586e-04 | Loss_d: 1.4232e-18 | Loss_e: 7.3227e-14\n",
      " Loss: 4.3476e-04 | Loss_d: 1.4236e-18 | Loss_e: 7.3042e-14\n",
      " Loss: 4.3476e-04 | Loss_d: 1.4231e-18 | Loss_e: 7.3042e-14\n",
      " Loss: 4.3476e-04 | Loss_d: 1.4232e-18 | Loss_e: 7.3042e-14\n",
      " Loss: 4.3476e-04 | Loss_d: 1.4232e-18 | Loss_e: 7.3042e-14\n",
      " Loss: 4.3586e-04 | Loss_d: 1.4232e-18 | Loss_e: 7.3227e-14\n",
      " Loss: 4.3586e-04 | Loss_d: 1.4232e-18 | Loss_e: 7.3227e-14\n",
      " Loss: 4.3586e-04 | Loss_d: 1.4232e-18 | Loss_e: 7.3227e-14\n",
      " Loss: 4.3586e-04 | Loss_d: 1.4232e-18 | Loss_e: 7.3227e-14\n",
      " Loss: 4.3476e-04 | Loss_d: 1.4232e-18 | Loss_e: 7.3042e-14\n",
      " Loss: 4.3476e-04 | Loss_d: 1.4232e-18 | Loss_e: 7.3042e-14\n",
      " Loss: 4.3586e-04 | Loss_d: 1.4232e-18 | Loss_e: 7.3227e-14\n",
      " Loss: 4.3586e-04 | Loss_d: 1.4232e-18 | Loss_e: 7.3227e-14\n",
      " Loss: 4.3476e-04 | Loss_d: 1.4232e-18 | Loss_e: 7.3042e-14\n",
      " Loss: 4.3586e-04 | Loss_d: 1.4232e-18 | Loss_e: 7.3227e-14\n",
      " Loss: 4.5422e-04 | Loss_d: 2.4296e-18 | Loss_e: 7.6264e-14\n",
      " Loss: 4.3349e-04 | Loss_d: 1.3232e-19 | Loss_e: 7.2892e-14\n",
      " Loss: 4.3407e-04 | Loss_d: 1.2937e-19 | Loss_e: 7.2991e-14\n",
      " Loss: 4.3104e-04 | Loss_d: 1.3070e-19 | Loss_e: 7.2480e-14\n",
      " Loss: 4.3370e-04 | Loss_d: 1.3000e-19 | Loss_e: 7.2927e-14\n",
      " Loss: 4.3071e-04 | Loss_d: 1.3003e-19 | Loss_e: 7.2426e-14\n",
      " Loss: 4.3275e-04 | Loss_d: 1.3042e-19 | Loss_e: 7.2767e-14\n",
      " Loss: 4.3351e-04 | Loss_d: 1.3006e-19 | Loss_e: 7.2895e-14\n",
      " Loss: 4.3304e-04 | Loss_d: 1.3018e-19 | Loss_e: 7.2818e-14\n",
      " Loss: 4.3301e-04 | Loss_d: 1.3028e-19 | Loss_e: 7.2812e-14\n",
      " Loss: 4.3350e-04 | Loss_d: 1.3026e-19 | Loss_e: 7.2895e-14\n",
      " Loss: 4.3350e-04 | Loss_d: 1.3004e-19 | Loss_e: 7.2895e-14\n",
      " Loss: 4.3303e-04 | Loss_d: 1.2992e-19 | Loss_e: 7.2816e-14\n",
      " Loss: 4.3350e-04 | Loss_d: 1.3024e-19 | Loss_e: 7.2895e-14\n",
      " Loss: 4.3348e-04 | Loss_d: 1.3015e-19 | Loss_e: 7.2891e-14\n",
      " Loss: 4.3348e-04 | Loss_d: 1.3025e-19 | Loss_e: 7.2891e-14\n",
      " Loss: 4.3350e-04 | Loss_d: 1.3028e-19 | Loss_e: 7.2895e-14\n",
      " Loss: 4.3348e-04 | Loss_d: 1.3044e-19 | Loss_e: 7.2891e-14\n",
      " Loss: 4.3350e-04 | Loss_d: 1.3029e-19 | Loss_e: 7.2895e-14\n",
      " Loss: 4.3348e-04 | Loss_d: 1.3027e-19 | Loss_e: 7.2891e-14\n",
      " Loss: 4.3348e-04 | Loss_d: 1.3027e-19 | Loss_e: 7.2891e-14\n",
      " Loss: 4.3348e-04 | Loss_d: 1.3027e-19 | Loss_e: 7.2891e-14\n",
      " Loss: 4.3350e-04 | Loss_d: 1.3029e-19 | Loss_e: 7.2895e-14\n",
      " Loss: 4.3348e-04 | Loss_d: 1.3027e-19 | Loss_e: 7.2891e-14\n",
      " Loss: 4.3350e-04 | Loss_d: 1.3027e-19 | Loss_e: 7.2895e-14\n",
      " Loss: 4.3348e-04 | Loss_d: 1.3027e-19 | Loss_e: 7.2891e-14\n",
      " Loss: 4.3348e-04 | Loss_d: 1.3027e-19 | Loss_e: 7.2891e-14\n",
      " Loss: 4.3350e-04 | Loss_d: 1.3027e-19 | Loss_e: 7.2895e-14\n",
      " Loss: 4.2768e-04 | Loss_d: 2.5090e-19 | Loss_e: 7.1910e-14\n",
      " Loss: 4.2787e-04 | Loss_d: 1.0940e-18 | Loss_e: 7.1899e-14\n",
      " Loss: 4.2574e-04 | Loss_d: 3.6222e-19 | Loss_e: 7.1577e-14\n",
      " Loss: 4.2186e-04 | Loss_d: 2.7973e-20 | Loss_e: 7.0941e-14\n",
      " Loss: 4.1004e-04 | Loss_d: 2.4595e-18 | Loss_e: 6.8833e-14\n",
      " Loss: 4.1522e-04 | Loss_d: 5.9973e-19 | Loss_e: 6.9797e-14\n",
      " Loss: 4.1445e-04 | Loss_d: 3.7958e-19 | Loss_e: 6.9678e-14\n",
      " Loss: 4.1682e-04 | Loss_d: 2.5009e-22 | Loss_e: 7.0095e-14\n",
      " Loss: 4.1515e-04 | Loss_d: 2.6676e-19 | Loss_e: 6.9801e-14\n",
      " Loss: 4.0906e-04 | Loss_d: 6.3134e-20 | Loss_e: 6.8788e-14\n",
      " Loss: 4.1402e-04 | Loss_d: 1.1024e-17 | Loss_e: 6.9074e-14\n",
      " Loss: 4.0474e-04 | Loss_d: 7.9818e-19 | Loss_e: 6.8024e-14\n",
      " Loss: 4.0154e-04 | Loss_d: 5.4330e-19 | Loss_e: 6.7499e-14\n",
      " Loss: 4.0098e-04 | Loss_d: 1.2545e-20 | Loss_e: 6.7432e-14\n",
      " Loss: 4.5352e-04 | Loss_d: 9.0888e-18 | Loss_e: 7.5814e-14\n",
      " Loss: 4.0039e-04 | Loss_d: 1.1567e-20 | Loss_e: 6.7333e-14\n",
      " Loss: 3.9839e-04 | Loss_d: 1.8716e-20 | Loss_e: 6.6995e-14\n",
      " Loss: 3.9318e-04 | Loss_d: 6.1405e-20 | Loss_e: 6.6117e-14\n",
      " Loss: 3.9383e-04 | Loss_d: 2.6810e-20 | Loss_e: 6.6228e-14\n",
      " Loss: 3.9160e-04 | Loss_d: 1.2418e-18 | Loss_e: 6.5793e-14\n",
      " Loss: 3.8896e-04 | Loss_d: 3.4121e-19 | Loss_e: 6.5394e-14\n",
      " Loss: 3.8717e-04 | Loss_d: 1.2766e-19 | Loss_e: 6.5103e-14\n",
      " Loss: 3.9542e-04 | Loss_d: 3.0540e-19 | Loss_e: 6.6482e-14\n",
      " Loss: 3.9082e-04 | Loss_d: 1.0589e-19 | Loss_e: 6.5718e-14\n",
      " Loss: 3.8870e-04 | Loss_d: 1.1718e-19 | Loss_e: 6.5361e-14\n",
      " Loss: 3.9166e-04 | Loss_d: 8.8561e-21 | Loss_e: 6.5864e-14\n",
      " Loss: 3.8925e-04 | Loss_d: 1.3143e-21 | Loss_e: 6.5459e-14\n",
      " Loss: 3.9216e-04 | Loss_d: 2.3403e-20 | Loss_e: 6.5947e-14\n",
      " Loss: 3.8923e-04 | Loss_d: 6.1426e-20 | Loss_e: 6.5453e-14\n",
      " Loss: 3.8964e-04 | Loss_d: 8.6942e-20 | Loss_e: 6.5520e-14\n",
      " Loss: 3.8863e-04 | Loss_d: 1.0149e-19 | Loss_e: 6.5350e-14\n",
      " Loss: 3.9052e-04 | Loss_d: 9.3988e-20 | Loss_e: 6.5668e-14\n",
      " Loss: 3.8863e-04 | Loss_d: 9.7962e-20 | Loss_e: 6.5350e-14\n",
      " Loss: 3.9052e-04 | Loss_d: 9.6093e-20 | Loss_e: 6.5668e-14\n",
      " Loss: 3.8864e-04 | Loss_d: 9.6910e-20 | Loss_e: 6.5352e-14\n",
      " Loss: 3.8864e-04 | Loss_d: 9.6572e-20 | Loss_e: 6.5352e-14\n",
      " Loss: 3.8864e-04 | Loss_d: 9.6040e-20 | Loss_e: 6.5352e-14\n",
      " Loss: 3.9052e-04 | Loss_d: 9.6058e-20 | Loss_e: 6.5669e-14\n",
      " Loss: 3.8864e-04 | Loss_d: 9.6164e-20 | Loss_e: 6.5352e-14\n",
      " Loss: 3.8864e-04 | Loss_d: 9.6076e-20 | Loss_e: 6.5352e-14\n",
      " Loss: 3.8864e-04 | Loss_d: 9.6111e-20 | Loss_e: 6.5352e-14\n",
      " Loss: 3.9052e-04 | Loss_d: 9.6058e-20 | Loss_e: 6.5669e-14\n",
      " Loss: 3.8864e-04 | Loss_d: 9.6049e-20 | Loss_e: 6.5352e-14\n",
      " Loss: 3.9052e-04 | Loss_d: 9.6058e-20 | Loss_e: 6.5669e-14\n",
      " Loss: 3.9052e-04 | Loss_d: 9.6049e-20 | Loss_e: 6.5669e-14\n",
      " Loss: 3.9052e-04 | Loss_d: 9.6049e-20 | Loss_e: 6.5669e-14\n",
      " Loss: 3.8864e-04 | Loss_d: 9.6049e-20 | Loss_e: 6.5352e-14\n",
      " Loss: 3.8864e-04 | Loss_d: 9.6049e-20 | Loss_e: 6.5352e-14\n",
      " Loss: 3.9052e-04 | Loss_d: 9.6049e-20 | Loss_e: 6.5669e-14\n",
      " Loss: 3.8864e-04 | Loss_d: 9.6049e-20 | Loss_e: 6.5352e-14\n",
      " Loss: 3.8864e-04 | Loss_d: 9.6049e-20 | Loss_e: 6.5352e-14\n",
      " Loss: 3.8864e-04 | Loss_d: 9.6049e-20 | Loss_e: 6.5352e-14\n",
      " Loss: 3.9052e-04 | Loss_d: 9.6049e-20 | Loss_e: 6.5669e-14\n",
      " Loss: 3.9052e-04 | Loss_d: 9.6049e-20 | Loss_e: 6.5669e-14\n",
      " Loss: 3.8864e-04 | Loss_d: 9.6049e-20 | Loss_e: 6.5352e-14\n",
      " Total iterations: 127\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Preparing new collocation points and NN for stage 2\"\"\"\n",
    "epoch1 = 20000\n",
    "epoch2 = 40000\n",
    "lw = [0.02, 0.001]\n",
    "\n",
    "dataf2 = data_func_create(N_col*2, ds) # collocation points\n",
    "data2 = dataf2()\n",
    "\n",
    "trained_params2 = sol_init_MLP(keys[1], n_hl, n_unit)\n",
    "\n",
    "pred_u2 = mNN_pred_create(f_u1, lmt, scl2, epsil, act_s=1) # create the solution function\n",
    "\n",
    "NN_loss2 = loss_create(pred_u2, cond, lw, loss_ref=1)\n",
    "loss0 = NN_loss2(trained_params2, data2)[0] \n",
    "NN_loss2 = loss_create(pred_u2, cond, lw, loss_ref=loss0) # calculate the loss function\n",
    "\n",
    "\"\"\"Second stage of training\"\"\"\n",
    "trained_params2, loss1 = adam_optimizer(NN_loss2, trained_params2, dataf2, epoch1, lr=lr)\n",
    "data2 = dataf2()\n",
    "trained_params2, loss2 = lbfgs_optimizer(NN_loss2, trained_params2, data2, epoch2)\n",
    "\n",
    "# calculate the equation residue\n",
    "f_u2 = lambda z: pred_u2(trained_params2, z)\n",
    "fu2_x = lambda z: vectgrad(f_u2, z)[0]\n",
    "\n",
    "# calculate the solution\n",
    "u2_p = f_u2(x_star)\n",
    "u2_xx, u2_x = vectgrad(fu2_x, x_star)\n",
    "\n",
    "f2_p = gov_eqn(f_u2, x_star)\n",
    "df2, d2f2 = gov_d3_eqn(f_u2, x_star)\n",
    "\n",
    "# generate the last loss\n",
    "loss_all = loss_all + (loss1+loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Stage Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant Frequency: 0.0 Hz\n",
      "Kappa 2 0.0\n",
      "1.999999940396105\n",
      "Magnitude Prefactor inf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Finding new scale factor and magnitude prefactor for stage 3\"\"\"\n",
    "g = np.array(f2_p).flatten()\n",
    "N = len(g)\n",
    "sample_rate = N / total_time_range\n",
    "# Perform FFT\n",
    "T = 1.0 / sample_rate\n",
    "yf = fft(g)\n",
    "xf = fftfreq(N, T)[:N//2]\n",
    "dominant_freq_idx = np.argmax(np.abs(yf[:N//2]))\n",
    "dominant_freq = xf[dominant_freq_idx]\n",
    "magnitude = np.abs(yf[dominant_freq_idx]) / N\n",
    "\n",
    "print(f\"Dominant Frequency: {dominant_freq} Hz\")\n",
    "\n",
    "# Calculate kappa for Stage 3\n",
    "if dominant_freq < g.size / (6 * np.pi):\n",
    "    scl3 = 2 * np.pi * dominant_freq\n",
    "    print('Kappa 2', scl3)\n",
    "else:\n",
    "    print('Not enough data: Please Increase')\n",
    "\n",
    "\n",
    "# find magnitude prefactor\n",
    "eps_r2 = np.sqrt(1/len(f2_p) * sum(i ** 2 for i in f2_p.flatten())) # RMS of equation residue\n",
    "u, x = data2['z_col'].flatten(), data2['z_col'].flatten()\n",
    "f = func(u, x).flatten()\n",
    "b_m = np.gradient(f, u) # partial of governing equation wrt highest order derivative of u in f\n",
    "eps_bm = np.sqrt(1/len(b_m) * sum(i ** 2 for i in b_m))\n",
    "\n",
    "epsil = eps_r2 / (((2 * np.pi * dominant_freq) ** 1) * eps_bm)\n",
    "print(\"Magnitude Prefactor\", epsil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"preparing new collocation points\"\"\"\n",
    "# collocation points\n",
    "dataf3 = data_func_create(N_col*4, ds)\n",
    "data3 = dataf3()\n",
    "\n",
    "epoch1 = 60000\n",
    "lw = [0.001, 0.000002]\n",
    "\n",
    "trained_params3 = sol_init_MLP(keys[2], n_hl, n_unit)\n",
    "\n",
    "pred_u3 = mNN_pred_create(f_u2, lmt, scl3, epsil, act_s=1) # create the solution function\n",
    "\n",
    "NN_loss3 = loss_create(pred_u3, cond, lw, loss_ref=1)\n",
    "loss0 = NN_loss3(trained_params3, data3)[0]\n",
    "NN_loss3 = loss_create(pred_u3, cond, lw, loss_ref=loss0) # calculate the loss function\n",
    "\n",
    "\"\"\"Third stage of training\"\"\"\n",
    "trained_params3, loss1 = adam_optimizer(NN_loss3, trained_params3, dataf3, epoch1, lr=lr)\n",
    "\n",
    "# calculate the equation residue\n",
    "f_u3 = lambda z: pred_u3(trained_params3, z)\n",
    "fu3_x = lambda z: vectgrad(f_u3, z)[0]\n",
    "\n",
    "# calculate the solution\n",
    "u3_p = f_u3(x_star)\n",
    "u3_xx, u3_x = vectgrad(fu3_x, x_star)\n",
    "\n",
    "f3_p = gov_eqn(f_u3, x_star)\n",
    "df3, d2f3 = gov_d3_eqn(f_u3, x_star)\n",
    "\n",
    "# generate the last loss\n",
    "loss_all = loss_all + loss1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10, 16], dpi=100)\n",
    "\n",
    "ax = plt.subplot(411)\n",
    "ax.plot(x_star, fg(x_star), 'b-', linewidth = 2, label = 'Exact')\n",
    "ax.plot(x_star, u3_p, 'r--', linewidth = 2, label = 'NN')\n",
    "ax.set_ylabel('$x$', fontsize=15, rotation = 0)\n",
    "ax.set_title('Function', fontsize=10)\n",
    "ax.set_xlim([-1, 1])\n",
    "\n",
    "ax1 = plt.subplot(412)\n",
    "ax1.plot(x_star, f1_p, 'b-', linewidth=2, label='Exact')\n",
    "ax1.set_ylabel('$x$', fontsize=15, rotation=0)\n",
    "ax1.set_title('Residue order 1', fontsize=10)\n",
    "ax1.set_xlim([-1.0, 1.0])\n",
    "\n",
    "\n",
    "ax2 = plt.subplot(413)\n",
    "ax2.plot(x_star, f2_p, 'b-', linewidth=2, label='Exact')\n",
    "ax2.set_ylabel('$x$', fontsize = 15, rotation = 0)\n",
    "ax2.set_title('Residue order 2', fontsize = 10)\n",
    "ax2.set_xlim([-1.0, 1.0])\n",
    "\n",
    "\n",
    "ax3 = plt.subplot(414)\n",
    "ax3.plot(x_star, f3_p, 'b-', linewidth=2, label='Exact')\n",
    "ax3.set_xlabel('$t$', fontsize = 15)\n",
    "ax3.set_ylabel('$x$', fontsize = 15, rotation = 0)\n",
    "ax3.set_title('Residue order 3', fontsize = 10)\n",
    "ax3.set_xlim([-1.0, 1.0])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
