{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7cf07-eafb-4de8-9351-79b1fdab0375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "from matplotlib import pyplot as plt, cm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.fft import fft2, fftshift, fftfreq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b2586b-025b-4bb4-b41a-cb65b8111866",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, t_u, x_u, layers, kappa, lt, ut, acts=0):\n",
    "\n",
    "        self.scale = tf.reduce_max(tf.abs(x_u)) / 2\n",
    "        x_u2 = x_u / self.scale\n",
    "        actv = [tf.tanh, tf.sin]\n",
    "\n",
    "        self.t_u = t_u\n",
    "        self.x_u = x_u2\n",
    "        self.datatype = t_u.dtype\n",
    "\n",
    "        self.lt = lt\n",
    "        self.ut = ut\n",
    "\n",
    "        self.layers = layers\n",
    "        self.kappa = kappa\n",
    "\n",
    "        # determine the activation function to use\n",
    "        self.actv = actv[acts]\n",
    "\n",
    "        # Initialize NNs\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # Create a list including all training variables\n",
    "        self.train_variables = self.weights + self.biases\n",
    "        # Key point: anything updates in train_variables will be\n",
    "        #            automatically updated in the original tf.Variable\n",
    "\n",
    "        # define the loss function\n",
    "        self.loss0 = self.scale ** 2\n",
    "        self.loss = []\n",
    "        self.loss_0 = self.loss_NN()\n",
    "\n",
    "        self.optimizer_Adam = tf.optimizers.Adam()\n",
    "\n",
    "    '''\n",
    "    Functions used to establish the initial neural network\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.MPL_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=self.datatype))\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    def MPL_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev, dtype=self.datatype))\n",
    "\n",
    "    '''\n",
    "    Functions used to building the physics-informed contrainst and loss\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def neural_net(self, X):\n",
    "        weights = self.weights\n",
    "        biases = self.biases\n",
    "\n",
    "        num_layers = len(weights) + 1\n",
    "        \n",
    "        H = 2.0 * tf.math.divide(tf.math.subtract(X, tf.transpose(self.lt)), tf.transpose(tf.math.subtract(self.ut, self.lt))) - 1.0\n",
    "        \n",
    "        W = weights[0]\n",
    "        b = biases[0]\n",
    "        H = self.actv(tf.add(self.kappa * tf.matmul(H, W), b)) # for scalar kappa value\n",
    "        # H = self.actv(tf.add(tf.matmul(H * self.kappa, W), b)) # for n-dim kappa values\n",
    "\n",
    "        for l in range(1, num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "\n",
    "    @tf.function\n",
    "    # calculate the physics-informed loss function\n",
    "    def loss_NN(self):\n",
    "        self.x_pred = self.neural_net(self.t_u)\n",
    "        loss = tf.reduce_mean(tf.square(self.x_u - self.x_pred))\n",
    "        return loss\n",
    "\n",
    "    '''\n",
    "    Functions used to define ADAM optimizers\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    # define the function to apply the ADAM optimizer\n",
    "    def adam_function(self):\n",
    "        @tf.function\n",
    "        def f():\n",
    "            # calculate the loss\n",
    "            loss_norm = self.loss_NN()\n",
    "            loss_value = loss_norm * self.loss0\n",
    "            # store loss value so we can retrieve later\n",
    "            tf.py_function(f.loss.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "            # print out iteration & loss\n",
    "            f.iter.assign_add(1)\n",
    "\n",
    "            str_iter = tf.strings.as_string([f.iter])\n",
    "            str_loss = tf.strings.as_string([loss_value], precision=4, scientific=True)\n",
    "\n",
    "            str_print = tf.strings.join([\"Mode: Adam\", \", Iter: \", str_iter[0],\n",
    "                                         \", loss: \", str_loss[0]])\n",
    "            tf.cond(\n",
    "                f.iter % 100 == 0,\n",
    "                lambda: tf.print(str_print),\n",
    "                lambda: tf.constant(True)  # return arbitrary for non-printing case\n",
    "            )\n",
    "            return loss_norm\n",
    "\n",
    "        f.iter = tf.Variable(0)\n",
    "        f.term = []\n",
    "        f.loss = []\n",
    "        return f\n",
    "\n",
    "    def Adam_optimizer(self, nIter):\n",
    "        varlist = self.train_variables\n",
    "        func_adam = self.adam_function()\n",
    "        for it in range(nIter):\n",
    "            tf.keras.optimizers.Adam(func_adam, varlist)\n",
    "            #self.optimizer_Adam.minimize(func_adam, varlist)\n",
    "        return func_adam\n",
    "\n",
    "    '''\n",
    "    Functions used to define L-BFGS optimizers\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    # A factory to create a function required by tfp.optimizer.lbfgs_minimize.\n",
    "    def Lbfgs_function(self, varlist):\n",
    "        # obtain the shapes of all trainable parameters in the model\n",
    "        shapes = tf.shape_n(varlist)\n",
    "        n_tensors = len(shapes)\n",
    "\n",
    "        # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n",
    "        # prepare required information first\n",
    "        count = 0\n",
    "        idx = []  # stitch indices\n",
    "        part = []  # partition indices\n",
    "\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        for i, shape in enumerate(shapes):\n",
    "            n = np.prod(shape)\n",
    "            idx.append(tf.reshape(tf.range(count, count + n, dtype=tf.int32), shape))\n",
    "            part.extend([i] * n)\n",
    "            count += n\n",
    "\n",
    "        part = tf.constant(part)\n",
    "\n",
    "        def assign_new_model_parameters(params_1d):\n",
    "            # A function updating the model's parameters with a 1D tf.Tensor.\n",
    "            # Sub-function under function of class not need to input self\n",
    "\n",
    "            params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "            for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "                varlist[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "        @tf.function\n",
    "        def f(params_1d):\n",
    "            # A function that can be used by tfp.optimizer.lbfgs_minimize.\n",
    "            # This function is created by function_factory.\n",
    "            # Sub-function under function of class not need to input self\n",
    "\n",
    "            # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters\n",
    "            with tf.GradientTape() as tape:\n",
    "                # update the parameters in the model\n",
    "                # this step is critical for self-defined function for L-BFGS\n",
    "                assign_new_model_parameters(params_1d)\n",
    "                # calculate the loss\n",
    "                loss_norm = self.loss_NN()\n",
    "                loss_value = loss_norm * self.loss0\n",
    "\n",
    "            # calculate gradients and convert to 1D tf.Tensor\n",
    "            grads = tape.gradient(loss_norm, varlist)\n",
    "            grads = tf.dynamic_stitch(idx, grads)\n",
    "\n",
    "            # store loss value so we can retrieve later\n",
    "            tf.py_function(f.loss.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "            # print out iteration & loss\n",
    "            f.iter.assign_add(1)\n",
    "\n",
    "            str_iter = tf.strings.as_string([f.iter])\n",
    "            str_loss = tf.strings.as_string([loss_value], precision=4, scientific=True)\n",
    "\n",
    "            str_print = tf.strings.join([\"Mode: LBFGS\", \", Iter: \", str_iter[0],\n",
    "                                         \", loss: \", str_loss[0]])\n",
    "            tf.cond(\n",
    "                f.iter % 3000 == 0,\n",
    "                lambda: tf.print(str_print),\n",
    "                lambda: tf.constant(True)  # return arbitrary for non-printing case\n",
    "            )\n",
    "\n",
    "            return loss_value, grads\n",
    "\n",
    "        # store these information as members so we can use them outside the scope\n",
    "        f.iter = tf.Variable(0)\n",
    "        f.idx = idx\n",
    "        f.part = part\n",
    "        f.shapes = shapes\n",
    "        f.assign_new_model_parameters = assign_new_model_parameters\n",
    "        f.loss = []\n",
    "\n",
    "        return f\n",
    "\n",
    "    # define the function to apply the L-BFGS optimizer\n",
    "    def Lbfgs_optimizer(self, nIter, varlist):\n",
    "\n",
    "        func = self.Lbfgs_function(varlist)\n",
    "\n",
    "        # convert initial model parameters to a 1D tf.Tensor\n",
    "        init_params = tf.dynamic_stitch(func.idx, varlist)\n",
    "\n",
    "        max_nIter = tf.cast(nIter / 3, dtype=tf.int32)\n",
    "\n",
    "        # train the model with L-BFGS solver\n",
    "        results = tfp.optimizer.lbfgs_minimize(\n",
    "            value_and_gradients_function=func, initial_position=init_params,\n",
    "            tolerance=1e-11, max_iterations=max_nIter)\n",
    "\n",
    "        # after training, the final optimized parameters are still in results.position\n",
    "        # so we have to manually put them back to the model\n",
    "        func.assign_new_model_parameters(results.position)\n",
    "\n",
    "        return func\n",
    "\n",
    "    '''\n",
    "    Function used for training the model\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def train(self, nIter, idxOpt):\n",
    "        if idxOpt == 1:\n",
    "            # mode 1: running the Adam optimization\n",
    "            func_adam = self.Adam_optimizer(nIter)\n",
    "            self.loss += func_adam.loss\n",
    "        elif idxOpt == 2:\n",
    "            # mode 2: running the Lbfgs optimization\n",
    "            func_bfgs = self.Lbfgs_optimizer(nIter, self.train_variables)\n",
    "            self.loss += func_bfgs.loss\n",
    "\n",
    "    # @tf.function\n",
    "    def predict(self, t):\n",
    "        x_p = self.neural_net(t) * self.scale\n",
    "        return x_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab9287-4503-4539-8c3e-2550bd0e1d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.0        \n",
    "np.random.seed(234)\n",
    "tf.random.set_seed(234)\n",
    "\n",
    "def fun_test(t):\n",
    "    # customize the function by the user\n",
    "    # x = (1 - (t**2) / 2) * np.cos(30*(t + 0.5*(t ** 3))) # example 1\n",
    "    # x = (t ** 3) / (0.01 + t ** 4) # example 2\n",
    "    x = tf.exp(t) - t - 1 # example 3\n",
    "    return x\n",
    "\n",
    "def fun_test2d(t):\n",
    "    x = (np.sin(2 * t[:, 0] + 1) - 0.5 * t[:, 0])*(1 - t[:, 1] ** 2)\n",
    "    return x\n",
    "\n",
    "def fun_(t1, t2):\n",
    "    return (np.sin(2 * t1 + 1) - 0.5 * t1)*(1 - t2 ** 2)\n",
    "\n",
    "# Number of train points (total # = Ntrain ** 2)\n",
    "Ntrain = 20\n",
    "t1 = np.linspace(-1.05, 1.05, Ntrain)[:, None]\n",
    "t2 = np.linspace(-1.05, 1.05, Ntrain)[:, None]\n",
    "xx, yy = np.meshgrid(t1, t2)\n",
    "t = tf.stack([tf.reshape(xx, [-1]), tf.reshape(yy, [-1])], axis=1)\n",
    "\n",
    "t_train_plot = tf.stack([t1, t2], axis=1)\n",
    "t_train_plot = tf.squeeze(t_train_plot)\n",
    "\n",
    "t_train = tf.cast(t, dtype=tf.float64)\n",
    "x_train = fun_test2d(t)\n",
    "x_train = tf.reshape(x_train, [len(t), 1])\n",
    "# For plotting purposes\n",
    "x_plot = fun_(xx, yy)\n",
    "print(t_train.shape)\n",
    "print(x_train.shape)\n",
    "\n",
    "# Domain bounds\n",
    "lt = [t1.min(0)[0], t2.min(0)[0]]\n",
    "ut = [t1.max(0)[0], t2.max(0)[0]]\n",
    "lt = tf.convert_to_tensor(lt)\n",
    "ut = tf.convert_to_tensor(ut)\n",
    "\n",
    "# Number of test points (total # = N_eval ** 2)\n",
    "N_eval = 200\n",
    "t1 = np.linspace(-1, 1, N_eval)[:, None]\n",
    "t2 = np.linspace(-1, 1, N_eval)[:, None]\n",
    "xx_eval, yy_eval = np.meshgrid(t1, t2)\n",
    "tt = tf.stack([tf.reshape(xx_eval, [-1]), tf.reshape(yy_eval, [-1])], axis=1)\n",
    "t_eval = tf.cast(tt, dtype=tf.float64)\n",
    "x_eval = fun_test2d(tt)\n",
    "x_eval = tf.reshape(x_eval, [len(tt), 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc862d7e-8945-465d-8b83-562fd98ac5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(xx, yy, x_plot, cmap=cm.Spectral)\n",
    "ax.set_xticks([-1, 0, 1])\n",
    "ax.set_yticks([-1, 0, 1])\n",
    "ax.set_zticks([-0.5, 0, 0.5, 1])\n",
    "ax.invert_xaxis()\n",
    "ax.view_init(elev=30, azim=45)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_zlabel('$u_g$')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(x_plot, extent=[-1, 1, -1, 1], origin='lower', cmap=cm.Spectral)\n",
    "plt.xticks([-1, -0.5, 0, 0.5, 1])\n",
    "plt.yticks([-1, -0.5, 0, 0.5, 1])\n",
    "contour_filled = plt.contourf(x_plot, extent=[-1, 1, -1, 1], cmap=cm.Spectral, alpha=0.4)\n",
    "plt.colorbar(contour_filled)\n",
    "#plt.gca().invert_yaxis()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "g = x_train.numpy().flatten()\n",
    "# grad_x = np.gradient(g, t_train.numpy().flatten())\n",
    "# print(max(grad_x)/(2 * np.pi))\n",
    "\n",
    "# ax = plt.plot(t_train, grad_x, 'b')\n",
    "# plt.grid(True)\n",
    "# plt.xlabel(r'$t$')\n",
    "# plt.ylabel(r'$\\frac{du}{dt}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd91d9b4",
   "metadata": {},
   "source": [
    "## Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f1fa1b-8657-46ff-878d-01fe82b1c70d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Train a neural network to learn the error of stage 1\n",
    "'''\n",
    "# acts = 0 indicates selecting tanh as the activation function\n",
    "layers = [2, 30, 30, 30, 30, 1]\n",
    "kappa0 =  [1, 1]\n",
    "model_0 = PhysicsInformedNN(t_train, x_train, layers, kappa0, lt, ut, acts=1)\n",
    "# start the first stage training\n",
    "model_0.train(5000, 1)     # mode 1 use Adam\n",
    "model_0.train(20000, 2)    # mode 2 use L-bfgs\n",
    "x_pred = model_0.predict(t_eval)\n",
    "\n",
    "error_x = np.linalg.norm(x_eval-x_pred, 2)/np.linalg.norm(x_eval, 2)\n",
    "print('Error u: %e' % (error_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a066d44-d30f-4bd2-9635-0c1c06509a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.plot_surface(xx_eval, yy_eval, x_pred.numpy().reshape(N_eval, N_eval), cmap=cm.Spectral)\n",
    "ax.set_xticks([-1, 0, 1])\n",
    "ax.set_yticks([-1, 0, 1])\n",
    "ax.set_zticks([-0.5, 0, 0.5, 1])\n",
    "ax.invert_xaxis()\n",
    "ax.view_init(elev=30, azim=45)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_zlabel('$u_0$')\n",
    "plt.title(\"First stage prediction $u_0$\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeff897-c91a-44c7-ae0b-e6c9abad7d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the error for the first stage\n",
    "eg_1 = x_train - model_0.predict(t_train)\n",
    "eps_1 = np.sqrt(1/t_train.shape[0] * sum([i**2 for i in eg_1]))\n",
    "eg_1_norm = eg_1 / eps_1\n",
    "g = eg_1_norm.numpy().flatten()\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.plot_surface(xx, yy, eg_1.numpy().reshape(Ntrain, Ntrain), cmap=cm.Spectral)\n",
    "ax.zaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True))\n",
    "#ax.zaxis.get_offset_text().set_position((0, 0, 1))\n",
    "ax.set_xticks([-1, 0, 1])\n",
    "ax.set_yticks([-1, 0, 1])\n",
    "ax.zaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(axis='z', style='sci', scilimits=(0,0))\n",
    "ax.invert_xaxis()\n",
    "ax.invert_yaxis()\n",
    "ax.view_init(elev=20, azim=45)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_zlabel('$e_1$')\n",
    "plt.grid(True)\n",
    "plt.title('First stage residue: $e_1 = u_g - u_0$', fontsize=10)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(eps_1 * g.reshape(Ntrain, Ntrain), extent=[-1, 1, -1, 1], origin='lower', cmap=cm.Spectral)\n",
    "plt.xticks([-1, -0.5, 0, 0.5, 1])\n",
    "plt.yticks([-1, -0.5, 0, 0.5, 1])\n",
    "colorbar = plt.colorbar()\n",
    "colorbar.formatter.set_powerlimits((0, 0))\n",
    "colorbar.update_ticks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f11a519-0365-414a-9b37-43521d9d2c2e",
   "metadata": {},
   "source": [
    "## Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a63ac-c722-4bde-a837-8141624bdf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 2D FFT\n",
    "GG = g.reshape(Ntrain, Ntrain)\n",
    "G = fft2(GG)\n",
    "G_shifted = fftshift(G)  # Shift zero frequency component to the center\n",
    "\n",
    "# Frequency coordinates\n",
    "N = GG.shape[0]\n",
    "total_time_range = 2  # From -1 to 1\n",
    "sample_rate = N / total_time_range\n",
    "freq_x = fftshift(fftfreq(GG.shape[1], d=1/sample_rate))\n",
    "freq_y = fftshift(fftfreq(GG.shape[0], d=1/sample_rate))\n",
    "\n",
    "half_N = N // 2\n",
    "G_pos = G_shifted[half_N:, half_N:]\n",
    "freq_x_pos = freq_x[half_N:]\n",
    "freq_y_pos = freq_y[half_N:]\n",
    "\n",
    "# Magnitude of the positive frequency FFT\n",
    "magnitude_spectrum = np.abs(G_pos)\n",
    "\n",
    "max_idx = np.unravel_index(np.argmax(magnitude_spectrum), magnitude_spectrum.shape)\n",
    "dominant_freq_x = freq_x_pos[max_idx[1]]\n",
    "dominant_freq_y = freq_y_pos[max_idx[0]]\n",
    "magnitude = magnitude_spectrum[max_idx] / (N * N)  # Normalize the magnitude\n",
    "\n",
    "magnitude_spectrum_norm = magnitude_spectrum/np.max(np.max(magnitude_spectrum))\n",
    "print(f\"Sample Rate: {sample_rate} Hz\")\n",
    "print(f\"Dominant Frequency X: {dominant_freq_x} Hz\")\n",
    "print(f\"Dominant Frequency Y: {dominant_freq_y} Hz\")\n",
    "print(f\"Magnitude: {magnitude}\")\n",
    "\n",
    "dominant_freq = np.max([dominant_freq_x, dominant_freq_y])\n",
    "print(f\"Dominant Frequency: {dominant_freq} Hz\")\n",
    "\n",
    "X, Y = np.meshgrid(freq_x_pos, freq_y_pos)\n",
    "# Magnitude spectrum of the 2D FFT\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pcolormesh(X, Y, magnitude_spectrum_norm, shading='auto', cmap='magma')\n",
    "plt.colorbar(label='Freq. amp')\n",
    "plt.title('Frequency domain of $e_1(x, y)$')\n",
    "#plt.xlim([0, 5])\n",
    "#plt.ylim([0, 5])\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('m')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071fc4a-e43a-4085-8788-05445a7df013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate kappa for Stage 2\n",
    "if dominant_freq < g.size / (6 * np.pi):\n",
    "    kappa1 = 2 * np.pi * dominant_freq\n",
    "else:\n",
    "    print('Not enough data: Please Increase')\n",
    "\n",
    "# 2d case with 2 values for kappa\n",
    "# if dominant_freq_x < g.size / (6 * np.pi) and dominant_freq_y < g.size / (6 * np.pi):\n",
    "#     kappa1 = [[2 * np.pi * dominant_freq_x], [2 * np.pi * dominant_freq_y]]\n",
    "#     print('Kappa 1', kappa1)\n",
    "# else:\n",
    "#     print('Not enough data: Please Increase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98d3b8-384c-4f0c-976e-6b5d03f3eda5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Train Neural Network (the error)\n",
    "'''\n",
    "layers = [2, 30, 30, 30, 30, 1]\n",
    "model_1 = PhysicsInformedNN(t_train, eg_1_norm, layers, kappa1, lt, ut, acts=1)\n",
    "# start the second stage training\n",
    "model_1.train(5000, 1)    # mode 1 use Adam\n",
    "model_1.train(20000, 2)   # mode 2 use L-bfgs\n",
    "eg_1_pred = model_1.predict(t_eval)\n",
    "# combining the result from first and second stage\n",
    "x_p = x_pred + eps_1 * eg_1_pred\n",
    "\n",
    "error_x = np.linalg.norm(x_eval-x_p, 2)/np.linalg.norm(x_eval, 2)\n",
    "print('Error u: %e' % (error_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27871d8c-b48b-47ea-bad4-cced8666fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the residue for the second stage\n",
    "eg_2 = x_train - model_0.predict(t_train) - eps_1 * model_1.predict(t_train)\n",
    "eg_2_plot = x_plot - model_0.predict(t_train_plot) - eps_1 * model_1.predict(t_train_plot)\n",
    "eps_2 = np.sqrt(1/t_train.shape[0] * sum([i**2 for i in eg_2]))\n",
    "eg_2_norm = eg_2 / eps_2\n",
    "g = eg_2_norm.numpy().flatten()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.plot_surface(xx, yy, eg_2.numpy().reshape(Ntrain, Ntrain), cmap=cm.Spectral)\n",
    "ax.zaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True))\n",
    "#ax.zaxis.get_offset_text().set_position((0, 0, 1))\n",
    "ax.set_xticks([-1, 0, 1])\n",
    "ax.set_yticks([-1, 0, 1])\n",
    "ax.zaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(axis='z', style='sci', scilimits=(0,0))\n",
    "ax.invert_xaxis()\n",
    "ax.invert_yaxis()\n",
    "ax.view_init(elev=20, azim=45)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_zlabel('$e_1$')\n",
    "plt.grid(True)\n",
    "plt.title('Second stage residue: $e_2 = e_1 - eps_1*u_1$', fontsize=10)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(eps_2 * g.reshape(Ntrain, Ntrain), extent=[-1, 1, -1, 1], origin='lower', cmap=cm.Spectral)\n",
    "plt.xticks([-1, -0.5, 0, 0.5, 1])\n",
    "plt.yticks([-1, -0.5, 0, 0.5, 1])\n",
    "colorbar = plt.colorbar()\n",
    "colorbar.formatter.set_powerlimits((0, 0))\n",
    "colorbar.update_ticks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f605c719-464b-4061-9eea-5a46a3f358a6",
   "metadata": {},
   "source": [
    "## Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae03a5-d891-4dcc-a1e6-a6a96e19ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 2D FFT\n",
    "GG = g.reshape(Ntrain, Ntrain)\n",
    "G = fft2(GG)\n",
    "G_shifted = fftshift(G)  # Shift zero frequency component to the center\n",
    "\n",
    "# Frequency coordinates\n",
    "N = GG.shape[0]\n",
    "total_time_range = 2  # From -1 to 1\n",
    "sample_rate = N / total_time_range\n",
    "freq_x = fftshift(fftfreq(GG.shape[1], d=1/sample_rate))\n",
    "freq_y = fftshift(fftfreq(GG.shape[0], d=1/sample_rate))\n",
    "\n",
    "half_N = N // 2\n",
    "G_pos = G_shifted[half_N:, half_N:]\n",
    "freq_x_pos = freq_x[half_N:]\n",
    "freq_y_pos = freq_y[half_N:]\n",
    "\n",
    "# Magnitude of the positive frequency FFT\n",
    "magnitude_spectrum = np.abs(G_pos)\n",
    "\n",
    "max_idx = np.unravel_index(np.argmax(magnitude_spectrum), magnitude_spectrum.shape)\n",
    "dominant_freq_x = freq_x_pos[max_idx[1]]\n",
    "dominant_freq_y = freq_y_pos[max_idx[0]]\n",
    "magnitude = magnitude_spectrum[max_idx] / (N * N)  # Normalize the magnitude\n",
    "\n",
    "magnitude_spectrum_norm = magnitude_spectrum/np.max(np.max(magnitude_spectrum))\n",
    "print(f\"Sample Rate: {sample_rate} Hz\")\n",
    "print(f\"Dominant Frequency X: {dominant_freq_x} Hz\")\n",
    "print(f\"Dominant Frequency Y: {dominant_freq_y} Hz\")\n",
    "print(f\"Magnitude: {magnitude}\")\n",
    "\n",
    "dominant_freq = np.max([dominant_freq_x, dominant_freq_y])\n",
    "print(f\"Dominant Frequency: {dominant_freq} Hz\")\n",
    "\n",
    "X, Y = np.meshgrid(freq_x_pos, freq_y_pos)\n",
    "# Magnitude spectrum of the 2D FFT\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pcolormesh(X, Y, magnitude_spectrum_norm, shading='auto', cmap='magma')\n",
    "plt.colorbar(label='Freq. amp')\n",
    "plt.title('Frequency domain of $e_2(x, y)$')\n",
    "#plt.xlim([0, 5])\n",
    "#plt.ylim([0, 5])\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('m')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dominant_freq < g.size / (6 * np.pi):\n",
    "    kappa2 = 2 * np.pi * dominant_freq\n",
    "    print('Kappa 2', kappa2)\n",
    "else:\n",
    "    print('Not enough data: Please Increase')\n",
    "\n",
    "# 2d case with 2 values for kappa\n",
    "# if dominant_freq_x < g.size / (6 * np.pi) and dominant_freq_y < g.size / (6 * np.pi):\n",
    "#     kappa2 = [[2 * np.pi * dominant_freq_x], [2 * np.pi * dominant_freq_y]]\n",
    "#     print('Kappa 2', kappa2)\n",
    "# else:\n",
    "#     print('Not enough data: Please Increase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train Neural Network (the error)\n",
    "'''\n",
    "# (acts = 1 indicates selecting sin as the activation function)\n",
    "layers = [2, 20, 20, 20, 1]\n",
    "model_2 = PhysicsInformedNN(t_train, eg_2_norm, layers, kappa2, lt, ut, acts=1)\n",
    "# start the third stage training\n",
    "model_2.train(5000, 1)      # mode 1 use Adam\n",
    "model_2.train(20000, 2)     # mode 2 use L-bfgs\n",
    "eg_2_pred = model_2.predict(t_eval)\n",
    "# combining the result from first, second and third stages\n",
    "x_p = x_pred + eps_1 * eg_1_pred + eps_2 * eg_2_pred\n",
    "\n",
    "error_x = np.linalg.norm(x_eval-x_p, 2)/np.linalg.norm(x_eval, 2)\n",
    "print('Error u: %e' % (error_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the residue for the second stage\n",
    "eg_3 = x_train - model_0.predict(t_train) - eps_1 * model_1.predict(t_train) - eps_2 * model_2.predict(t_train)\n",
    "eps_3 = np.sqrt(1/t_train.shape[0] * sum([i**2 for i in eg_3]))\n",
    "eg_3_norm = eg_3 / eps_3\n",
    "g = eg_3_norm.numpy().flatten()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.plot_surface(xx, yy, eg_3.numpy().reshape(Ntrain, Ntrain), cmap=cm.Spectral)\n",
    "ax.zaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True))\n",
    "#ax.zaxis.get_offset_text().set_position((0, 0, 1))\n",
    "ax.set_xticks([-1, 0, 1])\n",
    "ax.set_yticks([-1, 0, 1])\n",
    "ax.zaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(axis='z', style='sci', scilimits=(0,0))\n",
    "ax.invert_xaxis()\n",
    "ax.invert_yaxis()\n",
    "ax.view_init(elev=20, azim=45)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_zlabel('$e_1$')\n",
    "plt.grid(True)\n",
    "plt.title('Third stage residue: $e_3 = e_1 - eps_1*u_1$ - eps_2*u_2', fontsize=10)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(eps_3 * g.reshape(Ntrain, Ntrain), extent=[-1, 1, -1, 1], origin='lower', cmap=cm.Spectral)\n",
    "plt.xticks([-1, -0.5, 0, 0.5, 1])\n",
    "plt.yticks([-1, -0.5, 0, 0.5, 1])\n",
    "colorbar = plt.colorbar()\n",
    "colorbar.formatter.set_powerlimits((0, 0))\n",
    "colorbar.update_ticks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39411a69",
   "metadata": {},
   "source": [
    "## Stage 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b276e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 2D FFT\n",
    "GG = g.reshape(Ntrain, Ntrain)\n",
    "G = fft2(GG)\n",
    "G_shifted = fftshift(G)  # Shift zero frequency component to the center\n",
    "\n",
    "# Frequency coordinates\n",
    "N = GG.shape[0]\n",
    "total_time_range = 2  # From -1 to 1\n",
    "sample_rate = N / total_time_range\n",
    "freq_x = fftshift(fftfreq(GG.shape[1], d=1/sample_rate))\n",
    "freq_y = fftshift(fftfreq(GG.shape[0], d=1/sample_rate))\n",
    "\n",
    "half_N = N // 2\n",
    "G_pos = G_shifted[half_N:, half_N:]\n",
    "freq_x_pos = freq_x[half_N:]\n",
    "freq_y_pos = freq_y[half_N:]\n",
    "\n",
    "# Magnitude of the positive frequency FFT\n",
    "magnitude_spectrum = np.abs(G_pos)\n",
    "\n",
    "max_idx = np.unravel_index(np.argmax(magnitude_spectrum), magnitude_spectrum.shape)\n",
    "dominant_freq_x = freq_x_pos[max_idx[1]]\n",
    "dominant_freq_y = freq_y_pos[max_idx[0]]\n",
    "magnitude = magnitude_spectrum[max_idx] / (N * N)  # Normalize the magnitude\n",
    "\n",
    "magnitude_spectrum_norm = magnitude_spectrum/np.max(np.max(magnitude_spectrum))\n",
    "print(f\"Sample Rate: {sample_rate} Hz\")\n",
    "print(f\"Dominant Frequency X: {dominant_freq_x} Hz\")\n",
    "print(f\"Dominant Frequency Y: {dominant_freq_y} Hz\")\n",
    "print(f\"Magnitude: {magnitude}\")\n",
    "\n",
    "dominant_freq = np.max([dominant_freq_x, dominant_freq_y])\n",
    "print(f\"Dominant Frequency: {dominant_freq} Hz\")\n",
    "\n",
    "X, Y = np.meshgrid(freq_x_pos, freq_y_pos)\n",
    "# Magnitude spectrum of the 2D FFT\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pcolormesh(X, Y, magnitude_spectrum_norm, shading='auto', cmap='magma')\n",
    "plt.colorbar(label='Freq. amp')\n",
    "plt.title('Frequency domain of $e_3(x, y)$')\n",
    "#plt.xlim([0, 5])\n",
    "#plt.ylim([0, 5])\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('m')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dominant_freq < g.size / (6 * np.pi):\n",
    "    kappa3 = 2 * np.pi * dominant_freq\n",
    "    print('Kappa 3', kappa3)\n",
    "else:\n",
    "    print('Not enough data: Please Increase')\n",
    "\n",
    "# 2d case with 2 values for kappa\n",
    "# if dominant_freq_x < g.size / (6 * np.pi) and dominant_freq_y < g.size / (6 * np.pi):\n",
    "#     kappa3 = [[2 * np.pi * dominant_freq_x], [2 * np.pi * dominant_freq_y]]\n",
    "#     print('Kappa 3', kappa3)\n",
    "# else:\n",
    "#     print('Not enough data: Please Increase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54344cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train Neural Network (the error)\n",
    "'''\n",
    "# (acts = 1 indicates selecting sin as the activation function)\n",
    "layers = [2, 20, 20, 20, 1]\n",
    "model_3 = PhysicsInformedNN(t_train, eg_3_norm, layers, kappa3, lt, ut, acts=1)\n",
    "# start the third stage training\n",
    "model_3.train(5000, 1)      # mode 1 use Adam\n",
    "model_3.train(20000, 2)     # mode 2 use L-bfgs\n",
    "eg_3_pred = model_3.predict(t_eval)\n",
    "# combining the result from first, second and third stages\n",
    "x_p = x_pred + eps_1 * eg_1_pred + eps_2 * eg_2_pred + eps_3 * eg_3_pred\n",
    "\n",
    "error_x = np.linalg.norm(x_eval-x_p, 2)/np.linalg.norm(x_eval, 2)\n",
    "print('Error u: %e' % (error_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cff1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the residue for the second stage\n",
    "eg_4 = x_train - model_0.predict(t_train) - eps_1 * model_1.predict(t_train) - eps_2 * model_2.predict(t_train) - eps_3 * model_3.predict(t_train)\n",
    "eps_4 = np.sqrt(1/t_train.shape[0] * sum([i**2 for i in eg_4]))\n",
    "eg_4_norm = eg_4 / eps_4\n",
    "g = eg_4_norm.numpy().flatten()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.plot_surface(xx, yy, eg_4.numpy().reshape(Ntrain, Ntrain), cmap=cm.Spectral)\n",
    "ax.zaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True))\n",
    "#ax.zaxis.get_offset_text().set_position((0, 0, 1))\n",
    "ax.set_xticks([-1, 0, 1])\n",
    "ax.set_yticks([-1, 0, 1])\n",
    "ax.zaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(axis='z', style='sci', scilimits=(0,0))\n",
    "ax.invert_xaxis()\n",
    "ax.invert_yaxis()\n",
    "ax.view_init(elev=20, azim=45)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_zlabel('$e_1$')\n",
    "plt.grid(True)\n",
    "plt.title('Fourth stage residue: $e_4 = e_1 - eps_1*u_1$ - eps_2*u_2 - eps_3*u_3', fontsize=10)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(eps_4 * g.reshape(Ntrain, Ntrain), extent=[-1, 1, -1, 1], origin='lower', cmap=cm.Spectral)\n",
    "plt.xticks([-1, -0.5, 0, 0.5, 1])\n",
    "plt.yticks([-1, -0.5, 0, 0.5, 1])\n",
    "colorbar = plt.colorbar()\n",
    "colorbar.formatter.set_powerlimits((0, 0))\n",
    "colorbar.update_ticks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
