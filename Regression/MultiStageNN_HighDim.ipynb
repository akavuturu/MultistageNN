{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import tensorflow as tf\n",
    "from tensorflow.math import reduce_min, reduce_max\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "from pyDOE import lhs\n",
    "from scipy.io import savemat\n",
    "from scipy.fft import fft, fftfreq, fftshift, fft2, fftn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    # Initialize the class\n",
    "    def __init__(self, t_u, x_u, layers, kappa, lt, ut, acts=0):\n",
    "\n",
    "        self.scale = tf.reduce_max(tf.abs(x_u)) / 2\n",
    "        x_u2 = x_u / self.scale\n",
    "        actv = [tf.tanh, tf.sin]\n",
    "\n",
    "        self.t_u = t_u\n",
    "        self.x_u = x_u2\n",
    "        self.datatype = t_u.dtype\n",
    "\n",
    "        self.lt = lt\n",
    "        self.ut = ut\n",
    "\n",
    "        self.layers = layers\n",
    "        self.kappa = kappa\n",
    "\n",
    "        # determine the activation function to use\n",
    "        self.actv = actv[acts]\n",
    "\n",
    "        # Initialize NNs\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # Create a list including all training variables\n",
    "        self.train_variables = self.weights + self.biases\n",
    "        # Key point: anything updates in train_variables will be\n",
    "        #            automatically updated in the original tf.Variable\n",
    "\n",
    "        # define the loss function\n",
    "        self.loss0 = self.scale ** 2\n",
    "        self.loss = []\n",
    "        self.loss_0 = self.loss_NN()\n",
    "\n",
    "        self.optimizer_Adam = tf.optimizers.Adam()\n",
    "\n",
    "    '''\n",
    "    Functions used to establish the initial neural network\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.MPL_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=self.datatype))\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    def MPL_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev, dtype=self.datatype))\n",
    "\n",
    "    def get_params(self):\n",
    "        return (self.weights, self.biases)\n",
    "\n",
    "    '''\n",
    "    Functions used to building the physics-informed contrainst and loss\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def neural_net(self, X):\n",
    "        weights = self.weights\n",
    "        biases = self.biases\n",
    "\n",
    "        num_layers = len(weights) + 1\n",
    "\n",
    "        H = 2.0 * tf.math.divide(\n",
    "                    tf.math.subtract(X, tf.transpose(self.lt)), \n",
    "                    tf.transpose(tf.math.subtract(self.ut, self.lt))) \\\n",
    "            - 1.0\n",
    "\n",
    "        W = weights[0]\n",
    "        b = biases[0]\n",
    "        H = self.actv(tf.add(self.kappa * tf.matmul(H, W), b))\n",
    "\n",
    "        for l in range(1, num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "\n",
    "    @tf.function\n",
    "    # calculate the physics-informed loss function\n",
    "    def loss_NN(self):\n",
    "        self.x_pred = self.neural_net(self.t_u)\n",
    "        loss = tf.reduce_mean(tf.square(self.x_u - self.x_pred))\n",
    "        return loss\n",
    "\n",
    "    '''\n",
    "    Functions used to define ADAM optimizers\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    # define the function to apply the ADAM optimizer\n",
    "    def adam_function(self):\n",
    "        @tf.function\n",
    "        def f():\n",
    "            # calculate the loss\n",
    "            loss_norm = self.loss_NN()\n",
    "            loss_value = loss_norm * self.loss0\n",
    "            # store loss value so we can retrieve later\n",
    "            tf.py_function(f.loss.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "            # print out iteration & loss\n",
    "            f.iter.assign_add(1)\n",
    "\n",
    "            str_iter = tf.strings.as_string([f.iter])\n",
    "            str_loss = tf.strings.as_string([loss_value], precision=4, scientific=True)\n",
    "\n",
    "            str_print = tf.strings.join([\"Mode: Adam\", \"Iter: \", str_iter[0],\n",
    "                                         \", loss: \", str_loss[0]])\n",
    "            tf.cond(\n",
    "                f.iter % 10 == 0,\n",
    "                lambda: tf.print(str_print),\n",
    "                lambda: tf.constant(True)  # return arbitrary for non-printing case\n",
    "            )\n",
    "            return loss_norm\n",
    "\n",
    "        f.iter = tf.Variable(0)\n",
    "        f.term = []\n",
    "        f.loss = []\n",
    "        return f\n",
    "\n",
    "    def Adam_optimizer(self, nIter):\n",
    "        varlist = self.train_variables\n",
    "        func_adam = self.adam_function()\n",
    "        for it in range(nIter):\n",
    "            tf.keras.optimizers.Adam(func_adam, varlist)\n",
    "            #self.optimizer_Adam.minimize(func_adam, varlist)\n",
    "        return func_adam\n",
    "\n",
    "    '''\n",
    "    Functions used to define L-BFGS optimizers\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    # A factory to create a function required by tfp.optimizer.lbfgs_minimize.\n",
    "    def Lbfgs_function(self, varlist):\n",
    "        # obtain the shapes of all trainable parameters in the model\n",
    "        shapes = tf.shape_n(varlist)\n",
    "        n_tensors = len(shapes)\n",
    "\n",
    "        # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n",
    "        # prepare required information first\n",
    "        count = 0\n",
    "        idx = []  # stitch indices\n",
    "        part = []  # partition indices\n",
    "\n",
    "        for i, shape in enumerate(shapes):\n",
    "            n = np.prod(shape)\n",
    "            idx.append(tf.reshape(tf.range(count, count + n, dtype=tf.int32), shape))\n",
    "            part.extend([i] * n)\n",
    "            count += n\n",
    "\n",
    "        part = tf.constant(part)\n",
    "\n",
    "        def assign_new_model_parameters(params_1d):\n",
    "            # A function updating the model's parameters with a 1D tf.Tensor.\n",
    "            # Sub-function under function of class not need to input self\n",
    "\n",
    "            params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "            for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "                varlist[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "        @tf.function\n",
    "        def f(params_1d):\n",
    "            # A function that can be used by tfp.optimizer.lbfgs_minimize.\n",
    "            # This function is created by function_factory.\n",
    "            # Sub-function under function of class not need to input self\n",
    "\n",
    "            # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters\n",
    "            with tf.GradientTape() as tape:\n",
    "                # update the parameters in the model\n",
    "                # this step is critical for self-defined function for L-BFGS\n",
    "                assign_new_model_parameters(params_1d)\n",
    "                # calculate the loss\n",
    "                loss_norm = self.loss_NN()\n",
    "                loss_value = loss_norm * self.loss0\n",
    "\n",
    "            # calculate gradients and convert to 1D tf.Tensor\n",
    "            grads = tape.gradient(loss_norm, varlist)\n",
    "            grads = tf.dynamic_stitch(idx, grads)\n",
    "\n",
    "            # store loss value so we can retrieve later\n",
    "            tf.py_function(f.loss.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "            # print out iteration & loss\n",
    "            f.iter.assign_add(1)\n",
    "\n",
    "            str_iter = tf.strings.as_string([f.iter])\n",
    "            str_loss = tf.strings.as_string([loss_value], precision=4, scientific=True)\n",
    "\n",
    "            str_print = tf.strings.join([\"Mode: LBFGS\", \"Iter: \", str_iter[0],\n",
    "                                         \", loss: \", str_loss[0]])\n",
    "            tf.cond(\n",
    "                f.iter % 3000 == 0,\n",
    "                lambda: tf.print(str_print),\n",
    "                lambda: tf.constant(True)  # return arbitrary for non-printing case\n",
    "            )\n",
    "\n",
    "            return loss_value, grads\n",
    "\n",
    "        # store these information as members so we can use them outside the scope\n",
    "        f.iter = tf.Variable(0)\n",
    "        f.idx = idx\n",
    "        f.part = part\n",
    "        f.shapes = shapes\n",
    "        f.assign_new_model_parameters = assign_new_model_parameters\n",
    "        f.loss = []\n",
    "\n",
    "        return f\n",
    "\n",
    "    # define the function to apply the L-BFGS optimizer\n",
    "    def Lbfgs_optimizer(self, nIter, varlist):\n",
    "\n",
    "        func = self.Lbfgs_function(varlist)\n",
    "\n",
    "        # convert initial model parameters to a 1D tf.Tensor\n",
    "        init_params = tf.dynamic_stitch(func.idx, varlist)\n",
    "\n",
    "        max_nIter = tf.cast(nIter / 3, dtype=tf.int32)\n",
    "\n",
    "        # train the model with L-BFGS solver\n",
    "        results = tfp.optimizer.lbfgs_minimize(\n",
    "            value_and_gradients_function=func, initial_position=init_params,\n",
    "            tolerance=1e-11, max_iterations=max_nIter)\n",
    "\n",
    "        # after training, the final optimized parameters are still in results.position\n",
    "        # so we have to manually put them back to the model\n",
    "        func.assign_new_model_parameters(results.position)\n",
    "\n",
    "        return func\n",
    "\n",
    "    '''\n",
    "    Function used for training the model\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def train(self, nIter, idxOpt):\n",
    "        if idxOpt == 1:\n",
    "            # mode 1: running the Adam optimization\n",
    "            func_adam = self.Adam_optimizer(nIter)\n",
    "            self.loss += func_adam.loss\n",
    "        elif idxOpt == 2:\n",
    "            # mode 2: running the Lbfgs optimization\n",
    "            func_bfgs = self.Lbfgs_optimizer(nIter, self.train_variables)\n",
    "            self.loss += func_bfgs.loss\n",
    "\n",
    "    # @tf.function\n",
    "    def predict(self, t):\n",
    "        x_p = self.neural_net(t) * self.scale\n",
    "        return x_p\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultistageNeuralNetwork:\n",
    "    '''MultistageNeuralNetwork is the multi-stage model used for predicting\n",
    "    high-dimensional function outputs through regression. The network takes as parameters the input dataset,\n",
    "    an n-dimensional square tensor of grid points, a list of labels corresponding to function outputs at each grid point,\n",
    "    a number of stages for the MSNN, a number of hidden layers for each stage, and a number of hidden nodes in each\n",
    "    hidden layer of each stage of the MSNN'''\n",
    "    def __init__(self, dataset, labels, num_stages, num_hidden_layers, num_hidden_nodes):\n",
    "        # metadata for MSNN setup\n",
    "        self.dim = len(dataset.shape)\n",
    "        self.input = dataset\n",
    "        self.labels = labels\n",
    "        self.stages = [None] * num_stages\n",
    "        self.layers = [self.dim] + ([num_hidden_nodes] * num_hidden_layers) + [1]\n",
    "        self.lt = [reduce_min(self.input[:, i]) for i in range(self.input.shape[-1])]\n",
    "        self.ut = [reduce_max(self.input[:, i]) for i in range(self.input.shape[-1])]\n",
    "        \n",
    "        # initialize first stage network\n",
    "        self.sample(dataset, labels, sample_step=5)\n",
    "        # print(stage_1_data, stage_1_labels)\n",
    "        # self.stages[0] = NeuralNet(self.data, self.labels, self.layers, kappa=1, lt=self.lt, ut=self.ut, acts=0)\n",
    "\n",
    "    ## TODO - fix how we sample points so that the resulting grid is also squrae\n",
    "    def sample(self, dataset, labels, sample_step):\n",
    "        flattened_data = dataset.numpy().flatten()\n",
    "        sample_data = flattened_data[::sample_step]\n",
    "        sample_tf = tf.convert_to_tensor(sample_data, tf.float64)\n",
    "        new_shape = [dataset.shape[0] // sample_step] * self.dim\n",
    "        sample_tf = tf.reshape(sample_tf, new_shape)\n",
    "        sample_labels = labels[::sample_step]\n",
    "        return sample_tf, sample_labels\n",
    "\n",
    "    def train(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.zeros([20, 20])\n",
    "labels = [0] * 100 ** 3\n",
    "test_net = MultistageNeuralNetwork(dataset, labels, 4, 3, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(234)\n",
    "tf.random.set_seed(234)\n",
    "\n",
    "def fun_test(t):\n",
    "    # customize the function by the user\n",
    "    x = (t ** 3) / (0.01 + t ** 4) # example 2\n",
    "    #x = tf.math.log(t+2) * tf.cos(2*t + t**3)   # example 1\n",
    "    # x = tf.sin(2*t+1) + 0.2*tf.exp(1.3*t)  # example 2\n",
    "    return x\n",
    "\n",
    "def create_ds(dims, lo, hi, N):\n",
    "    dimensions = [np.linspace(lo, hi, N)] * dims\n",
    "    mesh = np.meshgrid(*dimensions, indexing='xy')\n",
    "    mesh_points = np.stack([m.flatten() for m in mesh], axis=-1)\n",
    "    mesh_tf = tf.cast(mesh_points, dtype=tf.float64)\n",
    "    return mesh_tf\n",
    "\n",
    "def fun_test2d(t):\n",
    "    x = (np.sin(2 * t[:, 0] + 1) - 0.5 * t[:, 0])*(1 - t[:, 1] ** 2)\n",
    "    return x\n",
    "\n",
    "def nd_fun(t):\n",
    "    if t.shape[-1] <= 1: return t\n",
    "    x = [tf.math.reduce_sum(t, axis=-1)]\n",
    "    return x\n",
    "\n",
    "def poisson(t, dims):\n",
    "    # equation is:\n",
    "    # (sum of partials of x) = partials of (1 - l2 norm(x)^2)*(sum_{i=1}^{d-1}c_i * sin(x_i + cos(x_{i+1} + x_{i+1}cos(x_i))))\n",
    "    pass\n",
    "\n",
    "def fft_analysis(residue, N_train, dim):\n",
    "    # Calculate dominant frequency\n",
    "    g = residue.numpy().flatten()\n",
    "    GG = g.reshape(N_train, N_train)\n",
    "    print(GG)\n",
    "    G = fftn(GG)\n",
    "    G_shifted = fftshift(G)\n",
    "    print(G_shifted.shape)\n",
    "\n",
    "    N = len(G)\n",
    "    # Total time range\n",
    "    total_time_range = 2  # from -1 to 1\n",
    "\n",
    "    # Calculate the sample rate\n",
    "    sample_rate = N / total_time_range\n",
    "\n",
    "    # # Perform FFT\n",
    "    half_N = N // 2\n",
    "    T = 1.0 / sample_rate\n",
    "    G_pos = G_shifted[half_N:, half_N:]\n",
    "    freq_x = fftshift(fftfreq(GG.shape[1], d=T))\n",
    "    freq_y = fftshift(fftfreq(GG.shape[0], d=T))\n",
    "\n",
    "    freq_x_pos = freq_x[half_N:]\n",
    "    freq_y_pos = freq_y[half_N:]\n",
    "\n",
    "    # Identify the dominant frequency\n",
    "    magnitude_spectrum = np.abs(G_pos)\n",
    "    max_idx = np.unravel_index(np.argmax(magnitude_spectrum), magnitude_spectrum.shape)\n",
    "    dominant_freq_x = freq_x_pos[max_idx[1]]\n",
    "    dominant_freq_y = freq_y_pos[max_idx[0]]\n",
    "    magnitude = magnitude_spectrum[max_idx] / (N * N) # normalize magnitude\n",
    "    dominant_freq = max(dominant_freq_x, dominant_freq_y)\n",
    "\n",
    "    print(f\"Sample rate: {sample_rate} Hz\")\n",
    "    print(f\"Dominant Frequency: {dominant_freq} Hz\")\n",
    "    print(f\"Magnitude: {magnitude}\")\n",
    "    kappa_f =  2 * np.pi * dominant_freq\n",
    "    return kappa_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for each dimension size over 4 stages of MSNN\n",
    "# for dim in range(1, 3):\n",
    "dim = 1\n",
    "N_train = 1501\n",
    "N_eval = 8000\n",
    "N_train2 = 21\n",
    "N_eval2 = 200\n",
    "t_train = create_ds(dim, -1.02, 1.02, N_train)\n",
    "x_train = tf.reshape(nd_fun(t_train), [len(t_train)], 1)\n",
    "t_eval = create_ds(dim, -1.02, 1.02, N_eval)\n",
    "x_eval = tf.reshape(nd_fun(t_eval), [len(t_eval)], 1)\n",
    "\n",
    "lt = [reduce_min(t_train[:, i]) for i in range(t_train.shape[-1])]\n",
    "ut = [reduce_max(t_train[:, i]) for i in range(t_train.shape[-1])]\n",
    "lt = tf.convert_to_tensor(lt)\n",
    "ut = tf.convert_to_tensor(ut)\n",
    "\n",
    "# stage 1\n",
    "layers = [dim, 30, 30, 30, 1]\n",
    "kappa1 = 1\n",
    "model1 = NeuralNet(t_train, x_train, layers, kappa1, lt, ut, acts=0)\n",
    "model1.train(3000, 1)     # mode 1 use Adam\n",
    "model1.train(10000, 2)    # mode 2 use L-bfgs\n",
    "x_train2 = x_train - model1.predict(t_train)\n",
    "x_pred1 = model1.predict(t_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa2 = fft_analysis(x_train2, N_train, dim=1)\n",
    "print(kappa2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING 3D POISSON DATA GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, cos, sin, exp, lambdify\n",
    "import numpy as np\n",
    "from shenfun.tensorproductspace import TensorProductSpace\n",
    "from shenfun import inner, div, grad, TestFunction, TrialFunction, Function, \\\n",
    "    project, Dx, FunctionSpace, comm, Array, chebyshev, dx, la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Size of discretization\u001b[39;00m\n\u001b[1;32m      6\u001b[0m N \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m16\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m SD \u001b[38;5;241m=\u001b[39m \u001b[43mFunctionSpace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/shenfun/forms/arguments.py:141\u001b[0m, in \u001b[0;36mFunctionSpace\u001b[0;34m(N, family, bc, dtype, quad, domain, scaled, padding_factor, basis, dealias_direct, coordinates, **kw)\u001b[0m\n\u001b[1;32m    139\u001b[0m         B \u001b[38;5;241m=\u001b[39m fourier\u001b[38;5;241m.\u001b[39mbases\u001b[38;5;241m.\u001b[39mR2C\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m par[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mB\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m family\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchebyshev\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshenfun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chebyshev\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/shenfun/fourier/bases.py:332\u001b[0m, in \u001b[0;36mR2C.__init__\u001b[0;34m(self, N, padding_factor, domain, dealias_direct, coordinates, **kw)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sn \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sm \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpadding_factor\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/shenfun/fourier/bases.py:250\u001b[0m, in \u001b[0;36mFourierBase.plan\u001b[0;34m(self, shape, axis, dtype, options)\u001b[0m\n\u001b[1;32m    246\u001b[0m flags \u001b[38;5;241m=\u001b[39m (fftw\u001b[38;5;241m.\u001b[39mflag_dict[opts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplanner_effort\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[1;32m    247\u001b[0m          fftw\u001b[38;5;241m.\u001b[39mflag_dict[opts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite_input\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m    249\u001b[0m U \u001b[38;5;241m=\u001b[39m fftw\u001b[38;5;241m.\u001b[39maligned(shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 250\u001b[0m xfftn_fwd \u001b[38;5;241m=\u001b[39m \u001b[43mplan_fwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m V \u001b[38;5;241m=\u001b[39m xfftn_fwd\u001b[38;5;241m.\u001b[39moutput_array\n\u001b[1;32m    253\u001b[0m opts \u001b[38;5;241m=\u001b[39m plan_bck\u001b[38;5;241m.\u001b[39mopts\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/mpi4py_fft/fftw/xfftn.py:239\u001b[0m, in \u001b[0;36mrfftn\u001b[0;34m(input_array, s, axes, threads, flags, output_array)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m input_array\u001b[38;5;241m.\u001b[39mshape[axes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m output_array\u001b[38;5;241m.\u001b[39mshape[axes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m    238\u001b[0m M \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mprod(np\u001b[38;5;241m.\u001b[39mtake(input_array\u001b[38;5;241m.\u001b[39mshape, axes))\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_planned_FFT\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/mpi4py_fft/fftw/factory.py:104\u001b[0m, in \u001b[0;36mget_planned_FFT\u001b[0;34m(input_array, output_array, axes, kind, threads, flags, normalization)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return instance of transform class\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m dtype \u001b[38;5;241m=\u001b[39m input_array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mchar\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;129;01min\u001b[39;00m fftlib\n\u001b[1;32m    105\u001b[0m _fft \u001b[38;5;241m=\u001b[39m fftlib[dtype\u001b[38;5;241m.\u001b[39mupper()]\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _fft\u001b[38;5;241m.\u001b[39mFFT(input_array, output_array, axes, kind, threads, flags,\n\u001b[1;32m    107\u001b[0m                 normalization)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "x, y, z = symbols(\"x,y,z\")\n",
    "ue = (cos(4*x) + sin(2*y) + sin(4*z))*(1-x**2)\n",
    "fe = ue.diff(x, 2) + ue.diff(y, 2) + ue.diff(z, 2)\n",
    "\n",
    "# Size of discretization\n",
    "N = [14, 15, 16]\n",
    "SD = FunctionSpace(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
