{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "from scipy.io import savemat\n",
    "from scipy.fft import fftfreq, fftshift, fftn\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    # Initialize the class\n",
    "    def __init__(self, t_u, x_u, layers, kappa, lt, ut, acts=0):\n",
    "\n",
    "        self.scale = tf.reduce_max(tf.abs(x_u)) / 2\n",
    "        x_u2 = x_u / self.scale\n",
    "        actv = [tf.tanh, tf.sin]\n",
    "\n",
    "        self.t_u = t_u\n",
    "        self.x_u = x_u2\n",
    "        self.datatype = t_u.dtype\n",
    "\n",
    "        self.lt = lt\n",
    "        self.ut = ut\n",
    "\n",
    "        self.layers = layers\n",
    "        self.kappa = kappa\n",
    "\n",
    "        # determine the activation function to use\n",
    "        self.actv = actv[acts]\n",
    "\n",
    "        # Initialize NNs\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # Create a list including all training variables\n",
    "        self.train_variables = self.weights + self.biases\n",
    "        # Key point: anything updates in train_variables will be\n",
    "        #            automatically updated in the original tf.Variable\n",
    "\n",
    "        # define the loss function\n",
    "        self.loss0 = self.scale ** 2\n",
    "        self.loss = []\n",
    "        self.loss_0 = self.loss_NN()\n",
    "\n",
    "        self.optimizer_Adam = tf.optimizers.Adam()\n",
    "\n",
    "    '''\n",
    "    Functions used to establish the initial neural network\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.MPL_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=self.datatype))\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    def MPL_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev, dtype=self.datatype))\n",
    "\n",
    "    def get_params(self):\n",
    "        return (self.weights, self.biases)\n",
    "\n",
    "    '''\n",
    "    Functions used to building the physics-informed contrainst and loss\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def neural_net(self, X):\n",
    "        weights = self.weights\n",
    "        biases = self.biases\n",
    "\n",
    "        num_layers = len(weights) + 1\n",
    "\n",
    "        H = 2.0 * tf.math.divide(\n",
    "                    tf.math.subtract(X, tf.transpose(self.lt)), \n",
    "                    tf.transpose(tf.math.subtract(self.ut, self.lt))) \\\n",
    "            - 1.0\n",
    "\n",
    "        W = weights[0]\n",
    "        b = biases[0]\n",
    "        H = self.actv(tf.add(self.kappa * tf.matmul(H, W), b))\n",
    "\n",
    "        for l in range(1, num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    # calculate the physics-informed loss function\n",
    "    def loss_NN(self):\n",
    "        self.x_pred = self.neural_net(self.t_u)\n",
    "        loss = tf.reduce_mean(tf.square(self.x_u - self.x_pred))\n",
    "        return loss\n",
    "\n",
    "    '''\n",
    "    Functions used to define ADAM optimizers\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    # define the function to apply the ADAM optimizer\n",
    "    def adam_function(self):\n",
    "        @tf.function(reduce_retracing=True)\n",
    "        def f1():\n",
    "            # calculate the loss\n",
    "            loss_norm = self.loss_NN()\n",
    "            loss_value = loss_norm * self.loss0\n",
    "            # store loss value so we can retrieve later\n",
    "            tf.py_function(f1.loss.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "            # print out iteration & loss\n",
    "            f1.iter.assign_add(1)\n",
    "\n",
    "            str_iter = tf.strings.as_string([f1.iter])\n",
    "            str_loss = tf.strings.as_string([loss_value], precision=4, scientific=True)\n",
    "\n",
    "            str_print = tf.strings.join([\"Mode: Adam\", \"Iter: \", str_iter[0],\n",
    "                                         \", loss: \", str_loss[0]])\n",
    "            tf.cond(\n",
    "                f1.iter % 10 == 0,\n",
    "                lambda: tf.print(str_print),\n",
    "                lambda: tf.constant(True)  # return arbitrary for non-printing case\n",
    "            )\n",
    "            return loss_norm\n",
    "\n",
    "        f1.iter = tf.Variable(0)\n",
    "        f1.term = []\n",
    "        f1.loss = []\n",
    "        return f1\n",
    "\n",
    "    def Adam_optimizer(self, nIter):\n",
    "        varlist = self.train_variables\n",
    "        func_adam = self.adam_function()\n",
    "        for it in range(nIter):\n",
    "            tf.keras.optimizers.Adam(func_adam, varlist)\n",
    "            #self.optimizer_Adam.minimize(func_adam, varlist)\n",
    "        return func_adam\n",
    "\n",
    "    '''\n",
    "    Functions used to define L-BFGS optimizers\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    # A factory to create a function required by tfp.optimizer.lbfgs_minimize.\n",
    "    def Lbfgs_function(self, varlist):\n",
    "        # obtain the shapes of all trainable parameters in the model\n",
    "        shapes = tf.shape_n(varlist)\n",
    "        n_tensors = len(shapes)\n",
    "\n",
    "        # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n",
    "        # prepare required information first\n",
    "        count = 0\n",
    "        idx = []  # stitch indices\n",
    "        part = []  # partition indices\n",
    "\n",
    "        for i, shape in enumerate(shapes):\n",
    "            n = np.prod(shape)\n",
    "            idx.append(tf.reshape(tf.range(count, count + n, dtype=tf.int32), shape))\n",
    "            part.extend([i] * n)\n",
    "            count += n\n",
    "\n",
    "        part = tf.constant(part)\n",
    "\n",
    "        def assign_new_model_parameters(params_1d):\n",
    "            # A function updating the model's parameters with a 1D tf.Tensor.\n",
    "            # Sub-function under function of class not need to input self\n",
    "\n",
    "            params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "            for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "                varlist[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "        @tf.function(reduce_retracing=True)\n",
    "        def f2(params_1d):\n",
    "            # A function that can be used by tfp.optimizer.lbfgs_minimize.\n",
    "            # This function is created by function_factory.\n",
    "            # Sub-function under function of class not need to input self\n",
    "\n",
    "            # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters\n",
    "            with tf.GradientTape() as tape:\n",
    "                # update the parameters in the model\n",
    "                # this step is critical for self-defined function for L-BFGS\n",
    "                assign_new_model_parameters(params_1d)\n",
    "                # calculate the loss\n",
    "                loss_norm = self.loss_NN()\n",
    "                loss_value = loss_norm * self.loss0\n",
    "\n",
    "            # calculate gradients and convert to 1D tf.Tensor\n",
    "            grads = tape.gradient(loss_norm, varlist)\n",
    "            grads = tf.dynamic_stitch(idx, grads)\n",
    "\n",
    "            # store loss value so we can retrieve later\n",
    "            tf.py_function(f2.loss.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "            # print out iteration & loss\n",
    "            f2.iter.assign_add(1)\n",
    "\n",
    "            str_iter = tf.strings.as_string([f2.iter])\n",
    "            str_loss = tf.strings.as_string([loss_value], precision=4, scientific=True)\n",
    "\n",
    "            str_print = tf.strings.join([\"Mode: LBFGS\", \"Iter: \", str_iter[0],\n",
    "                                         \", loss: \", str_loss[0]])\n",
    "            tf.cond(\n",
    "                f2.iter % 3000 == 0,\n",
    "                lambda: tf.print(str_print),\n",
    "                lambda: tf.constant(True)  # return arbitrary for non-printing case\n",
    "            )\n",
    "\n",
    "            return loss_value, grads\n",
    "\n",
    "        # store these information as members so we can use them outside the scope\n",
    "        f2.iter = tf.Variable(0)\n",
    "        f2.idx = idx\n",
    "        f2.part = part\n",
    "        f2.shapes = shapes\n",
    "        f2.assign_new_model_parameters = assign_new_model_parameters\n",
    "        f2.loss = []\n",
    "\n",
    "        return f2\n",
    "\n",
    "    # define the function to apply the L-BFGS optimizer\n",
    "    def Lbfgs_optimizer(self, nIter, varlist):\n",
    "\n",
    "        func = self.Lbfgs_function(varlist)\n",
    "\n",
    "        # convert initial model parameters to a 1D tf.Tensor\n",
    "        init_params = tf.dynamic_stitch(func.idx, varlist)\n",
    "\n",
    "        max_nIter = tf.cast(nIter / 3, dtype=tf.int32)\n",
    "\n",
    "        # train the model with L-BFGS solver\n",
    "        results = tfp.optimizer.lbfgs_minimize(\n",
    "            value_and_gradients_function=func, initial_position=init_params,\n",
    "            tolerance=1e-11, max_iterations=max_nIter)\n",
    "\n",
    "        # after training, the final optimized parameters are still in results.position\n",
    "        # so we have to manually put them back to the model\n",
    "        func.assign_new_model_parameters(results.position)\n",
    "\n",
    "        return func\n",
    "\n",
    "    '''\n",
    "    Function used for training the model\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def train(self, nIter, idxOpt):\n",
    "        if idxOpt == 1:\n",
    "            # mode 1: running the Adam optimization\n",
    "            func_adam = self.Adam_optimizer(nIter)\n",
    "            self.loss += func_adam.loss\n",
    "        elif idxOpt == 2:\n",
    "            # mode 2: running the Lbfgs optimization\n",
    "            func_bfgs = self.Lbfgs_optimizer(nIter, self.train_variables)\n",
    "            self.loss += func_bfgs.loss\n",
    "\n",
    "    # @tf.function\n",
    "    def predict(self, t):\n",
    "        x_p = self.neural_net(t) * self.scale\n",
    "        return x_p\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultistageNeuralNetwork:\n",
    "    '''MultistageNeuralNetwork is the multi-stage model used for predicting\n",
    "    high-dimensional function outputs through regression. The network takes as parameters the input dataset,\n",
    "    an n-dimensional square tensor of grid points, a list of labels corresponding to function outputs at each grid point,\n",
    "    a number of stages for the MSNN, a number of hidden layers for each stage, and a number of hidden nodes in each\n",
    "    hidden layer of each stage of the MSNN'''\n",
    "    def __init__(self, data, labels, num_stages, num_hidden_layers, num_hidden_nodes):\n",
    "        # metadata for MSNN setup\n",
    "        self.dim = data.shape[-1]                                 # number of dimensions\n",
    "        self.N = int(round(data.shape[0] ** (1/self.dim)))        # number of points per dimension\n",
    "        self.data = data                                          # copy dataset into class variable\n",
    "        self.labels = labels                                           # copy labels into class variable\n",
    "        self.stages = [None] * num_stages                               # Number of stages for MSNN\n",
    "        self.layers = [self.dim] + ([num_hidden_nodes] * num_hidden_layers) + [1] # NN architecture for each stage \n",
    "        self.lt = [tf.math.reduce_min(self.data[:, i]) for i in range(self.data.shape[-1])] # min of each dim\n",
    "        self.ut = [tf.math.reduce_max(self.data[:, i]) for i in range(self.data.shape[-1])] # max of each dim\n",
    "    \n",
    "    @staticmethod\n",
    "    def sample(data, labels, step):\n",
    "        '''sample returns a subset of evenly-spaced data points from dataset/labels by converting\n",
    "        both into a grid format and taking slices of the grid at specified intervals'''\n",
    "        # print(f\"Data size: {self.data.shape}, Label size: {self.labels.shape}\")\n",
    "        dim = data.shape[-1]\n",
    "        N = int(round(data.shape[0] ** (1/dim)))  \n",
    "        reshaped_grid = tf.reshape(data, [N] * dim + [dim])\n",
    "        reshaped_labels = tf.reshape(labels, [N] * dim)\n",
    "\n",
    "        slices = tuple(slice(0, N, step) for _ in range(dim))\n",
    "\n",
    "        sampled_grid = reshaped_grid[slices + (slice(None),)]\n",
    "        sampled_labels = reshaped_labels[slices]\n",
    "\n",
    "        sampled_points = tf.reshape(sampled_grid, (-1, dim))\n",
    "        sampled_labels = tf.reshape(sampled_labels, (-1, 1))\n",
    "        # print(f\"Sampled data size: {sampled_points.shape}, Sampled labels size: {sampled_labels.shape}\")\n",
    "        return sampled_points, sampled_labels\n",
    "    \n",
    "\n",
    "    def train(self, data, labels, stage, kappa):\n",
    "        act = 0 if stage == 0 else 1\n",
    "        self.stages[stage] = NeuralNet(data, labels, self.layers, kappa=kappa, lt=self.lt, ut=self.ut, acts=act)\n",
    "        model = self.stages[stage]\n",
    "        # print(f\"Adam Training for stage: {stage}\")\n",
    "        model.train(3000, 1)     # mode 1 use Adam\n",
    "        # print(f\"L-BFGS Training for stage: {stage}\")\n",
    "        model.train(10000, 2)    # mode 2 use L-bfgs\n",
    "\n",
    "    def predict(self, stage, data):\n",
    "        return self.stages[stage].predict(data)\n",
    "    \n",
    "    # TODO: fix for higher dimensions than just 2 by working on reshape function\n",
    "    def fftn_(self, stage, data, labels, pred=None):\n",
    "        N_train = data.shape[0]\n",
    "        print(data.shape, labels.shape)\n",
    "        \n",
    "        if not pred: residue = labels - sum(self.stages[i].predict(data) for i in range(stage))\n",
    "        else: residue = labels - pred\n",
    "        g = residue.numpy()\n",
    "        print(g.shape)\n",
    "\n",
    "        GG = g.reshape([N_train] * self.dim)\n",
    "        G = fftn(GG)\n",
    "        G_shifted = fftshift(G)\n",
    "\n",
    "        N = len(G)\n",
    "        # Total time range\n",
    "        total_time_range = 2  # from -1 to 1\n",
    "\n",
    "        # Calculate the sample rate\n",
    "        sample_rate = N / total_time_range\n",
    "\n",
    "        # # Perform FFT\n",
    "        half_N = N // 2\n",
    "        T = 1.0 / sample_rate\n",
    "        G_pos = G_shifted[half_N:, half_N:]\n",
    "        freq_x = fftshift(fftfreq(GG.shape[1], d=T))\n",
    "        freq_y = fftshift(fftfreq(GG.shape[0], d=T))\n",
    "\n",
    "        freq_x_pos = freq_x[half_N:]\n",
    "        freq_y_pos = freq_y[half_N:]\n",
    "\n",
    "        # Identify the dominant frequency\n",
    "        magnitude_spectrum = np.abs(G_pos)\n",
    "        max_idx = np.unravel_index(np.argmax(magnitude_spectrum), magnitude_spectrum.shape)\n",
    "        dominant_freq_x = freq_x_pos[max_idx[1]]\n",
    "        dominant_freq_y = freq_y_pos[max_idx[0]]\n",
    "        magnitude = magnitude_spectrum[max_idx] / (N * N) # normalize magnitude\n",
    "        dominant_freq = max(dominant_freq_x, dominant_freq_y)\n",
    "        print(f\"Stage {stage} Kappa Calculation\")\n",
    "        print(f\"Sample rate: {sample_rate} Hz\")\n",
    "        print(f\"Dominant Frequency: {dominant_freq} Hz\")\n",
    "        print(f\"Magnitude: {magnitude}\")\n",
    "        kappa_f =  2 * np.pi * dominant_freq\n",
    "        return kappa_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ds(dims, lo, hi, N):\n",
    "    dimensions = [np.linspace(lo, hi, N)] * dims\n",
    "    mesh = np.meshgrid(*dimensions, indexing='xy')\n",
    "    mesh_points = np.stack([m.flatten() for m in mesh], axis=-1)\n",
    "    mesh_tf = tf.cast(mesh_points, dtype=tf.float64)\n",
    "    return mesh_tf\n",
    "\n",
    "def poisson(x_train, x_radius=1):\n",
    "    N_f = x_train.shape[0]\n",
    "    dim = x_train.shape[-1]\n",
    "    coeffs = 1 # used to be random from 1 to dim - 1\n",
    "    const_2 = 1\n",
    "\n",
    "    xf = x_train.numpy() # used to be random points of size (N_f, args.dim) normalized by sqrt(sum(squares))\n",
    "    x = xf\n",
    "\n",
    "    u1 = x_radius**2 - np.sum(x**2, 1, keepdims=True)\n",
    "    du1_dx = -2 * x\n",
    "    d2u1_dx2 = -2\n",
    "\n",
    "    x1, x2 = x[:, :-1], x[:, 1:]\n",
    "    u2 = coeffs * np.sin(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1))\n",
    "    u2 = np.sum(u2, 1, keepdims=True)\n",
    "    du2_dx_part1 = coeffs * np.cos(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * \\\n",
    "            (1 - x2 * np.sin(x1))\n",
    "    du2_dx_part2 = coeffs * np.cos(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * \\\n",
    "            (-const_2 * np.sin(x2) + np.cos(x1))\n",
    "    du2_dx = np.zeros((N_f, dim))\n",
    "    du2_dx[:, :-1] += du2_dx_part1\n",
    "    du2_dx[:, 1:] += du2_dx_part2\n",
    "    d2u2_dx2_part1 = -coeffs * np.sin(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * (1 - x2 * np.sin(x1))**2 + \\\n",
    "            coeffs * np.cos(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * (- x2 * np.cos(x1))\n",
    "    d2u2_dx2_part2 = -coeffs * np.sin(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * (-const_2 * np.sin(x2) + np.cos(x1))**2 + \\\n",
    "            coeffs * np.cos(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * (-const_2 * np.cos(x2))\n",
    "    d2u2_dx2 = np.zeros((N_f, dim))\n",
    "    d2u2_dx2[:, :-1] += d2u2_dx2_part1\n",
    "    d2u2_dx2[:, 1:] += d2u2_dx2_part2\n",
    "    ff = u1 * d2u2_dx2 + 2 * du1_dx * du2_dx + u2 * d2u1_dx2\n",
    "    ff = np.sum(ff, 1)\n",
    "    u = (u1 * u2).reshape(-1)\n",
    "    ff = ff + np.sin(u)\n",
    "\n",
    "    ff = tf.convert_to_tensor(ff, dtype=tf.float64)  \n",
    "    return ff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(441, 2) (441, 1)\n",
      "tf.Tensor(\n",
      "[[ 5.61620733]\n",
      " [ 5.92261875]\n",
      " [ 6.29388888]\n",
      " [ 6.71912699]\n",
      " [ 7.17882071]\n",
      " [ 7.64394555]\n",
      " [ 8.07588068]\n",
      " [ 8.42745838]\n",
      " [ 8.64549537]\n",
      " [ 8.67510186]\n",
      " [ 8.46590101]\n",
      " [ 7.97999314]\n",
      " [ 7.20105382]\n",
      " [ 6.14339028]\n",
      " [ 4.85920302]\n",
      " [ 3.4419456 ]\n",
      " [ 2.02388749]\n",
      " [ 0.76697648]\n",
      " [-0.15248281]\n",
      " [-0.56416452]\n",
      " [-0.32917791]\n",
      " [ 6.18523653]\n",
      " [ 6.47675831]\n",
      " [ 6.82487635]\n",
      " [ 7.20894205]\n",
      " [ 7.59975139]\n",
      " [ 7.96004004]\n",
      " [ 8.24582743]\n",
      " [ 8.40880923]\n",
      " [ 8.39996082]\n",
      " [ 8.17441594]\n",
      " [ 7.69751124]\n",
      " [ 6.95163073]\n",
      " [ 5.94314477]\n",
      " [ 4.70834507]\n",
      " [ 3.31693088]\n",
      " [ 1.87151143]\n",
      " [ 0.50201625]\n",
      " [-0.64506609]\n",
      " [-1.42148691]\n",
      " [-1.6945386 ]\n",
      " [-1.36650941]\n",
      " [ 6.40787881]\n",
      " [ 6.63824629]\n",
      " [ 6.91088873]\n",
      " [ 7.19765701]\n",
      " [ 7.46308522]\n",
      " [ 7.66609602]\n",
      " [ 7.76244491]\n",
      " [ 7.7079682 ]\n",
      " [ 7.46263362]\n",
      " [ 6.99528968]\n",
      " [ 6.2888659 ]\n",
      " [ 5.3455858 ]\n",
      " [ 4.19151652]\n",
      " [ 2.87952289]\n",
      " [ 1.4895286 ]\n",
      " [ 0.12510251]\n",
      " [-1.09403418]\n",
      " [-2.04299165]\n",
      " [-2.60466511]\n",
      " [-2.68309108]\n",
      " [-2.21694005]\n",
      " [ 6.35442938]\n",
      " [ 6.48228293]\n",
      " [ 6.63419441]\n",
      " [ 6.77765785]\n",
      " [ 6.87501468]\n",
      " [ 6.88603139]\n",
      " [ 6.77097304]\n",
      " [ 6.49413381]\n",
      " [ 6.02772027]\n",
      " [ 5.35590224]\n",
      " [ 4.4787441 ]\n",
      " [ 3.41559541]\n",
      " [ 2.20735179]\n",
      " [ 0.91684626]\n",
      " [-0.37338123]\n",
      " [-1.56639919]\n",
      " [-2.55908587]\n",
      " [-3.25215322]\n",
      " [-3.5603534 ]\n",
      " [-3.42156925]\n",
      " [-2.80492261]\n",
      " [ 6.10413223]\n",
      " [ 6.09621656]\n",
      " [ 6.09318698]\n",
      " [ 6.06171705]\n",
      " [ 5.96597589]\n",
      " [ 5.77065629]\n",
      " [ 5.44418206]\n",
      " [ 4.96200687]\n",
      " [ 4.30986812]\n",
      " [ 3.48680499]\n",
      " [ 2.50767366]\n",
      " [ 1.40478549]\n",
      " [ 0.22817577]\n",
      " [-0.95603869]\n",
      " [-2.06961274]\n",
      " [-3.02841413]\n",
      " [-3.75027708]\n",
      " [-4.16316085]\n",
      " [-4.21212471]\n",
      " [-3.86430867]\n",
      " [-3.11258199]\n",
      " [ 5.73557945]\n",
      " [ 5.56866896]\n",
      " [ 5.38945271]\n",
      " [ 5.16718643]\n",
      " [ 4.87125589]\n",
      " [ 4.47424339]\n",
      " [ 3.95485824]\n",
      " [ 3.3006508 ]\n",
      " [ 2.51039591]\n",
      " [ 1.59598247]\n",
      " [ 0.58356917]\n",
      " [-0.48632714]\n",
      " [-1.56019764]\n",
      " [-2.57467121]\n",
      " [-3.46143344]\n",
      " [-4.15348741]\n",
      " [-4.591784  ]\n",
      " [-4.73092651]\n",
      " [-4.54283625]\n",
      " [-4.01816015]\n",
      " [-3.16657705]\n",
      " [ 5.32017766]\n",
      " [ 4.98114004]\n",
      " [ 4.61686548]\n",
      " [ 4.20209065]\n",
      " [ 3.71386455]\n",
      " [ 3.13432738]\n",
      " [ 2.45310594]\n",
      " [ 1.66928895]\n",
      " [ 0.7929021 ]\n",
      " [-0.15426677]\n",
      " [-1.13872598]\n",
      " [-2.11650776]\n",
      " [-3.03579728]\n",
      " [-3.84098297]\n",
      " [-4.47779027]\n",
      " [-4.89875598]\n",
      " [-5.06800176]\n",
      " [-4.9642519 ]\n",
      " [-4.58148588]\n",
      " [-3.92757513]\n",
      " [-3.02240982]\n",
      " [ 4.91805278]\n",
      " [ 4.40222851]\n",
      " [ 3.85380996]\n",
      " [ 3.25527111]\n",
      " [ 2.59281073]\n",
      " [ 1.85864354]\n",
      " [ 1.05276985]\n",
      " [ 0.18422717]\n",
      " [-0.72825495]\n",
      " [-1.65636788]\n",
      " [-2.56333293]\n",
      " [-3.40618973]\n",
      " [-4.13926559]\n",
      " [-4.71853236]\n",
      " [-5.10623778]\n",
      " [-5.27496462]\n",
      " [-5.21022325]\n",
      " [-4.91090854]\n",
      " [-4.387528  ]\n",
      " [-3.65899631]\n",
      " [-2.7496528 ]\n",
      " [ 4.57624538]\n",
      " [ 3.88484371]\n",
      " [ 3.15939595]\n",
      " [ 2.3917992 ]\n",
      " [ 1.57812759]\n",
      " [ 0.72036055]\n",
      " [-0.17243112]\n",
      " [-1.08344245]\n",
      " [-1.98816334]\n",
      " [-2.85528818]\n",
      " [-3.64868101]\n",
      " [-4.33039627]\n",
      " [-4.86448072]\n",
      " [-5.22100582]\n",
      " [-5.37960008]\n",
      " [-5.331726  ]\n",
      " [-5.08109991]\n",
      " [-4.64201138]\n",
      " [-4.03588303]\n",
      " [-3.2871362 ]\n",
      " [-2.41997203]\n",
      " [ 4.32896513]\n",
      " [ 3.46629378]\n",
      " [ 2.57352825]\n",
      " [ 1.65331198]\n",
      " [ 0.71186204]\n",
      " [-0.23985109]\n",
      " [-1.18538457]\n",
      " [-2.10263439]\n",
      " [-2.96439404]\n",
      " [-3.73988081]\n",
      " [-4.39727792]\n",
      " [-4.90705272]\n",
      " [-5.24553082]\n",
      " [-5.398046  ]\n",
      " [-5.36098942]\n",
      " [-5.14223243]\n",
      " [-4.7596671 ]\n",
      " [-4.23799782]\n",
      " [-3.60442222]\n",
      " [-2.8843527 ]\n",
      " [-2.09858111]\n",
      " [ 4.19934931]\n",
      " [ 3.17052837]\n",
      " [ 2.1198087 ]\n",
      " [ 1.0618012 ]\n",
      " [ 0.01298518]\n",
      " [-1.00760746]\n",
      " [-1.97799759]\n",
      " [-2.87335067]\n",
      " [-3.66686928]\n",
      " [-4.33166903]\n",
      " [-4.84350407]\n",
      " [-5.18387242]\n",
      " [-5.34283524]\n",
      " [-5.32088981]\n",
      " [-5.12940349]\n",
      " [-4.78937068]\n",
      " [-4.32855614]\n",
      " [-3.77743291]\n",
      " [-3.16469603]\n",
      " [-2.51342754]\n",
      " [-1.8390069 ]\n",
      " [ 4.20194734]\n",
      " [ 3.01149344]\n",
      " [ 1.80988301]\n",
      " [ 0.62506389]\n",
      " [-0.51598616]\n",
      " [-1.58702453]\n",
      " [-2.56232626]\n",
      " [-3.41684876]\n",
      " [-4.12718171]\n",
      " [-4.67339245]\n",
      " [-5.04145162]\n",
      " [-5.22561468]\n",
      " [-5.2300761 ]\n",
      " [-5.06937729]\n",
      " [-4.76732143]\n",
      " [-4.35443357]\n",
      " [-3.86427096]\n",
      " [-3.32914287]\n",
      " [-2.77601864]\n",
      " [-2.22350494]\n",
      " [-1.68064118]\n",
      " [ 4.34515393]\n",
      " [ 2.99658744]\n",
      " [ 1.64796863]\n",
      " [ 0.34230836]\n",
      " [-0.88234959]\n",
      " [-1.99313402]\n",
      " [-2.9621561 ]\n",
      " [-3.76648105]\n",
      " [-4.38875757]\n",
      " [-4.81855538]\n",
      " [-5.05397702]\n",
      " [-5.10287562]\n",
      " [-4.9830933 ]\n",
      " [-4.72140968]\n",
      " [-4.35120021]\n",
      " [-3.90905876]\n",
      " [-3.4308303 ]\n",
      " [-2.94763827]\n",
      " [-2.48256652]\n",
      " [-2.04861612]\n",
      " [-1.64835926]\n",
      " [ 4.63299163]\n",
      " [ 3.12946432]\n",
      " [ 1.63467523]\n",
      " [ 0.20893601]\n",
      " [-1.09756616]\n",
      " [-2.24546627]\n",
      " [-3.20590665]\n",
      " [-3.95998482]\n",
      " [-4.49859439]\n",
      " [-4.82272288]\n",
      " [-4.94376157]\n",
      " [-4.88323061]\n",
      " [-4.67150675]\n",
      " [-4.34546589]\n",
      " [-3.94524039]\n",
      " [-3.51047603]\n",
      " [-3.07656631]\n",
      " [-2.67136632]\n",
      " [-2.31284885]\n",
      " [-2.00804862]\n",
      " [-1.75344682]\n",
      " [ 5.06590169]\n",
      " [ 3.41175453]\n",
      " [ 1.76963896]\n",
      " [ 0.22001422]\n",
      " [-1.17304994]\n",
      " [-2.36319873]\n",
      " [-3.32121637]\n",
      " [-4.03357503]\n",
      " [-4.50096438]\n",
      " [-4.73700776]\n",
      " [-4.76684301]\n",
      " [-4.62513866]\n",
      " [-4.35333956]\n",
      " [-3.99624605]\n",
      " [-3.59825234]\n",
      " [-3.19966148]\n",
      " [-2.83348563]\n",
      " [-2.52307063]\n",
      " [-2.28077596]\n",
      " [-2.10780753]\n",
      " [-1.99516017]\n",
      " [ 5.64046176]\n",
      " [ 3.84356952]\n",
      " [ 2.05282234]\n",
      " [ 0.37231137]\n",
      " [-1.11749135]\n",
      " [-2.36193444]\n",
      " [-3.33126893]\n",
      " [-4.01792463]\n",
      " [-4.43319684]\n",
      " [-4.60386351]\n",
      " [-4.56867826]\n",
      " [-4.37455187]\n",
      " [-4.07242078]\n",
      " [-3.71303792]\n",
      " [-3.34304654]\n",
      " [-3.00168892]\n",
      " [-2.71840689]\n",
      " [-2.51146296]\n",
      " [-2.38758419]\n",
      " [-2.34252912]\n",
      " [-2.362413  ]\n",
      " [ 6.34817114]\n",
      " [ 4.42288279]\n",
      " [ 2.48455838]\n",
      " [ 0.66498511]\n",
      " [-0.93560061]\n",
      " [-2.25198096]\n",
      " [-3.25272755]\n",
      " [-3.9358967 ]\n",
      " [-4.32338359]\n",
      " [-4.45496958]\n",
      " [-4.38242667]\n",
      " [-4.16379317]\n",
      " [-3.85799715]\n",
      " [-3.52011657]\n",
      " [-3.19757491]\n",
      " [-2.92747095]\n",
      " [-2.73508965]\n",
      " [-2.63349722]\n",
      " [-2.62402336]\n",
      " [-2.69739277]\n",
      " [-2.83528242]\n",
      " [ 7.1736176 ]\n",
      " [ 5.14404519]\n",
      " [ 3.06456301]\n",
      " [ 1.09914001]\n",
      " [-0.62804574]\n",
      " [-2.03788634]\n",
      " [-3.09500928]\n",
      " [-3.80172268]\n",
      " [-4.18964233]\n",
      " [-4.31076211]\n",
      " [-4.22891017]\n",
      " [-4.01206255]\n",
      " [-3.72583471]\n",
      " [-3.42840971]\n",
      " [-3.16704824]\n",
      " [-2.97615472]\n",
      " [-2.8767036 ]\n",
      " [-2.87671272]\n",
      " [-2.97241029]\n",
      " [-3.14977486]\n",
      " [-3.38621303]\n",
      " [ 8.09246004]\n",
      " [ 5.99580847]\n",
      " [ 3.79023393]\n",
      " [ 1.67653958]\n",
      " [-0.19230597]\n",
      " [-1.718934  ]\n",
      " [-2.8605616 ]\n",
      " [-3.62123121]\n",
      " [-4.04047082]\n",
      " [-4.18106592]\n",
      " [-4.1176343 ]\n",
      " [-3.92690087]\n",
      " [-3.68011095]\n",
      " [-3.43773549]\n",
      " [-3.24638125]\n",
      " [-3.13760472]\n",
      " [-3.12817953]\n",
      " [-3.22131218]\n",
      " [-3.40834044]\n",
      " [-3.67055693]\n",
      " [-3.98094447]\n",
      " [ 9.0697204 ]\n",
      " [ 6.95930706]\n",
      " [ 4.65461829]\n",
      " [ 2.39779835]\n",
      " [ 0.37585798]\n",
      " [-1.29029139]\n",
      " [-2.54579943]\n",
      " [-3.39272976]\n",
      " [-3.87573127]\n",
      " [-4.06630904]\n",
      " [-4.04830861]\n",
      " [-3.90602945]\n",
      " [-3.71552404]\n",
      " [-3.53907873]\n",
      " [-3.42250029]\n",
      " [-3.3946003 ]\n",
      " [-3.46817557]\n",
      " [-3.64181611]\n",
      " [-3.90200164]\n",
      " [-4.22512836]\n",
      " [-4.57929882]\n",
      " [10.05885612]\n",
      " [ 8.00647768]\n",
      " [ 5.6444733 ]\n",
      " [ 3.2604076 ]\n",
      " [ 1.08061713]\n",
      " [-0.7445316 ]\n",
      " [-2.14240876]\n",
      " [-3.10821012]\n",
      " [-3.68789398]\n",
      " [-3.95891323]\n",
      " [-4.01244024]\n",
      " [-3.93914205]\n",
      " [-3.81922336]\n",
      " [-3.71656181]\n",
      " [-3.67624204]\n",
      " [-3.7245607 ]\n",
      " [-3.87055908]\n",
      " [-4.10827806]\n",
      " [-4.41915482]\n",
      " [-4.77423021]\n",
      " [-5.13606809]], shape=(441, 1), dtype=float64)\n",
      "(441, 2) (441, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train for each dimension size over 4 stages of MSNN\n",
    "# for dim in range(1, 3):\n",
    "dim = 2\n",
    "N_train = 21\n",
    "N_eval = 200\n",
    "x_train = create_ds(dim, -1.02, 1.02, N_train)\n",
    "y_train = tf.reshape(poisson(x_train), [len(x_train), 1])\n",
    "\n",
    "x_eval = create_ds(dim, -1.02, 1.02, N_eval)\n",
    "y_eval = tf.reshape(poisson(x_eval), [len(x_eval), 1])\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(y_train)\n",
    "\n",
    "x_sampled, y_sampled = MultistageNeuralNetwork.sample(x_train, y_train, 1)\n",
    "print(x_sampled.shape, y_sampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: LBFGSIter: 3000, loss: 8.2789e-04\n",
      "Mode: LBFGSIter: 6000, loss: 2.7923e-04\n",
      "Mode: LBFGSIter: 9000, loss: 1.0931e-04\n"
     ]
    }
   ],
   "source": [
    "MSNN = MultistageNeuralNetwork(data=x_train, labels=y_train, num_stages=4, num_hidden_layers=3, num_hidden_nodes=20)\n",
    "MSNN.train(x_sampled, y_sampled, stage=0, kappa=1)\n",
    "pred = MSNN.predict(0, x_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSNN = MultistageNeuralNetwork(data=x_train, labels=y_train, num_stages=4, num_hidden_layers=3, num_hidden_nodes=20)\n",
    "kappa2 = MSNN.fftn_(stage=0, data=x_sampled, labels=y_sampled, pred=pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
