{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "from pyDOE import lhs\n",
    "from scipy.io import savemat\n",
    "from scipy.fft import fftfreq, fftshift, fftn\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    # Initialize the class\n",
    "    def __init__(self, t_u, x_u, layers, kappa, lt, ut, acts=0):\n",
    "\n",
    "        self.scale = tf.reduce_max(tf.abs(x_u)) / 2\n",
    "        x_u2 = x_u / self.scale\n",
    "        actv = [tf.tanh, tf.sin]\n",
    "\n",
    "        self.t_u = t_u\n",
    "        self.x_u = x_u2\n",
    "        self.datatype = t_u.dtype\n",
    "\n",
    "        self.lt = lt\n",
    "        self.ut = ut\n",
    "\n",
    "        self.layers = layers\n",
    "        self.kappa = kappa\n",
    "\n",
    "        # determine the activation function to use\n",
    "        self.actv = actv[acts]\n",
    "\n",
    "        # Initialize NNs\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # Create a list including all training variables\n",
    "        self.train_variables = self.weights + self.biases\n",
    "        # Key point: anything updates in train_variables will be\n",
    "        #            automatically updated in the original tf.Variable\n",
    "\n",
    "        # define the loss function\n",
    "        self.loss0 = self.scale ** 2\n",
    "        self.loss = []\n",
    "        self.loss_0 = self.loss_NN()\n",
    "\n",
    "        self.optimizer_Adam = tf.optimizers.Adam()\n",
    "\n",
    "    '''\n",
    "    Functions used to establish the initial neural network\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.MPL_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=self.datatype))\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    def MPL_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev, dtype=self.datatype))\n",
    "\n",
    "    def get_params(self):\n",
    "        return (self.weights, self.biases)\n",
    "\n",
    "    '''\n",
    "    Functions used to building the physics-informed contrainst and loss\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def neural_net(self, X):\n",
    "        weights = self.weights\n",
    "        biases = self.biases\n",
    "\n",
    "        num_layers = len(weights) + 1\n",
    "\n",
    "        H = 2.0 * tf.math.divide(\n",
    "                    tf.math.subtract(X, tf.transpose(self.lt)), \n",
    "                    tf.transpose(tf.math.subtract(self.ut, self.lt))) \\\n",
    "            - 1.0\n",
    "\n",
    "        W = weights[0]\n",
    "        b = biases[0]\n",
    "        H = self.actv(tf.add(self.kappa * tf.matmul(H, W), b))\n",
    "\n",
    "        for l in range(1, num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "\n",
    "    @tf.function\n",
    "    # calculate the physics-informed loss function\n",
    "    def loss_NN(self):\n",
    "        self.x_pred = self.neural_net(self.t_u)\n",
    "        loss = tf.reduce_mean(tf.square(self.x_u - self.x_pred))\n",
    "        return loss\n",
    "\n",
    "    '''\n",
    "    Functions used to define ADAM optimizers\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    # define the function to apply the ADAM optimizer\n",
    "    def adam_function(self):\n",
    "        @tf.function(reduce_retracing=True)\n",
    "        def f1():\n",
    "            # calculate the loss\n",
    "            loss_norm = self.loss_NN()\n",
    "            loss_value = loss_norm * self.loss0\n",
    "            # store loss value so we can retrieve later\n",
    "            tf.py_function(f1.loss.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "            # print out iteration & loss\n",
    "            f1.iter.assign_add(1)\n",
    "\n",
    "            str_iter = tf.strings.as_string([f1.iter])\n",
    "            str_loss = tf.strings.as_string([loss_value], precision=4, scientific=True)\n",
    "\n",
    "            str_print = tf.strings.join([\"Mode: Adam\", \"Iter: \", str_iter[0],\n",
    "                                         \", loss: \", str_loss[0]])\n",
    "            tf.cond(\n",
    "                f1.iter % 10 == 0,\n",
    "                lambda: tf.print(str_print),\n",
    "                lambda: tf.constant(True)  # return arbitrary for non-printing case\n",
    "            )\n",
    "            return loss_norm\n",
    "\n",
    "        f1.iter = tf.Variable(0)\n",
    "        f1.term = []\n",
    "        f1.loss = []\n",
    "        return f1\n",
    "\n",
    "    def Adam_optimizer(self, nIter):\n",
    "        varlist = self.train_variables\n",
    "        func_adam = self.adam_function()\n",
    "        for it in range(nIter):\n",
    "            tf.keras.optimizers.Adam(func_adam, varlist)\n",
    "            #self.optimizer_Adam.minimize(func_adam, varlist)\n",
    "        return func_adam\n",
    "\n",
    "    '''\n",
    "    Functions used to define L-BFGS optimizers\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    # A factory to create a function required by tfp.optimizer.lbfgs_minimize.\n",
    "    def Lbfgs_function(self, varlist):\n",
    "        # obtain the shapes of all trainable parameters in the model\n",
    "        shapes = tf.shape_n(varlist)\n",
    "        n_tensors = len(shapes)\n",
    "\n",
    "        # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n",
    "        # prepare required information first\n",
    "        count = 0\n",
    "        idx = []  # stitch indices\n",
    "        part = []  # partition indices\n",
    "\n",
    "        for i, shape in enumerate(shapes):\n",
    "            n = np.prod(shape)\n",
    "            idx.append(tf.reshape(tf.range(count, count + n, dtype=tf.int32), shape))\n",
    "            part.extend([i] * n)\n",
    "            count += n\n",
    "\n",
    "        part = tf.constant(part)\n",
    "\n",
    "        def assign_new_model_parameters(params_1d):\n",
    "            # A function updating the model's parameters with a 1D tf.Tensor.\n",
    "            # Sub-function under function of class not need to input self\n",
    "\n",
    "            params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "            for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "                varlist[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "        @tf.function(reduce_retracing=True)\n",
    "        def f2(params_1d):\n",
    "            # A function that can be used by tfp.optimizer.lbfgs_minimize.\n",
    "            # This function is created by function_factory.\n",
    "            # Sub-function under function of class not need to input self\n",
    "\n",
    "            # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters\n",
    "            with tf.GradientTape() as tape:\n",
    "                # update the parameters in the model\n",
    "                # this step is critical for self-defined function for L-BFGS\n",
    "                assign_new_model_parameters(params_1d)\n",
    "                # calculate the loss\n",
    "                loss_norm = self.loss_NN()\n",
    "                loss_value = loss_norm * self.loss0\n",
    "\n",
    "            # calculate gradients and convert to 1D tf.Tensor\n",
    "            grads = tape.gradient(loss_norm, varlist)\n",
    "            grads = tf.dynamic_stitch(idx, grads)\n",
    "\n",
    "            # store loss value so we can retrieve later\n",
    "            tf.py_function(f2.loss.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "            # print out iteration & loss\n",
    "            f2.iter.assign_add(1)\n",
    "\n",
    "            str_iter = tf.strings.as_string([f2.iter])\n",
    "            str_loss = tf.strings.as_string([loss_value], precision=4, scientific=True)\n",
    "\n",
    "            str_print = tf.strings.join([\"Mode: LBFGS\", \"Iter: \", str_iter[0],\n",
    "                                         \", loss: \", str_loss[0]])\n",
    "            tf.cond(\n",
    "                f2.iter % 3000 == 0,\n",
    "                lambda: tf.print(str_print),\n",
    "                lambda: tf.constant(True)  # return arbitrary for non-printing case\n",
    "            )\n",
    "\n",
    "            return loss_value, grads\n",
    "\n",
    "        # store these information as members so we can use them outside the scope\n",
    "        f2.iter = tf.Variable(0)\n",
    "        f2.idx = idx\n",
    "        f2.part = part\n",
    "        f2.shapes = shapes\n",
    "        f2.assign_new_model_parameters = assign_new_model_parameters\n",
    "        f2.loss = []\n",
    "\n",
    "        return f2\n",
    "\n",
    "    # define the function to apply the L-BFGS optimizer\n",
    "    def Lbfgs_optimizer(self, nIter, varlist):\n",
    "\n",
    "        func = self.Lbfgs_function(varlist)\n",
    "\n",
    "        # convert initial model parameters to a 1D tf.Tensor\n",
    "        init_params = tf.dynamic_stitch(func.idx, varlist)\n",
    "\n",
    "        max_nIter = tf.cast(nIter / 3, dtype=tf.int32)\n",
    "\n",
    "        # train the model with L-BFGS solver\n",
    "        results = tfp.optimizer.lbfgs_minimize(\n",
    "            value_and_gradients_function=func, initial_position=init_params,\n",
    "            tolerance=1e-11, max_iterations=max_nIter)\n",
    "\n",
    "        # after training, the final optimized parameters are still in results.position\n",
    "        # so we have to manually put them back to the model\n",
    "        func.assign_new_model_parameters(results.position)\n",
    "\n",
    "        return func\n",
    "\n",
    "    '''\n",
    "    Function used for training the model\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def train(self, nIter, idxOpt):\n",
    "        if idxOpt == 1:\n",
    "            # mode 1: running the Adam optimization\n",
    "            func_adam = self.Adam_optimizer(nIter)\n",
    "            self.loss += func_adam.loss\n",
    "        elif idxOpt == 2:\n",
    "            # mode 2: running the Lbfgs optimization\n",
    "            func_bfgs = self.Lbfgs_optimizer(nIter, self.train_variables)\n",
    "            self.loss += func_bfgs.loss\n",
    "\n",
    "    # @tf.function\n",
    "    def predict(self, t):\n",
    "        x_p = self.neural_net(t) * self.scale\n",
    "        return x_p\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultistageNeuralNetwork:\n",
    "    '''MultistageNeuralNetwork is the multi-stage model used for predicting\n",
    "    high-dimensional function outputs through regression. The network takes as parameters the input dataset,\n",
    "    an n-dimensional square tensor of grid points, a list of labels corresponding to function outputs at each grid point,\n",
    "    a number of stages for the MSNN, a number of hidden layers for each stage, and a number of hidden nodes in each\n",
    "    hidden layer of each stage of the MSNN'''\n",
    "    def __init__(self, data, labels, num_stages, num_hidden_layers, num_hidden_nodes):\n",
    "        # metadata for MSNN setup\n",
    "        self.dim = data.shape[-1]                                 # number of dimensions\n",
    "        self.N = int(round(data.shape[0] ** (1/self.dim)))        # number of points per dimension\n",
    "        self.data = data                                          # copy dataset into class variable\n",
    "        self.labels = labels                                           # copy labels into class variable\n",
    "        self.stages = [None] * num_stages                               # Number of stages for MSNN\n",
    "        self.layers = [self.dim] + ([num_hidden_nodes] * num_hidden_layers) + [1] # NN architecture for each stage \n",
    "        self.lt = [tf.math.reduce_min(self.data[:, i]) for i in range(self.data.shape[-1])] # min of each dim\n",
    "        self.ut = [tf.math.reduce_max(self.data[:, i]) for i in range(self.data.shape[-1])] # max of each dim\n",
    "    \n",
    "    def sample(self, step):\n",
    "        '''sample returns a subset of evenly-spaced data points from dataset/labels by converting\n",
    "        both into a grid format and taking slices of the grid at specified intervals'''\n",
    "        print(f\"Data size: {self.data.shape}, Label size: {self.labels.shape}\")\n",
    "        reshaped_grid = tf.reshape(self.data, [self.N] * self.dim + [self.dim])\n",
    "        reshaped_labels = tf.reshape(self.labels, [self.N] * self.dim)\n",
    "\n",
    "        slices = tuple(slice(0, self.N, step) for _ in range(self.dim))\n",
    "\n",
    "        sampled_grid = reshaped_grid[slices + (slice(None),)]\n",
    "        sampled_labels = reshaped_labels[slices]\n",
    "\n",
    "        sampled_points = tf.reshape(sampled_grid, (-1, self.dim))\n",
    "        sampled_labels = tf.reshape(sampled_labels, (-1,))\n",
    "        print(f\"Sampled data size: {sampled_points.shape}, Sampled labels size: {sampled_labels.shape}\")\n",
    "        return sampled_points, sampled_labels\n",
    "    \n",
    "\n",
    "    def train(self, data, labels, stage, kappa):\n",
    "        act = 0 if stage == 0 else 1\n",
    "        self.stages[stage] = model = NeuralNet(data, labels, self.layers, kappa=kappa, lt=self.lt, ut=self.ut, acts=act)\n",
    "        print(f\"Adam Training for stage: {stage}\")\n",
    "        model.train(3000, 1)     # mode 1 use Adam\n",
    "        print(f\"L-BFGS Training for stage: {stage}\")\n",
    "        model.train(10000, 2)    # mode 2 use L-bfgs\n",
    "\n",
    "    def predict(self, stage, data):\n",
    "        return self.stages[stage](data)\n",
    "    \n",
    "    # TODO: fix for higher dimensions than just 2 by working on reshape function\n",
    "    def fftn_(self, stage, data, labels, pred=None):\n",
    "        N_train = data.shape[0]\n",
    "        print(data.shape, labels.shape)\n",
    "        \n",
    "        if not pred: residue = labels - sum(self.stages[i].predict(data) for i in range(stage))\n",
    "        else: residue = labels - pred\n",
    "        g = residue.numpy()\n",
    "        print(g.shape)\n",
    "\n",
    "        GG = g.reshape([N_train] * self.dim)\n",
    "        G = fftn(GG)\n",
    "        G_shifted = fftshift(G)\n",
    "\n",
    "        N = len(G)\n",
    "        # Total time range\n",
    "        total_time_range = 2  # from -1 to 1\n",
    "\n",
    "        # Calculate the sample rate\n",
    "        sample_rate = N / total_time_range\n",
    "\n",
    "        # # Perform FFT\n",
    "        half_N = N // 2\n",
    "        T = 1.0 / sample_rate\n",
    "        G_pos = G_shifted[half_N:, half_N:]\n",
    "        freq_x = fftshift(fftfreq(GG.shape[1], d=T))\n",
    "        freq_y = fftshift(fftfreq(GG.shape[0], d=T))\n",
    "\n",
    "        freq_x_pos = freq_x[half_N:]\n",
    "        freq_y_pos = freq_y[half_N:]\n",
    "\n",
    "        # Identify the dominant frequency\n",
    "        magnitude_spectrum = np.abs(G_pos)\n",
    "        max_idx = np.unravel_index(np.argmax(magnitude_spectrum), magnitude_spectrum.shape)\n",
    "        dominant_freq_x = freq_x_pos[max_idx[1]]\n",
    "        dominant_freq_y = freq_y_pos[max_idx[0]]\n",
    "        magnitude = magnitude_spectrum[max_idx] / (N * N) # normalize magnitude\n",
    "        dominant_freq = max(dominant_freq_x, dominant_freq_y)\n",
    "        print(f\"Stage {stage} Kappa Calculation\")\n",
    "        print(f\"Sample rate: {sample_rate} Hz\")\n",
    "        print(f\"Dominant Frequency: {dominant_freq} Hz\")\n",
    "        print(f\"Magnitude: {magnitude}\")\n",
    "        kappa_f =  2 * np.pi * dominant_freq\n",
    "        return kappa_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ds(dims, lo, hi, N):\n",
    "    dimensions = [np.linspace(lo, hi, N)] * dims\n",
    "    mesh = np.meshgrid(*dimensions, indexing='xy')\n",
    "    mesh_points = np.stack([m.flatten() for m in mesh], axis=-1)\n",
    "    mesh_tf = tf.cast(mesh_points, dtype=tf.float64)\n",
    "    return mesh_tf\n",
    "\n",
    "def nd_fun(x):\n",
    "    if x.shape[-1] <= 1: return x\n",
    "    y = [tf.math.reduce_sum(x, axis=-1)]\n",
    "    return y\n",
    "\n",
    "def poisson(x_train, x_radius=1):\n",
    "    N_f = x_train.shape[0]\n",
    "    dim = x_train.shape[-1]\n",
    "    coeffs = np.random.randn(1, dim - 1)\n",
    "    const_2 = 1\n",
    "\n",
    "    xf = x_train.numpy()\n",
    "    # rf = np.random.rand(N_f, 1) * x_radius\n",
    "    # xf = xf / np.linalg.norm(xf, axis=1, keepdims=True) * rf\n",
    "    x = xf\n",
    "\n",
    "    u1 = x_radius**2 - np.sum(x**2, 1, keepdims=True)\n",
    "    du1_dx = -2 * x\n",
    "    d2u1_dx2 = -2\n",
    "\n",
    "    x1, x2 = x[:, :-1], x[:, 1:]\n",
    "    u2 = coeffs * np.sin(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1))\n",
    "    u2 = np.sum(u2, 1, keepdims=True)\n",
    "    du2_dx_part1 = coeffs * np.cos(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * \\\n",
    "            (1 - x2 * np.sin(x1))\n",
    "    du2_dx_part2 = coeffs * np.cos(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * \\\n",
    "            (-const_2 * np.sin(x2) + np.cos(x1))\n",
    "    du2_dx = np.zeros((N_f, dim))\n",
    "    du2_dx[:, :-1] += du2_dx_part1\n",
    "    du2_dx[:, 1:] += du2_dx_part2\n",
    "    d2u2_dx2_part1 = -coeffs * np.sin(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * (1 - x2 * np.sin(x1))**2 + \\\n",
    "            coeffs * np.cos(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * (- x2 * np.cos(x1))\n",
    "    d2u2_dx2_part2 = -coeffs * np.sin(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * (-const_2 * np.sin(x2) + np.cos(x1))**2 + \\\n",
    "            coeffs * np.cos(x1 + const_2 * np.cos(x2) + x2 * np.cos(x1)) * (-const_2 * np.cos(x2))\n",
    "    d2u2_dx2 = np.zeros((N_f, dim))\n",
    "    d2u2_dx2[:, :-1] += d2u2_dx2_part1\n",
    "    d2u2_dx2[:, 1:] += d2u2_dx2_part2\n",
    "    ff = u1 * d2u2_dx2 + 2 * du1_dx * du2_dx + u2 * d2u1_dx2\n",
    "    ff = np.sum(ff, 1)\n",
    "    u = (u1 * u2).reshape(-1)\n",
    "    ff = ff + np.sin(u)\n",
    "\n",
    "    ff = tf.convert_to_tensor(ff, dtype=tf.float64)  \n",
    "    return ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125000, 3) (125000,)\n"
     ]
    }
   ],
   "source": [
    "# Train for each dimension size over 4 stages of MSNN\n",
    "# for dim in range(1, 3):\n",
    "dim = 3\n",
    "N_train = 50\n",
    "N_eval = 200\n",
    "x_train = create_ds(dim, -1, 1, N_train)\n",
    "y_train = poisson(x_train)\n",
    "\n",
    "x_eval = create_ds(dim, -1.02, 1.02, N_eval)\n",
    "y_eval = poisson(x_eval)\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: (125000, 3), Label size: (125000,)\n",
      "Sampled data size: (15625, 3), Sampled labels size: (15625,)\n",
      "Adam Training for stage: 0\n",
      "L-BFGS Training for stage: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m MSNN \u001b[38;5;241m=\u001b[39m MultistageNeuralNetwork(data\u001b[38;5;241m=\u001b[39mx_train, labels\u001b[38;5;241m=\u001b[39my_train, num_stages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_hidden_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, num_hidden_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m      2\u001b[0m sampled_data, sampled_labels \u001b[38;5;241m=\u001b[39m MSNN\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mMSNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampled_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkappa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m MSNN\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;241m0\u001b[39m, sampled_data)\n",
      "Cell \u001b[0;32mIn[54], line 42\u001b[0m, in \u001b[0;36mMultistageNeuralNetwork.train\u001b[0;34m(self, data, labels, stage, kappa)\u001b[0m\n\u001b[1;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m3000\u001b[39m, \u001b[38;5;241m1\u001b[39m)     \u001b[38;5;66;03m# mode 1 use Adam\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL-BFGS Training for stage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 256\u001b[0m, in \u001b[0;36mNeuralNet.train\u001b[0;34m(self, nIter, idxOpt)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m func_adam\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m idxOpt \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# mode 2: running the Lbfgs optimization\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     func_bfgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLbfgs_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnIter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m func_bfgs\u001b[38;5;241m.\u001b[39mloss\n",
      "Cell \u001b[0;32mIn[53], line 234\u001b[0m, in \u001b[0;36mNeuralNet.Lbfgs_optimizer\u001b[0;34m(self, nIter, varlist)\u001b[0m\n\u001b[1;32m    231\u001b[0m max_nIter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(nIter \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# train the model with L-BFGS solver\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlbfgs_minimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_and_gradients_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_nIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# after training, the final optimized parameters are still in results.position\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# so we have to manually put them back to the model\u001b[39;00m\n\u001b[1;32m    240\u001b[0m func\u001b[38;5;241m.\u001b[39massign_new_model_parameters(results\u001b[38;5;241m.\u001b[39mposition)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow_probability/python/optimizer/lbfgs.py:285\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(value_and_gradients_function, initial_position, previous_optimizer_results, num_correction_pairs, tolerance, x_tolerance, f_relative_tolerance, initial_inverse_hessian_estimate, max_iterations, parallel_iterations, stopping_condition, max_line_search_iterations, f_absolute_tolerance, name)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m   initial_state \u001b[38;5;241m=\u001b[39m previous_optimizer_results\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_iterations\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:660\u001b[0m, in \u001b[0;36mdeprecated_arg_values.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m           _PRINTED_WARNING[(func, arg_name)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    653\u001b[0m         _log_deprecation(\n\u001b[1;32m    654\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrom \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwill be removed \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstructions for updating:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min a future version\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m    659\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m date), instructions)\n\u001b[0;32m--> 660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/ops/while_loop.py:241\u001b[0m, in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile_loop\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m     36\u001b[0m \u001b[38;5;129m@deprecation\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated_arg_values(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m                   maximum_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     53\u001b[0m                   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Repeat `body` while the condition `cond` is true.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m  Note: This op is automatically used in a `tf.function` to convert Python for-\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape_invariants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape_invariants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m      \u001b[49m\u001b[43mparallel_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m      \u001b[49m\u001b[43mback_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mback_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m      \u001b[49m\u001b[43mswap_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswap_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m      \u001b[49m\u001b[43mreturn_same_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/ops/while_loop.py:488\u001b[0m, in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m    485\u001b[0m loop_var_structure \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(type_spec\u001b[38;5;241m.\u001b[39mtype_spec_from_value,\n\u001b[1;32m    486\u001b[0m                                         \u001b[38;5;28mlist\u001b[39m(loop_vars))\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cond(\u001b[38;5;241m*\u001b[39mloop_vars):\n\u001b[0;32m--> 488\u001b[0m   loop_vars \u001b[38;5;241m=\u001b[39m \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m try_to_pack \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loop_vars, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    490\u001b[0m     packed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow_probability/python/optimizer/lbfgs.py:259\u001b[0m, in \u001b[0;36mminimize.<locals>._body\u001b[0;34m(current_state)\u001b[0m\n\u001b[1;32m    253\u001b[0m search_direction \u001b[38;5;241m=\u001b[39m _get_search_direction(current_state)\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# TODO(b/120134934): Check if the derivative at the start point is not\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# negative, if so then reset position/gradient deltas and recompute\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# search direction.\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[43mbfgs_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline_search_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_and_gradients_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_direction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_relative_tolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopping_condition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_line_search_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_absolute_tolerance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# If not failed or converged, update the Hessian estimate.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m should_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39m(next_state\u001b[38;5;241m.\u001b[39mconverged \u001b[38;5;241m|\u001b[39m next_state\u001b[38;5;241m.\u001b[39mfailed)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py:209\u001b[0m, in \u001b[0;36mline_search_step\u001b[0;34m(state, value_and_gradients_function, search_direction, grad_tolerance, f_relative_tolerance, x_tolerance, stopping_condition, max_iterations, f_absolute_tolerance)\u001b[0m\n\u001b[1;32m    204\u001b[0m val_0 \u001b[38;5;241m=\u001b[39m ValueAndGradient(x\u001b[38;5;241m=\u001b[39m_broadcast(\u001b[38;5;241m0\u001b[39m, state\u001b[38;5;241m.\u001b[39mposition),\n\u001b[1;32m    205\u001b[0m                          f\u001b[38;5;241m=\u001b[39mstate\u001b[38;5;241m.\u001b[39mobjective_value,\n\u001b[1;32m    206\u001b[0m                          df\u001b[38;5;241m=\u001b[39mderivative_at_start_pt,\n\u001b[1;32m    207\u001b[0m                          full_gradient\u001b[38;5;241m=\u001b[39mstate\u001b[38;5;241m.\u001b[39mobjective_gradient)\n\u001b[1;32m    208\u001b[0m inactive \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;241m|\u001b[39m state\u001b[38;5;241m.\u001b[39mconverged\n\u001b[0;32m--> 209\u001b[0m ls_result \u001b[38;5;241m=\u001b[39m \u001b[43mhager_zhang\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mline_search_value_grad_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_step_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_broadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_at_zero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconverged\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minactive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iterations\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# No search needed for these.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m state_after_ls \u001b[38;5;241m=\u001b[39m update_fields(\n\u001b[1;32m    217\u001b[0m     state,\n\u001b[1;32m    218\u001b[0m     failed\u001b[38;5;241m=\u001b[39mstate\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;241m|\u001b[39m (\u001b[38;5;241m~\u001b[39mstate\u001b[38;5;241m.\u001b[39mconverged \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m~\u001b[39mls_result\u001b[38;5;241m.\u001b[39mconverged),\n\u001b[1;32m    219\u001b[0m     num_iterations\u001b[38;5;241m=\u001b[39mstate\u001b[38;5;241m.\u001b[39mnum_iterations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    220\u001b[0m     num_objective_evaluations\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    221\u001b[0m         state\u001b[38;5;241m.\u001b[39mnum_objective_evaluations \u001b[38;5;241m+\u001b[39m ls_result\u001b[38;5;241m.\u001b[39mfunc_evals))\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_update_position\u001b[39m():\n\u001b[1;32m    224\u001b[0m   \u001b[38;5;66;03m# For inactive batch members `left.x` is zero. However, their\u001b[39;00m\n\u001b[1;32m    225\u001b[0m   \u001b[38;5;66;03m# `search_direction` might also be undefined, so we can't rely on\u001b[39;00m\n\u001b[1;32m    226\u001b[0m   \u001b[38;5;66;03m# multiplication by zero to produce a `position_delta` of zero.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py:274\u001b[0m, in \u001b[0;36mhager_zhang\u001b[0;34m(value_and_gradients_function, initial_step_size, value_at_initial_step, value_at_zero, converged, threshold_use_approximate_wolfe_condition, shrinkage_param, expansion_param, sufficient_decrease_param, curvature_param, max_iterations, name)\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _bracket_and_search(\n\u001b[1;32m    269\u001b[0m       value_and_gradients_function, init_interval, f_lim, max_iterations,\n\u001b[1;32m    270\u001b[0m       shrinkage_param, expansion_param, sufficient_decrease_param,\n\u001b[1;32m    271\u001b[0m       curvature_param)\n\u001b[1;32m    273\u001b[0m init_active \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39minit_interval\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m~\u001b[39minit_interval\u001b[38;5;241m.\u001b[39mconverged\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprefer_static\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_any\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_active\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_apply_bracket_and_search\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_interval\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow_probability/python/internal/prefer_static.py:249\u001b[0m, in \u001b[0;36mcond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pred_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m pred_value:\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrue_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m false_fn()\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py:268\u001b[0m, in \u001b[0;36mhager_zhang.<locals>._apply_bracket_and_search\u001b[0;34m()\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply_bracket_and_search\u001b[39m():\n\u001b[1;32m    267\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Bracketing and searching to do for valid inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bracket_and_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue_and_gradients_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_lim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshrinkage_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpansion_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msufficient_decrease_param\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcurvature_param\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py:371\u001b[0m, in \u001b[0;36m_bracket_and_search\u001b[0;34m(value_and_gradients_function, init_interval, f_lim, max_iterations, shrinkage_param, expansion_param, sufficient_decrease_param, curvature_param)\u001b[0m\n\u001b[1;32m    360\u001b[0m exhausted_iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mconverged \u001b[38;5;241m&\u001b[39m (\n\u001b[1;32m    361\u001b[0m     bracket_result\u001b[38;5;241m.\u001b[39miteration \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_iterations)\n\u001b[1;32m    363\u001b[0m line_search_args \u001b[38;5;241m=\u001b[39m HagerZhangLineSearchResult(\n\u001b[1;32m    364\u001b[0m     converged\u001b[38;5;241m=\u001b[39mconverged,\n\u001b[1;32m    365\u001b[0m     failed\u001b[38;5;241m=\u001b[39mbracket_result\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;241m|\u001b[39m exhausted_iterations,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m     left\u001b[38;5;241m=\u001b[39mbracket_result\u001b[38;5;241m.\u001b[39mleft,\n\u001b[1;32m    369\u001b[0m     right\u001b[38;5;241m=\u001b[39mbracket_result\u001b[38;5;241m.\u001b[39mright)\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_line_search_after_bracketing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_and_gradients_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_search_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_interval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf_lim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msufficient_decrease_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurvature_param\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinkage_param\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py:498\u001b[0m, in \u001b[0;36m_line_search_after_bracketing\u001b[0;34m(value_and_gradients_function, search_interval, val_0, f_lim, max_iterations, sufficient_decrease_param, curvature_param, shrinkage_param)\u001b[0m\n\u001b[1;32m    494\u001b[0m   interval_shrunk \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    495\u001b[0m       \u001b[38;5;241m~\u001b[39mnext_args\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;241m&\u001b[39m _very_close(next_args\u001b[38;5;241m.\u001b[39mleft\u001b[38;5;241m.\u001b[39mx, next_args\u001b[38;5;241m.\u001b[39mright\u001b[38;5;241m.\u001b[39mx))\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [next_args\u001b[38;5;241m.\u001b[39m_replace(converged\u001b[38;5;241m=\u001b[39mnext_args\u001b[38;5;241m.\u001b[39mconverged \u001b[38;5;241m|\u001b[39m interval_shrunk)]\n\u001b[0;32m--> 498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_loop_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_loop_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msearch_interval\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:660\u001b[0m, in \u001b[0;36mdeprecated_arg_values.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m           _PRINTED_WARNING[(func, arg_name)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    653\u001b[0m         _log_deprecation(\n\u001b[1;32m    654\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrom \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwill be removed \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstructions for updating:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min a future version\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m    659\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m date), instructions)\n\u001b[0;32m--> 660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/ops/while_loop.py:241\u001b[0m, in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile_loop\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m     36\u001b[0m \u001b[38;5;129m@deprecation\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated_arg_values(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m                   maximum_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     53\u001b[0m                   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Repeat `body` while the condition `cond` is true.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m  Note: This op is automatically used in a `tf.function` to convert Python for-\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape_invariants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape_invariants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m      \u001b[49m\u001b[43mparallel_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m      \u001b[49m\u001b[43mback_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mback_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m      \u001b[49m\u001b[43mswap_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswap_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m      \u001b[49m\u001b[43mreturn_same_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/ops/while_loop.py:488\u001b[0m, in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m    485\u001b[0m loop_var_structure \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(type_spec\u001b[38;5;241m.\u001b[39mtype_spec_from_value,\n\u001b[1;32m    486\u001b[0m                                         \u001b[38;5;28mlist\u001b[39m(loop_vars))\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cond(\u001b[38;5;241m*\u001b[39mloop_vars):\n\u001b[0;32m--> 488\u001b[0m   loop_vars \u001b[38;5;241m=\u001b[39m \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m try_to_pack \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loop_vars, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    490\u001b[0m     packed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py:448\u001b[0m, in \u001b[0;36m_line_search_after_bracketing.<locals>._loop_body\u001b[0;34m(curr_interval)\u001b[0m\n\u001b[1;32m    446\u001b[0m active \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39m(curr_interval\u001b[38;5;241m.\u001b[39mconverged \u001b[38;5;241m|\u001b[39m curr_interval\u001b[38;5;241m.\u001b[39mfailed)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# TODO(b/208441613): Skip updates for batch members that are not active?\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m secant2_raw_result \u001b[38;5;241m=\u001b[39m \u001b[43mhzl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msecant2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_and_gradients_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_lim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43msufficient_decrease_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurvature_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m secant2_result \u001b[38;5;241m=\u001b[39m HagerZhangLineSearchResult(\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;66;03m## TODO(b/208441613): `& ~curr_interval.failed` should not be needed.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     converged\u001b[38;5;241m=\u001b[39msecant2_raw_result\u001b[38;5;241m.\u001b[39mconverged \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m~\u001b[39mcurr_interval\u001b[38;5;241m.\u001b[39mfailed,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m     left\u001b[38;5;241m=\u001b[39msecant2_raw_result\u001b[38;5;241m.\u001b[39mleft,\n\u001b[1;32m    459\u001b[0m     right\u001b[38;5;241m=\u001b[39msecant2_raw_result\u001b[38;5;241m.\u001b[39mright)\n\u001b[1;32m    461\u001b[0m should_check_shrinkage \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39m(secant2_result\u001b[38;5;241m.\u001b[39mconverged \u001b[38;5;241m|\u001b[39m secant2_result\u001b[38;5;241m.\u001b[39mfailed)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py:139\u001b[0m, in \u001b[0;36msecant2\u001b[0;34m(value_and_gradients_function, val_0, search_interval, f_lim, sufficient_decrease_param, curvature_param, name)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the secant square procedure of Hager Zhang.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03mGiven an interval that brackets a root, this procedure performs an update of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m      end point of the interval.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecant2\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    138\u001b[0m   \u001b[38;5;66;03m# This will always be s.t. left <= c <= right\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m   val_c \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_and_gradients_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_secant\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_interval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_interval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m   finished \u001b[38;5;241m=\u001b[39m _is_negative_inf(val_c\u001b[38;5;241m.\u001b[39mf)\n\u001b[1;32m    142\u001b[0m   failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39msearch_interval\u001b[38;5;241m.\u001b[39mconverged \u001b[38;5;241m&\u001b[39m (\n\u001b[1;32m    143\u001b[0m       search_interval\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;241m|\u001b[39m (\u001b[38;5;241m~\u001b[39mfinished \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m~\u001b[39mis_finite(val_c)))\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py:310\u001b[0m, in \u001b[0;36m_restrict_along_direction.<locals>._restricted_func\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    308\u001b[0m pt \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m+\u001b[39m t[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, tf\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m*\u001b[39m direction\n\u001b[1;32m    309\u001b[0m t \u001b[38;5;241m=\u001b[39m _broadcast(t, position)\n\u001b[0;32m--> 310\u001b[0m objective_value, gradient \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_and_gradients_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ValueAndGradient(\n\u001b[1;32m    312\u001b[0m     x\u001b[38;5;241m=\u001b[39mt,\n\u001b[1;32m    313\u001b[0m     f\u001b[38;5;241m=\u001b[39mobjective_value,\n\u001b[1;32m    314\u001b[0m     df\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mreduce_sum(gradient \u001b[38;5;241m*\u001b[39m direction, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    315\u001b[0m     full_gradient\u001b[38;5;241m=\u001b[39mgradient)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MSNN = MultistageNeuralNetwork(data=x_train, labels=y_train, num_stages=4, num_hidden_layers=3, num_hidden_nodes=20)\n",
    "sampled_data, sampled_labels = MSNN.sample(2)\n",
    "MSNN.train(sampled_data, sampled_labels, stage=0, kappa=1)\n",
    "pred = MSNN.predict(0, sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSNN = MultistageNeuralNetwork(data=x_train, labels=y_train, num_stages=4, num_hidden_layers=3, num_hidden_nodes=20)\n",
    "kappa2 = MSNN.fftn_(stage=0, data=sampled_data, labels=sampled_labels, pred=pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
