{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73ebf16d-7f7f-4041-8121-1c234c399230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Yongji Wang\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import tensorflow as tf\n",
    "from tensorflow.math import reduce_min, reduce_max\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "from pyDOE import lhs\n",
    "from scipy.io import savemat\n",
    "from scipy.fft import fft, fftfreq, fftshift, fft2, fftn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e3ca06-f91d-4b3f-9db8-92e31d65c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, t_u, x_u, layers, kappa, lt, ut, acts=0):\n",
    "\n",
    "        self.scale = tf.reduce_max(tf.abs(x_u)) / 2\n",
    "        x_u2 = x_u / self.scale\n",
    "        actv = [tf.tanh, tf.sin]\n",
    "\n",
    "        self.t_u = t_u\n",
    "        self.x_u = x_u2\n",
    "        self.datatype = t_u.dtype\n",
    "\n",
    "        self.lt = lt\n",
    "        self.ut = ut\n",
    "\n",
    "        self.layers = layers\n",
    "        self.kappa = kappa\n",
    "\n",
    "        # determine the activation function to use\n",
    "        self.actv = actv[acts]\n",
    "\n",
    "        # Initialize NNs\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # Create a list including all training variables\n",
    "        self.train_variables = self.weights + self.biases\n",
    "        # Key point: anything updates in train_variables will be\n",
    "        #            automatically updated in the original tf.Variable\n",
    "\n",
    "        # define the loss function\n",
    "        self.loss0 = self.scale ** 2\n",
    "        self.loss = []\n",
    "        self.loss_0 = self.loss_NN()\n",
    "\n",
    "        self.optimizer_Adam = tf.optimizers.Adam()\n",
    "\n",
    "    '''\n",
    "    Functions used to establish the initial neural network\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.MPL_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=self.datatype))\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    def MPL_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev, dtype=self.datatype))\n",
    "\n",
    "    def get_params(self):\n",
    "        return (self.weights, self.biases)\n",
    "\n",
    "    '''\n",
    "    Functions used to building the physics-informed contrainst and loss\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def neural_net(self, X):\n",
    "        weights = self.weights\n",
    "        biases = self.biases\n",
    "\n",
    "        num_layers = len(weights) + 1\n",
    "\n",
    "        H = 2.0 * tf.math.divide(\n",
    "                    tf.math.subtract(X, tf.transpose(self.lt)), \n",
    "                    tf.transpose(tf.math.subtract(self.ut, self.lt))) \\\n",
    "            - 1.0\n",
    "\n",
    "        W = weights[0]\n",
    "        b = biases[0]\n",
    "        H = self.actv(tf.add(self.kappa * tf.matmul(H, W), b))\n",
    "\n",
    "        for l in range(1, num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "\n",
    "    @tf.function\n",
    "    # calculate the physics-informed loss function\n",
    "    def loss_NN(self):\n",
    "        self.x_pred = self.neural_net(self.t_u)\n",
    "        loss = tf.reduce_mean(tf.square(self.x_u - self.x_pred))\n",
    "        return loss\n",
    "\n",
    "    '''\n",
    "    Functions used to define ADAM optimizers\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    # define the function to apply the ADAM optimizer\n",
    "    def adam_function(self):\n",
    "        @tf.function\n",
    "        def f():\n",
    "            # calculate the loss\n",
    "            loss_norm = self.loss_NN()\n",
    "            loss_value = loss_norm * self.loss0\n",
    "            # store loss value so we can retrieve later\n",
    "            tf.py_function(f.loss.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "            # print out iteration & loss\n",
    "            f.iter.assign_add(1)\n",
    "\n",
    "            str_iter = tf.strings.as_string([f.iter])\n",
    "            str_loss = tf.strings.as_string([loss_value], precision=4, scientific=True)\n",
    "\n",
    "            str_print = tf.strings.join([\"Mode: Adam\", \"Iter: \", str_iter[0],\n",
    "                                         \", loss: \", str_loss[0]])\n",
    "            tf.cond(\n",
    "                f.iter % 10 == 0,\n",
    "                lambda: tf.print(str_print),\n",
    "                lambda: tf.constant(True)  # return arbitrary for non-printing case\n",
    "            )\n",
    "            return loss_norm\n",
    "\n",
    "        f.iter = tf.Variable(0)\n",
    "        f.term = []\n",
    "        f.loss = []\n",
    "        return f\n",
    "\n",
    "    def Adam_optimizer(self, nIter):\n",
    "        varlist = self.train_variables\n",
    "        func_adam = self.adam_function()\n",
    "        for it in range(nIter):\n",
    "            tf.keras.optimizers.Adam(func_adam, varlist)\n",
    "            #self.optimizer_Adam.minimize(func_adam, varlist)\n",
    "        return func_adam\n",
    "\n",
    "    '''\n",
    "    Functions used to define L-BFGS optimizers\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    # A factory to create a function required by tfp.optimizer.lbfgs_minimize.\n",
    "    def Lbfgs_function(self, varlist):\n",
    "        # obtain the shapes of all trainable parameters in the model\n",
    "        shapes = tf.shape_n(varlist)\n",
    "        n_tensors = len(shapes)\n",
    "\n",
    "        # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n",
    "        # prepare required information first\n",
    "        count = 0\n",
    "        idx = []  # stitch indices\n",
    "        part = []  # partition indices\n",
    "\n",
    "        for i, shape in enumerate(shapes):\n",
    "            n = np.prod(shape)\n",
    "            idx.append(tf.reshape(tf.range(count, count + n, dtype=tf.int32), shape))\n",
    "            part.extend([i] * n)\n",
    "            count += n\n",
    "\n",
    "        part = tf.constant(part)\n",
    "\n",
    "        def assign_new_model_parameters(params_1d):\n",
    "            # A function updating the model's parameters with a 1D tf.Tensor.\n",
    "            # Sub-function under function of class not need to input self\n",
    "\n",
    "            params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "            for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "                varlist[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "        @tf.function\n",
    "        def f(params_1d):\n",
    "            # A function that can be used by tfp.optimizer.lbfgs_minimize.\n",
    "            # This function is created by function_factory.\n",
    "            # Sub-function under function of class not need to input self\n",
    "\n",
    "            # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters\n",
    "            with tf.GradientTape() as tape:\n",
    "                # update the parameters in the model\n",
    "                # this step is critical for self-defined function for L-BFGS\n",
    "                assign_new_model_parameters(params_1d)\n",
    "                # calculate the loss\n",
    "                loss_norm = self.loss_NN()\n",
    "                loss_value = loss_norm * self.loss0\n",
    "\n",
    "            # calculate gradients and convert to 1D tf.Tensor\n",
    "            grads = tape.gradient(loss_norm, varlist)\n",
    "            grads = tf.dynamic_stitch(idx, grads)\n",
    "\n",
    "            # store loss value so we can retrieve later\n",
    "            tf.py_function(f.loss.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "            # print out iteration & loss\n",
    "            f.iter.assign_add(1)\n",
    "\n",
    "            str_iter = tf.strings.as_string([f.iter])\n",
    "            str_loss = tf.strings.as_string([loss_value], precision=4, scientific=True)\n",
    "\n",
    "            str_print = tf.strings.join([\"Mode: LBFGS\", \"Iter: \", str_iter[0],\n",
    "                                         \", loss: \", str_loss[0]])\n",
    "            tf.cond(\n",
    "                f.iter % 3000 == 0,\n",
    "                lambda: tf.print(str_print),\n",
    "                lambda: tf.constant(True)  # return arbitrary for non-printing case\n",
    "            )\n",
    "\n",
    "            return loss_value, grads\n",
    "\n",
    "        # store these information as members so we can use them outside the scope\n",
    "        f.iter = tf.Variable(0)\n",
    "        f.idx = idx\n",
    "        f.part = part\n",
    "        f.shapes = shapes\n",
    "        f.assign_new_model_parameters = assign_new_model_parameters\n",
    "        f.loss = []\n",
    "\n",
    "        return f\n",
    "\n",
    "    # define the function to apply the L-BFGS optimizer\n",
    "    def Lbfgs_optimizer(self, nIter, varlist):\n",
    "\n",
    "        func = self.Lbfgs_function(varlist)\n",
    "\n",
    "        # convert initial model parameters to a 1D tf.Tensor\n",
    "        init_params = tf.dynamic_stitch(func.idx, varlist)\n",
    "\n",
    "        max_nIter = tf.cast(nIter / 3, dtype=tf.int32)\n",
    "\n",
    "        # train the model with L-BFGS solver\n",
    "        results = tfp.optimizer.lbfgs_minimize(\n",
    "            value_and_gradients_function=func, initial_position=init_params,\n",
    "            tolerance=1e-11, max_iterations=max_nIter)\n",
    "\n",
    "        # after training, the final optimized parameters are still in results.position\n",
    "        # so we have to manually put them back to the model\n",
    "        func.assign_new_model_parameters(results.position)\n",
    "\n",
    "        return func\n",
    "\n",
    "    '''\n",
    "    Function used for training the model\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def train(self, nIter, idxOpt):\n",
    "        if idxOpt == 1:\n",
    "            # mode 1: running the Adam optimization\n",
    "            func_adam = self.Adam_optimizer(nIter)\n",
    "            self.loss += func_adam.loss\n",
    "        elif idxOpt == 2:\n",
    "            # mode 2: running the Lbfgs optimization\n",
    "            func_bfgs = self.Lbfgs_optimizer(nIter, self.train_variables)\n",
    "            self.loss += func_bfgs.loss\n",
    "\n",
    "    # @tf.function\n",
    "    def predict(self, t):\n",
    "        x_p = self.neural_net(t) * self.scale\n",
    "        return x_p\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e769d28-65b5-49b9-8abe-95d2bcc04191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(441, 2)\n",
      "(441, 1)\n",
      "(40000, 2)\n",
      "(40000, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(234)\n",
    "tf.random.set_seed(234)\n",
    "\n",
    "def fun_test(t):\n",
    "    # customize the function by the user\n",
    "    x = (t ** 3) / (0.01 + t ** 4) # example 2\n",
    "    #x = tf.math.log(t+2) * tf.cos(2*t + t**3)   # example 1\n",
    "    # x = tf.sin(2*t+1) + 0.2*tf.exp(1.3*t)  # example 2\n",
    "    return x\n",
    "\n",
    "def create_ds(dims, lo, hi, N):\n",
    "    dimensions = [np.linspace(lo, hi, N)] * dims\n",
    "    mesh = np.meshgrid(*dimensions, indexing='xy')\n",
    "    mesh_points = np.stack([m.flatten() for m in mesh], axis=-1)\n",
    "    mesh_tf = tf.cast(mesh_points, dtype=tf.float64)\n",
    "    return mesh_tf\n",
    "\n",
    "def fun_test2d(t):\n",
    "    x = (np.sin(2 * t[:, 0] + 1) - 0.5 * t[:, 0])*(1 - t[:, 1] ** 2)\n",
    "    return x\n",
    "\n",
    "N_train = 21\n",
    "t_train = create_ds(2, -1.02, 1.02, N_train)\n",
    "x_train = tf.reshape(fun_test2d(t_train), [len(t_train), 1])\n",
    "print(t_train.shape)\n",
    "print(x_train.shape)\n",
    "\n",
    "# Domain bounds\n",
    "lt = [reduce_min(t_train[:, 0]), reduce_min(t_train[:, 1])]\n",
    "ut = [reduce_max(t_train[:, 0]), reduce_max(t_train[:, 1])]\n",
    "lt = tf.convert_to_tensor(lt)\n",
    "ut = tf.convert_to_tensor(ut)\n",
    "\n",
    "# Number of test points (total # = N_eval ** 2)\n",
    "N_eval = 200\n",
    "t_eval = create_ds(2, -1.02, 1.02, N_eval)\n",
    "x_eval = tf.reshape(fun_test2d(t_eval), [len(t_eval), 1])\n",
    "print(t_eval.shape)\n",
    "print(x_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32bcbf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: LBFGSIter: 3000, loss: 8.6070e-07\n",
      "Mode: LBFGSIter: 6000, loss: 2.6280e-07\n",
      "Mode: LBFGSIter: 9000, loss: 1.0748e-07\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "First stage of training\n",
    "'''\n",
    "# acts = 0 indicates selecting tanh as the activation function\n",
    "layers = [2, 20, 20, 20, 1]\n",
    "kappa = 1\n",
    "model = PhysicsInformedNN(t_train, x_train, layers, kappa, lt, ut, acts=0)\n",
    "# start the first stage training\n",
    "model.train(3000, 1)     # mode 1 use Adam\n",
    "model.train(10000, 2)    # mode 2 use L-bfgs\n",
    "x_pred = model.predict(t_eval)\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "368b39ba-9a13-48bd-8c93-42355f33115e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Rate: 10.5 Hz\n",
      "Sample Rate: 10.5 Hz\n",
      "Dominant Frequency: 1.0 Hz\n",
      "Magnitude: 6.545446512673152e-05\n",
      "kappa2f: 6.283185307179586\n",
      "kappa2: 414\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Second stage of training\n",
    "'''\n",
    "# calculate the residue for the second stage\n",
    "x_train2 = x_train - model.predict(t_train)\n",
    "\n",
    "###############################################\n",
    "# Calculate dominant frequency\n",
    "g = x_train2.numpy().flatten()\n",
    "GG = g.reshape(N_train, N_train)\n",
    "G = fft2(GG)\n",
    "G_shifted = fftshift(G)\n",
    "\n",
    "N = len(G)\n",
    "# Total time range\n",
    "total_time_range = 2  # from -1 to 1\n",
    "\n",
    "# Calculate the sample rate\n",
    "sample_rate = N / total_time_range\n",
    "\n",
    "# Perform FFT\n",
    "half_N = N // 2\n",
    "T = 1.0 / sample_rate\n",
    "G_pos = G_shifted[half_N:, half_N:]\n",
    "freq_x = fftshift(fftfreq(GG.shape[1], d=T))\n",
    "freq_y = fftshift(fftfreq(GG.shape[0], d=T))\n",
    "\n",
    "freq_x_pos = freq_x[half_N:]\n",
    "freq_y_pos = freq_y[half_N:]\n",
    "\n",
    "# Identify the dominant frequency\n",
    "magnitude_spectrum = np.abs(G_pos)\n",
    "max_idx = np.unravel_index(np.argmax(magnitude_spectrum), magnitude_spectrum.shape)\n",
    "dominant_freq_x = freq_x_pos[max_idx[1]]\n",
    "dominant_freq_y = freq_y_pos[max_idx[0]]\n",
    "magnitude = magnitude_spectrum[max_idx] / (N * N) # normalize magnitude\n",
    "dominant_freq = max(dominant_freq_x, dominant_freq_y)\n",
    "\n",
    "print(f\"Sample Rate: {sample_rate} Hz\")\n",
    "print(f\"Dominant Frequency: {dominant_freq} Hz\")\n",
    "print(f\"Magnitude: {magnitude}\")\n",
    "kappa2f =  2 * np.pi * dominant_freq\n",
    "print(f\"kappa2f: {kappa2f}\")\n",
    "######################################################\n",
    "\n",
    "# get the scale factor approximately by finding the number of zeros of the residues)\n",
    "# (more official way is to use the fourier transform and get dominant frequency)\n",
    "idxZero = np.where(x_train2[0:-1, 0] * x_train2[1:, 0] < 0)[0]\n",
    "NumZero = idxZero.shape[0]\n",
    "kappa2 = 3*NumZero\n",
    "print(f\"kappa2: {kappa2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62b36864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase data from 21 -> 61 points\n",
    "N_train = 61\n",
    "t1 = np.linspace(-1.02, 1.02, N_train)[:, None]\n",
    "t2 = np.linspace(-1.02, 1.02, N_train)[:, None]\n",
    "xx, yy = np.meshgrid(t1, t2)\n",
    "t = tf.stack([tf.reshape(xx, [-1]), tf.reshape(yy, [-1])], axis=1)\n",
    "t_train2 = tf.cast(t, dtype=tf.float64)\n",
    "x_train = tf.reshape(fun_test2d(t_train2), [len(t), 1])\n",
    "x_train2 = x_train - model.predict(t_train2)\n",
    "\n",
    "# Domain bounds\n",
    "lt = [t1.min(0)[0], t2.min(0)[0]]\n",
    "ut = [t1.max(0)[0], t2.max(0)[0]]\n",
    "lt = tf.convert_to_tensor(lt)\n",
    "ut = tf.convert_to_tensor(ut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88ac6274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: LBFGSIter: 3000, loss: 1.1742e-10\n",
      "Mode: LBFGSIter: 6000, loss: 5.2201e-11\n",
      "Mode: LBFGSIter: 9000, loss: 2.9069e-11\n",
      "Mode: LBFGSIter: 12000, loss: 1.9028e-11\n",
      "Mode: LBFGSIter: 15000, loss: 1.3525e-11\n",
      "Mode: LBFGSIter: 18000, loss: 1.0473e-11\n"
     ]
    }
   ],
   "source": [
    "# (acts = 1 indicates selecting sin as the activation function)\n",
    "model2 = PhysicsInformedNN(t_train2, x_train2, layers, kappa2f, lt, ut, acts=1)\n",
    "# start the second stage training\n",
    "model2.train(5000, 1)    # mode 1 use Adam\n",
    "model2.train(20000, 2)   # mode 2 use L-bfgs\n",
    "x_pred2 = model2.predict(t_eval)\n",
    "# combining the result from first and second stage\n",
    "x_p = x_pred + x_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a890c445-2f00-4355-aea5-e51290c55f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Rate: 30.5 Hz\n",
      "Dominant Frequency: 4.5 Hz\n",
      "Magnitude: 2.908547487624634e-07\n",
      "kappa3f: 28.274333882308138\n",
      "kappa3: 3024\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Third stage of training\n",
    "'''\n",
    "\n",
    "# calculate the residue for the third stage\n",
    "x_train3 = x_train - model.predict(t_train2) - model2.predict(t_train2)\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Calculate dominant frequency\n",
    "g = x_train3.numpy().flatten()\n",
    "GG = g.reshape(N_train, N_train)\n",
    "G = fft2(GG)\n",
    "G_shifted = fftshift(G)\n",
    "\n",
    "N = len(G)\n",
    "# Total time range\n",
    "total_time_range = 2  # from -1 to 1\n",
    "\n",
    "# Calculate the sample rate\n",
    "sample_rate = N / total_time_range\n",
    "\n",
    "# Perform FFT\n",
    "half_N = N // 2\n",
    "T = 1.0 / sample_rate\n",
    "G_pos = G_shifted[half_N:, half_N:]\n",
    "freq_x = fftshift(fftfreq(GG.shape[1], d=T))\n",
    "freq_y = fftshift(fftfreq(GG.shape[0], d=T))\n",
    "\n",
    "freq_x_pos = freq_x[half_N:]\n",
    "freq_y_pos = freq_y[half_N:]\n",
    "\n",
    "# Identify the dominant frequency\n",
    "magnitude_spectrum = np.abs(G_pos)\n",
    "max_idx = np.unravel_index(np.argmax(magnitude_spectrum), magnitude_spectrum.shape)\n",
    "dominant_freq_x = freq_x_pos[max_idx[1]]\n",
    "dominant_freq_y = freq_y_pos[max_idx[0]]\n",
    "magnitude = magnitude_spectrum[max_idx] / (N * N) # normalize magnitude\n",
    "dominant_freq = max(dominant_freq_x, dominant_freq_y)\n",
    "\n",
    "print(f\"Sample Rate: {sample_rate} Hz\")\n",
    "print(f\"Dominant Frequency: {dominant_freq} Hz\")\n",
    "print(f\"Magnitude: {magnitude}\")\n",
    "kappa3f =  2 * np.pi * dominant_freq\n",
    "print(f\"kappa3f: {kappa3f}\")\n",
    "######################################################\n",
    "\n",
    "# get the scale factor approximately by finding the number of zeros of the residues)\n",
    "idxZero = np.where(x_train3[0:-1, 0] * x_train3[1:, 0] < 0)[0]\n",
    "NumZero2 = idxZero.shape[0]\n",
    "kappa3 = 3*NumZero2\n",
    "print(f\"kappa3: {kappa3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3ef0a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the data points from 61 -> 121\n",
    "N_train = 121\n",
    "t1 = np.linspace(-1.02, 1.02, N_train)[:, None]\n",
    "t2 = np.linspace(-1.02, 1.02, N_train)[:, None]\n",
    "xx, yy = np.meshgrid(t1, t2)\n",
    "t = tf.stack([tf.reshape(xx, [-1]), tf.reshape(yy, [-1])], axis=1)\n",
    "t_train3 = tf.cast(t, dtype=tf.float64)\n",
    "x_train = tf.reshape(fun_test2d(t_train3), [len(t), 1])\n",
    "x_train3 = x_train - model.predict(t_train3) - model2.predict(t_train3)\n",
    "\n",
    "# Domain bounds\n",
    "lt = [t1.min(0)[0], t2.min(0)[0]]\n",
    "ut = [t1.max(0)[0], t2.max(0)[0]]\n",
    "lt = tf.convert_to_tensor(lt)\n",
    "ut = tf.convert_to_tensor(ut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6d4b131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function PhysicsInformedNN.loss_NN at 0x000001A84236E160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Mode: LBFGSIter: 3000, loss: 1.0451e-12\n",
      "Mode: LBFGSIter: 6000, loss: 5.3529e-13\n",
      "Mode: LBFGSIter: 9000, loss: 3.5795e-13\n",
      "Mode: LBFGSIter: 12000, loss: 2.6487e-13\n",
      "Mode: LBFGSIter: 15000, loss: 2.1857e-13\n",
      "Mode: LBFGSIter: 18000, loss: 1.8124e-13\n",
      "Mode: LBFGSIter: 21000, loss: 1.4826e-13\n",
      "Mode: LBFGSIter: 24000, loss: 1.2403e-13\n",
      "Mode: LBFGSIter: 27000, loss: 9.8575e-14\n"
     ]
    }
   ],
   "source": [
    "# (acts = 1 indicates selecting sin as the activation function)\n",
    "layers2 = [2, 30, 30, 30, 1]\n",
    "model3 = PhysicsInformedNN(t_train, x_train3, layers2, kappa3f, lt, ut, acts=1)\n",
    "# start the third stage training\n",
    "model3.train(5000, 1)      # mode 1 use Adam\n",
    "model3.train(30000, 2)     # mode 2 use L-bfgs\n",
    "x_pred3 = model3.predict(t_eval)\n",
    "# combining the result from first, second and third stages\n",
    "x_p2 = x_pred + x_pred2 + x_pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ca75ef6-cb9f-4d94-aa88-8898c6726249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Rate: 60.5 Hz\n",
      "Dominant Frequency: 7.0 Hz\n",
      "Magnitude: 1.3306890773200296e-08\n",
      "kappa4f: 43.982297150257104\n",
      "kappa4: 10242\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Fourth stage of training\n",
    "'''\n",
    "# calculate the residue for the forth stage\n",
    "x_train4 = x_train - model.predict(t_train3) - model2.predict(t_train3) - model3.predict(t_train3)\n",
    "\n",
    "###############################################\n",
    "# Calculate dominant frequency\n",
    "g = x_train4.numpy().flatten()\n",
    "GG = g.reshape(N_train, N_train)\n",
    "G = fft2(GG)\n",
    "G_shifted = fftshift(G)\n",
    "\n",
    "N = len(G)\n",
    "# Total time range\n",
    "total_time_range = 2  # from -1 to 1\n",
    "\n",
    "# Calculate the sample rate\n",
    "sample_rate = N / total_time_range\n",
    "\n",
    "# Perform FFT\n",
    "half_N = N // 2\n",
    "T = 1.0 / sample_rate\n",
    "G_pos = G_shifted[half_N:, half_N:]\n",
    "freq_x = fftshift(fftfreq(GG.shape[1], d=T))\n",
    "freq_y = fftshift(fftfreq(GG.shape[0], d=T))\n",
    "\n",
    "freq_x_pos = freq_x[half_N:]\n",
    "freq_y_pos = freq_y[half_N:]\n",
    "\n",
    "# Identify the dominant frequency\n",
    "magnitude_spectrum = np.abs(G_pos)\n",
    "max_idx = np.unravel_index(np.argmax(magnitude_spectrum), magnitude_spectrum.shape)\n",
    "dominant_freq_x = freq_x_pos[max_idx[1]]\n",
    "dominant_freq_y = freq_y_pos[max_idx[0]]\n",
    "magnitude = magnitude_spectrum[max_idx] / (N * N) # normalize magnitude\n",
    "dominant_freq = max(dominant_freq_x, dominant_freq_y)\n",
    "\n",
    "print(f\"Sample Rate: {sample_rate} Hz\")\n",
    "print(f\"Dominant Frequency: {dominant_freq} Hz\")\n",
    "print(f\"Magnitude: {magnitude}\")\n",
    "kappa4f =  2 * np.pi * dominant_freq\n",
    "print(f\"kappa4f: {kappa4f}\")\n",
    "######################################################\n",
    "\n",
    "\n",
    "# get the scale factor approximately by finding the number of zeros of the residues)\n",
    "idxZero = np.where(x_train4[0:-1, 0] * x_train4[1:, 0] < 0)[0]\n",
    "NumZero3 = idxZero.shape[0]\n",
    "kappa4 = 3*NumZero3\n",
    "print(f\"kappa4: {kappa4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e84a9d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function PhysicsInformedNN.loss_NN at 0x000001A8E52EE160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Mode: LBFGSIter: 3000, loss: 8.2730e-14\n",
      "Mode: LBFGSIter: 6000, loss: 8.1677e-14\n",
      "Mode: LBFGSIter: 9000, loss: 8.1098e-14\n",
      "Mode: LBFGSIter: 12000, loss: 8.0407e-14\n",
      "Mode: LBFGSIter: 15000, loss: 8.0192e-14\n",
      "Mode: LBFGSIter: 18000, loss: 7.9867e-14\n",
      "Mode: LBFGSIter: 21000, loss: 7.9714e-14\n",
      "Mode: LBFGSIter: 24000, loss: 7.9604e-14\n",
      "Mode: LBFGSIter: 27000, loss: 7.9563e-14\n",
      "Mode: LBFGSIter: 30000, loss: 7.9560e-14\n",
      "Mode: LBFGSIter: 33000, loss: 7.9350e-14\n",
      "Mode: LBFGSIter: 36000, loss: 7.9174e-14\n",
      "Mode: LBFGSIter: 39000, loss: 7.9016e-14\n"
     ]
    }
   ],
   "source": [
    "# (acts = 1 indicates selecting sin as the activation function)\n",
    "model4 = PhysicsInformedNN(t_train, x_train4, layers2, kappa4f, lt, ut)\n",
    "# start the forth stage training\n",
    "model4.train(5000, 1)\n",
    "model4.train(40000, 2)\n",
    "x_pred4 = model4.predict(t_eval)\n",
    "# combining the result from all stages\n",
    "x_p3 = x_pred + x_pred2 + x_pred3 + x_pred4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f41a2be9-c8b1-4428-b53f-fb627cb2de93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error u: 7.185221e-04\n",
      "Error u: 7.520850e-06\n",
      "Error u: 7.586698e-07\n",
      "Error u: 7.310768e-07\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# combine the loss of all four stages of training\n",
    "loss = np.array(model.loss + model2.loss + model3.loss + model4.loss)\n",
    "\n",
    "residue = x_train4 - model4.predict(t_train)\n",
    "\n",
    "error_x = np.linalg.norm(x_eval-x_pred, 2)/np.linalg.norm(x_eval, 2)\n",
    "print('Error u: %e' % (error_x))\n",
    "\n",
    "error_x2 = np.linalg.norm(x_eval-x_p, 2)/np.linalg.norm(x_eval, 2)\n",
    "print('Error u: %e' % (error_x2))\n",
    "\n",
    "error_x3 = np.linalg.norm(x_eval-x_p2, 2)/np.linalg.norm(x_eval, 2)\n",
    "print('Error u: %e' % (error_x3))\n",
    "\n",
    "error_x4 = np.linalg.norm(x_eval-x_p3, 2)/np.linalg.norm(x_eval, 2)\n",
    "print('Error u: %e' % (error_x4))\n",
    "\n",
    "mdic = {\"t\": t_eval.numpy(), \"x_g\": x_eval.numpy(), \"x0\": x_pred.numpy(),\n",
    "        \"x1\": x_pred2.numpy(), \"x2\": x_pred3.numpy(), 'x3': x_pred4.numpy(),\n",
    "        \"err\": residue.numpy(), 'loss': loss}\n",
    "FileName = 'Reg_mNN_1D_64bit.mat'\n",
    "savemat(FileName, mdic)\n",
    "\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1fe4b79a-8857-470f-8c42-49735b0b67b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss, 'b-')\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1988f9-21e1-4f7a-8d68-1e5f61be3949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
