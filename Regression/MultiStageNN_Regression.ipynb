{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73ebf16d-7f7f-4041-8121-1c234c399230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 09:36:38.563875: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "@author: Yongji Wang\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import tensorflow_probability as tfp\n",
    "import time\n",
    "from pyDOE import lhs\n",
    "from scipy.io import savemat\n",
    "from scipy.fft import fft, fftfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e3ca06-f91d-4b3f-9db8-92e31d65c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, t_u, x_u, layers, kappa, lt, ut, acts=0):\n",
    "\n",
    "        self.scale = tf.reduce_max(tf.abs(x_u)) / 2\n",
    "        x_u2 = x_u / self.scale\n",
    "        actv = [tf.tanh, tf.sin]\n",
    "\n",
    "        self.t_u = t_u\n",
    "        self.x_u = x_u2\n",
    "        self.datatype = t_u.dtype\n",
    "\n",
    "        self.lt = lt\n",
    "        self.ut = ut\n",
    "\n",
    "        self.layers = layers\n",
    "        self.kappa = kappa\n",
    "\n",
    "        # determine the activation function to use\n",
    "        self.actv = actv[acts]\n",
    "\n",
    "        # Initialize NNs\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # Create a list including all training variables\n",
    "        self.train_variables = self.weights + self.biases\n",
    "        # Key point: anything updates in train_variables will be\n",
    "        #            automatically updated in the original tf.Variable\n",
    "\n",
    "        # define the loss function\n",
    "        self.loss0 = self.scale ** 2\n",
    "        self.loss = []\n",
    "        self.loss_0 = self.loss_NN()\n",
    "\n",
    "        self.optimizer_Adam = tf.optimizers.Adam()\n",
    "\n",
    "    '''\n",
    "    Functions used to establish the initial neural network\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.MPL_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=self.datatype))\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    def MPL_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev, dtype=self.datatype))\n",
    "\n",
    "    def get_params(self):\n",
    "        return (self.weights, self.biases)\n",
    "\n",
    "    '''\n",
    "    Functions used to building the physics-informed contrainst and loss\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def neural_net(self, X):\n",
    "        weights = self.weights\n",
    "        biases = self.biases\n",
    "\n",
    "        num_layers = len(weights) + 1\n",
    "\n",
    "        H = 2.0 * (X - self.lt) / (self.ut - self.lt) - 1.0\n",
    "\n",
    "        W = weights[0]\n",
    "        b = biases[0]\n",
    "        H = self.actv(tf.add(self.kappa * tf.matmul(H, W), b))\n",
    "\n",
    "        for l in range(1, num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "\n",
    "    @tf.function\n",
    "    # calculate the physics-informed loss function\n",
    "    def loss_NN(self):\n",
    "        self.x_pred = self.neural_net(self.t_u)\n",
    "        loss = tf.reduce_mean(tf.square(self.x_u - self.x_pred))\n",
    "        return loss\n",
    "\n",
    "    '''\n",
    "    Functions used to define ADAM optimizers\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    # define the function to apply the ADAM optimizer\n",
    "    def adam_function(self):\n",
    "        @tf.function\n",
    "        def f():\n",
    "            # calculate the loss\n",
    "            loss_norm = self.loss_NN()\n",
    "            loss_value = loss_norm * self.loss0\n",
    "            # store loss value so we can retrieve later\n",
    "            tf.py_function(f.loss.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "            # print out iteration & loss\n",
    "            f.iter.assign_add(1)\n",
    "\n",
    "            str_iter = tf.strings.as_string([f.iter])\n",
    "            str_loss = tf.strings.as_string([loss_value], precision=4, scientific=True)\n",
    "\n",
    "            str_print = tf.strings.join([\"Mode: Adam\", \"Iter: \", str_iter[0],\n",
    "                                         \", loss: \", str_loss[0]])\n",
    "            tf.cond(\n",
    "                f.iter % 10 == 0,\n",
    "                lambda: tf.print(str_print),\n",
    "                lambda: tf.constant(True)  # return arbitrary for non-printing case\n",
    "            )\n",
    "            return loss_norm\n",
    "\n",
    "        f.iter = tf.Variable(0)\n",
    "        f.term = []\n",
    "        f.loss = []\n",
    "        return f\n",
    "\n",
    "    def Adam_optimizer(self, nIter):\n",
    "        varlist = self.train_variables\n",
    "        func_adam = self.adam_function()\n",
    "        for it in range(nIter):\n",
    "            tf.keras.optimizers.Adam(func_adam, varlist)\n",
    "            #self.optimizer_Adam.minimize(func_adam, varlist)\n",
    "        return func_adam\n",
    "\n",
    "    '''\n",
    "    Functions used to define L-BFGS optimizers\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    # A factory to create a function required by tfp.optimizer.lbfgs_minimize.\n",
    "    def Lbfgs_function(self, varlist):\n",
    "        # obtain the shapes of all trainable parameters in the model\n",
    "        shapes = tf.shape_n(varlist)\n",
    "        n_tensors = len(shapes)\n",
    "\n",
    "        # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n",
    "        # prepare required information first\n",
    "        count = 0\n",
    "        idx = []  # stitch indices\n",
    "        part = []  # partition indices\n",
    "\n",
    "        for i, shape in enumerate(shapes):\n",
    "            n = np.prod(shape)\n",
    "            idx.append(tf.reshape(tf.range(count, count + n, dtype=tf.int32), shape))\n",
    "            part.extend([i] * n)\n",
    "            count += n\n",
    "\n",
    "        part = tf.constant(part)\n",
    "\n",
    "        def assign_new_model_parameters(params_1d):\n",
    "            # A function updating the model's parameters with a 1D tf.Tensor.\n",
    "            # Sub-function under function of class not need to input self\n",
    "\n",
    "            params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "            for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "                varlist[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "        @tf.function\n",
    "        def f(params_1d):\n",
    "            # A function that can be used by tfp.optimizer.lbfgs_minimize.\n",
    "            # This function is created by function_factory.\n",
    "            # Sub-function under function of class not need to input self\n",
    "\n",
    "            # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters\n",
    "            with tf.GradientTape() as tape:\n",
    "                # update the parameters in the model\n",
    "                # this step is critical for self-defined function for L-BFGS\n",
    "                assign_new_model_parameters(params_1d)\n",
    "                # calculate the loss\n",
    "                loss_norm = self.loss_NN()\n",
    "                loss_value = loss_norm * self.loss0\n",
    "\n",
    "            # calculate gradients and convert to 1D tf.Tensor\n",
    "            grads = tape.gradient(loss_norm, varlist)\n",
    "            grads = tf.dynamic_stitch(idx, grads)\n",
    "\n",
    "            # store loss value so we can retrieve later\n",
    "            tf.py_function(f.loss.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "            # print out iteration & loss\n",
    "            f.iter.assign_add(1)\n",
    "\n",
    "            str_iter = tf.strings.as_string([f.iter])\n",
    "            str_loss = tf.strings.as_string([loss_value], precision=4, scientific=True)\n",
    "\n",
    "            str_print = tf.strings.join([\"Mode: LBFGS\", \"Iter: \", str_iter[0],\n",
    "                                         \", loss: \", str_loss[0]])\n",
    "            tf.cond(\n",
    "                f.iter % 3000 == 0,\n",
    "                lambda: tf.print(str_print),\n",
    "                lambda: tf.constant(True)  # return arbitrary for non-printing case\n",
    "            )\n",
    "\n",
    "            return loss_value, grads\n",
    "\n",
    "        # store these information as members so we can use them outside the scope\n",
    "        f.iter = tf.Variable(0)\n",
    "        f.idx = idx\n",
    "        f.part = part\n",
    "        f.shapes = shapes\n",
    "        f.assign_new_model_parameters = assign_new_model_parameters\n",
    "        f.loss = []\n",
    "\n",
    "        return f\n",
    "\n",
    "    # define the function to apply the L-BFGS optimizer\n",
    "    def Lbfgs_optimizer(self, nIter, varlist):\n",
    "\n",
    "        func = self.Lbfgs_function(varlist)\n",
    "\n",
    "        # convert initial model parameters to a 1D tf.Tensor\n",
    "        init_params = tf.dynamic_stitch(func.idx, varlist)\n",
    "\n",
    "        max_nIter = tf.cast(nIter / 3, dtype=tf.int32)\n",
    "\n",
    "        # train the model with L-BFGS solver\n",
    "        results = tfp.optimizer.lbfgs_minimize(\n",
    "            value_and_gradients_function=func, initial_position=init_params,\n",
    "            tolerance=1e-11, max_iterations=max_nIter)\n",
    "\n",
    "        # after training, the final optimized parameters are still in results.position\n",
    "        # so we have to manually put them back to the model\n",
    "        func.assign_new_model_parameters(results.position)\n",
    "\n",
    "        return func\n",
    "\n",
    "    '''\n",
    "    Function used for training the model\n",
    "    ===============================================================\n",
    "    '''\n",
    "\n",
    "    def train(self, nIter, idxOpt):\n",
    "        if idxOpt == 1:\n",
    "            # mode 1: running the Adam optimization\n",
    "            func_adam = self.Adam_optimizer(nIter)\n",
    "            self.loss += func_adam.loss\n",
    "        elif idxOpt == 2:\n",
    "            # mode 2: running the Lbfgs optimization\n",
    "            func_bfgs = self.Lbfgs_optimizer(nIter, self.train_variables)\n",
    "            self.loss += func_bfgs.loss\n",
    "\n",
    "    # @tf.function\n",
    "    def predict(self, t):\n",
    "        x_p = self.neural_net(t) * self.scale\n",
    "        return x_p\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e769d28-65b5-49b9-8abe-95d2bcc04191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: LBFGSIter: 3000, loss: 1.5291e-07\n",
      "Mode: LBFGSIter: 6000, loss: 4.5174e-08\n",
      "Mode: LBFGSIter: 9000, loss: 1.5056e-08\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(234)\n",
    "tf.random.set_seed(234)\n",
    "\n",
    "N_eval = 8000\n",
    "layers = [1, 20, 20, 20, 1]\n",
    "layers2 = [1, 30, 30, 30, 1]\n",
    "kappa = 1\n",
    "\n",
    "\n",
    "def fun_test(t):\n",
    "    # customize the function by the user\n",
    "    x = (t ** 3) / (0.01 + t ** 4) # example 2\n",
    "    #x = tf.math.log(t+2) * tf.cos(2*t + t**3)   # example 1\n",
    "    # x = tf.sin(2*t+1) + 0.2*tf.exp(1.3*t)  # example 2\n",
    "    return x\n",
    "\n",
    "\n",
    "t = np.linspace(-1.02, 1.02, 1501)[:, None]\n",
    "t_train = tf.cast(t, dtype=tf.float64)\n",
    "x_train = fun_test(t_train)\n",
    "\n",
    "# Domain bounds\n",
    "lt = t.min(0)\n",
    "ut = t.max(0)\n",
    "\n",
    "t_eval = np.linspace(-1, 1, N_eval)[:, None]\n",
    "t_eval = tf.cast(t_eval, dtype=tf.float64)\n",
    "x_eval = fun_test(t_eval)\n",
    "\n",
    "'''\n",
    "First stage of training\n",
    "'''\n",
    "# acts = 0 indicates selecting tanh as the activation function\n",
    "model = PhysicsInformedNN(t_train, x_train, layers, kappa, lt, ut, acts=0)\n",
    "# start the first stage training\n",
    "model.train(3000, 1)     # mode 1 use Adam\n",
    "model.train(10000, 2)    # mode 2 use L-bfgs\n",
    "x_pred = model.predict(t_eval)\n",
    "######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "368b39ba-9a13-48bd-8c93-42355f33115e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Rate: 750.5 Hz\n",
      "Dominant Frequency: 3.5 Hz\n",
      "kappa2f: 21.991148575128552\n",
      "54\n",
      "Mode: LBFGSIter: 3000, loss: 6.4270e-12\n",
      "Mode: LBFGSIter: 6000, loss: 1.2514e-12\n",
      "Mode: LBFGSIter: 9000, loss: 5.7829e-13\n",
      "Mode: LBFGSIter: 12000, loss: 3.1082e-13\n",
      "Mode: LBFGSIter: 15000, loss: 1.1173e-13\n",
      "Mode: LBFGSIter: 18000, loss: 6.1973e-14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Second stage of training\n",
    "'''\n",
    "# calculate the residue for the second stage\n",
    "x_train2 = x_train - model.predict(t_train)\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Calculate dominant frequency\n",
    "g = x_train2.numpy().flatten()\n",
    "N = len(g)\n",
    "# Total time range\n",
    "total_time_range = 2  # from -1 to 1\n",
    "\n",
    "# Calculate the sample rate\n",
    "sample_rate = N / total_time_range\n",
    "print(f\"Sample Rate: {sample_rate} Hz\")\n",
    "\n",
    "# Perform FFT\n",
    "T = 1.0 / sample_rate\n",
    "yf = fft(g)\n",
    "xf = fftfreq(N, T)[:N//2]\n",
    "\n",
    "# Identify the dominant frequency\n",
    "dominant_freq_idx = np.argmax(np.abs(yf[:N//2]))\n",
    "dominant_freq = xf[dominant_freq_idx]\n",
    "magnitude = np.abs(yf[dominant_freq_idx]) / N\n",
    "\n",
    "print(f\"Dominant Frequency: {dominant_freq} Hz\")\n",
    "kappa2f =  2 * np.pi * dominant_freq\n",
    "print(f\"kappa2f: {kappa2f}\")\n",
    "######################################################\n",
    "\n",
    "# get the scale factor approximately by finding the number of zeros of the residues)\n",
    "# (more official way is to use the fourier transform and get dominant frequency)\n",
    "idxZero = np.where(x_train2[0:-1, 0] * x_train2[1:, 0] < 0)[0]\n",
    "NumZero = idxZero.shape[0]\n",
    "kappa2 = 3*NumZero\n",
    "print(kappa2)\n",
    "# (acts = 1 indicates selecting sin as the activation function)\n",
    "model2 = PhysicsInformedNN(t_train, x_train2, layers, kappa2f, lt, ut, acts=1)\n",
    "# start the second stage training\n",
    "model2.train(5000, 1)    # mode 1 use Adam\n",
    "model2.train(20000, 2)   # mode 2 use L-bfgs\n",
    "x_pred2 = model2.predict(t_eval)\n",
    "# combining the result from first and second stage\n",
    "x_p = x_pred + x_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a890c445-2f00-4355-aea5-e51290c55f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Rate: 2400.5 Hz\n",
      "Dominant Frequency: 20.0 Hz\n",
      "kappa3f: 125.66370614359172\n",
      "192\n",
      "Mode: LBFGSIter: 3000, loss: 5.4469e-16\n",
      "Mode: LBFGSIter: 6000, loss: 8.4263e-17\n",
      "Mode: LBFGSIter: 9000, loss: 2.7404e-17\n",
      "Mode: LBFGSIter: 12000, loss: 1.5154e-17\n",
      "Mode: LBFGSIter: 15000, loss: 7.5814e-18\n",
      "Mode: LBFGSIter: 18000, loss: 3.9419e-18\n",
      "Mode: LBFGSIter: 21000, loss: 2.5096e-18\n",
      "Mode: LBFGSIter: 24000, loss: 1.8322e-18\n",
      "Mode: LBFGSIter: 27000, loss: 1.3682e-18\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Third stage of training\n",
    "'''\n",
    "# increase the data points for the third stage (assuming it is available)\n",
    "t2 = np.linspace(-1.02, 1.02, 4801)[:, None]\n",
    "t_train2 = tf.cast(t2, dtype=tf.float64)\n",
    "x_train = fun_test(t_train2)\n",
    "# calculate the residue for the third stage\n",
    "x_train3 = x_train - model.predict(t_train2) - model2.predict(t_train2)\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Calculate dominant frequency\n",
    "g = x_train3.numpy().flatten()\n",
    "N = len(g)\n",
    "# Total time range\n",
    "total_time_range = 2  # from -1 to 1\n",
    "\n",
    "# Calculate the sample rate\n",
    "sample_rate = N / total_time_range\n",
    "print(f\"Sample Rate: {sample_rate} Hz\")\n",
    "\n",
    "# Perform FFT\n",
    "T = 1.0 / sample_rate\n",
    "yf = fft(g)\n",
    "xf = fftfreq(N, T)[:N//2]\n",
    "\n",
    "# Identify the dominant frequency\n",
    "dominant_freq_idx = np.argmax(np.abs(yf[:N//2]))\n",
    "dominant_freq = xf[dominant_freq_idx]\n",
    "magnitude = np.abs(yf[dominant_freq_idx]) / N\n",
    "\n",
    "print(f\"Dominant Frequency: {dominant_freq} Hz\")\n",
    "kappa3f =  2 * np.pi * dominant_freq\n",
    "print(f\"kappa3f: {kappa3f}\")\n",
    "######################################################\n",
    "\n",
    "\n",
    "\n",
    "# get the scale factor approximately by finding the number of zeros of the residues)\n",
    "idxZero = np.where(x_train3[0:-1, 0] * x_train3[1:, 0] < 0)[0]\n",
    "NumZero2 = idxZero.shape[0]\n",
    "kappa3 = 3*NumZero2\n",
    "print(kappa3)\n",
    "\n",
    "# (acts = 1 indicates selecting sin as the activation function)\n",
    "model3 = PhysicsInformedNN(t_train2, x_train3, layers2, kappa3f, lt, ut, acts=1)\n",
    "# start the third stage training\n",
    "model3.train(5000, 1)      # mode 1 use Adam\n",
    "model3.train(30000, 2)     # mode 2 use L-bfgs\n",
    "x_pred3 = model3.predict(t_eval)\n",
    "# combining the result from first, second and third stages\n",
    "x_p2 = x_pred + x_pred2 + x_pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ca75ef6-cb9f-4d94-aa88-8898c6726249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Rate: 2400.5 Hz\n",
      "Dominant Frequency: 209.0 Hz\n",
      "kappa4f: 1313.1857292005336\n",
      "Mode: LBFGSIter: 3000, loss: 1.0474e-18\n",
      "Mode: LBFGSIter: 6000, loss: 1.0468e-18\n",
      "Mode: LBFGSIter: 9000, loss: 1.0446e-18\n",
      "Mode: LBFGSIter: 12000, loss: 1.0440e-18\n",
      "Mode: LBFGSIter: 15000, loss: 1.0438e-18\n",
      "Mode: LBFGSIter: 18000, loss: 1.0437e-18\n",
      "Mode: LBFGSIter: 21000, loss: 1.0436e-18\n",
      "Mode: LBFGSIter: 24000, loss: 1.0436e-18\n",
      "Mode: LBFGSIter: 27000, loss: 1.0435e-18\n",
      "Mode: LBFGSIter: 30000, loss: 1.0435e-18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# start the forth stage training\u001b[39;00m\n\u001b[1;32m     43\u001b[0m model4\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m5000\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m \u001b[43mmodel4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m40000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m x_pred4 \u001b[38;5;241m=\u001b[39m model4\u001b[38;5;241m.\u001b[39mpredict(t_eval)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# combining the result from all stages\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 253\u001b[0m, in \u001b[0;36mPhysicsInformedNN.train\u001b[0;34m(self, nIter, idxOpt)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m func_adam\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m idxOpt \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# mode 2: running the Lbfgs optimization\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m     func_bfgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLbfgs_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnIter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m func_bfgs\u001b[38;5;241m.\u001b[39mloss\n",
      "Cell \u001b[0;32mIn[3], line 231\u001b[0m, in \u001b[0;36mPhysicsInformedNN.Lbfgs_optimizer\u001b[0;34m(self, nIter, varlist)\u001b[0m\n\u001b[1;32m    228\u001b[0m max_nIter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(nIter \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# train the model with L-BFGS solver\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlbfgs_minimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_and_gradients_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_nIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# after training, the final optimized parameters are still in results.position\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# so we have to manually put them back to the model\u001b[39;00m\n\u001b[1;32m    237\u001b[0m func\u001b[38;5;241m.\u001b[39massign_new_model_parameters(results\u001b[38;5;241m.\u001b[39mposition)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow_probability/python/optimizer/lbfgs.py:285\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(value_and_gradients_function, initial_position, previous_optimizer_results, num_correction_pairs, tolerance, x_tolerance, f_relative_tolerance, initial_inverse_hessian_estimate, max_iterations, parallel_iterations, stopping_condition, max_line_search_iterations, f_absolute_tolerance, name)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m   initial_state \u001b[38;5;241m=\u001b[39m previous_optimizer_results\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_iterations\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:660\u001b[0m, in \u001b[0;36mdeprecated_arg_values.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m           _PRINTED_WARNING[(func, arg_name)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    653\u001b[0m         _log_deprecation(\n\u001b[1;32m    654\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrom \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwill be removed \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstructions for updating:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min a future version\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m    659\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m date), instructions)\n\u001b[0;32m--> 660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/ops/while_loop.py:241\u001b[0m, in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile_loop\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m     36\u001b[0m \u001b[38;5;129m@deprecation\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated_arg_values(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m                   maximum_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     53\u001b[0m                   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Repeat `body` while the condition `cond` is true.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m  Note: This op is automatically used in a `tf.function` to convert Python for-\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape_invariants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape_invariants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m      \u001b[49m\u001b[43mparallel_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m      \u001b[49m\u001b[43mback_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mback_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m      \u001b[49m\u001b[43mswap_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswap_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m      \u001b[49m\u001b[43mreturn_same_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/ops/while_loop.py:488\u001b[0m, in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m    485\u001b[0m loop_var_structure \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(type_spec\u001b[38;5;241m.\u001b[39mtype_spec_from_value,\n\u001b[1;32m    486\u001b[0m                                         \u001b[38;5;28mlist\u001b[39m(loop_vars))\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cond(\u001b[38;5;241m*\u001b[39mloop_vars):\n\u001b[0;32m--> 488\u001b[0m   loop_vars \u001b[38;5;241m=\u001b[39m \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m try_to_pack \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loop_vars, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    490\u001b[0m     packed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow_probability/python/optimizer/lbfgs.py:253\u001b[0m, in \u001b[0;36mminimize.<locals>._body\u001b[0;34m(current_state)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Main optimization loop.\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m current_state \u001b[38;5;241m=\u001b[39m bfgs_utils\u001b[38;5;241m.\u001b[39mterminate_if_not_finite(current_state)\n\u001b[0;32m--> 253\u001b[0m search_direction \u001b[38;5;241m=\u001b[39m \u001b[43m_get_search_direction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# TODO(b/120134934): Check if the derivative at the start point is not\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# negative, if so then reset position/gradient deltas and recompute\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# search direction.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m next_state \u001b[38;5;241m=\u001b[39m bfgs_utils\u001b[38;5;241m.\u001b[39mline_search_step(\n\u001b[1;32m    260\u001b[0m     current_state, value_and_gradients_function, search_direction,\n\u001b[1;32m    261\u001b[0m     tolerance, f_relative_tolerance, x_tolerance, stopping_condition,\n\u001b[1;32m    262\u001b[0m     max_line_search_iterations, f_absolute_tolerance)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow_probability/python/optimizer/lbfgs.py:394\u001b[0m, in \u001b[0;36m_get_search_direction\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    389\u001b[0m   r_directions \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mscan(\n\u001b[1;32m    390\u001b[0m       second_loop, [alphas, position_deltas, gradient_deltas, inv_rhos],\n\u001b[1;32m    391\u001b[0m       initializer\u001b[38;5;241m=\u001b[39mr_direction)\n\u001b[1;32m    392\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mr_directions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m(\u001b[49m\u001b[43mps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_elements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjective_gradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m               \u001b[49m\u001b[43m_two_loop_algorithm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow_probability/python/internal/prefer_static.py:251\u001b[0m, in \u001b[0;36mcond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m true_fn()\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfalse_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcond(pred\u001b[38;5;241m=\u001b[39mpred, true_fn\u001b[38;5;241m=\u001b[39mtrue_fn, false_fn\u001b[38;5;241m=\u001b[39mfalse_fn, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow_probability/python/optimizer/lbfgs.py:377\u001b[0m, in \u001b[0;36m_get_search_direction.<locals>._two_loop_algorithm\u001b[0;34m()\u001b[0m\n\u001b[1;32m    370\u001b[0m alphas, q_directions \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mscan(\n\u001b[1;32m    371\u001b[0m     first_loop, [position_deltas, gradient_deltas, inv_rhos],\n\u001b[1;32m    372\u001b[0m     initializer\u001b[38;5;241m=\u001b[39m(zero, state\u001b[38;5;241m.\u001b[39mobjective_gradient), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# We use `H^0_k = gamma_k * I` as an estimate for the initial inverse\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# hessian for the k-th iteration; then `r_direction = H^0_k * q_direction`.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m gamma_k \u001b[38;5;241m=\u001b[39m inv_rhos[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(\n\u001b[0;32m--> 377\u001b[0m     gradient_deltas[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[43mgradient_deltas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    378\u001b[0m r_direction \u001b[38;5;241m=\u001b[39m gamma_k[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, tf\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m*\u001b[39m q_directions[\u001b[38;5;241m-\u001b[39mnum_elements]\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msecond_loop\u001b[39m(r_direction, args):\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/ops/tensor_getitem_override.py:230\u001b[0m, in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m begin:\n\u001b[1;32m    228\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops_stack  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m    229\u001b[0m   packed_begin, packed_end, packed_strides \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 230\u001b[0m       \u001b[43marray_ops_stack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    231\u001b[0m       array_ops_stack\u001b[38;5;241m.\u001b[39mstack(end),\n\u001b[1;32m    232\u001b[0m       array_ops_stack\u001b[38;5;241m.\u001b[39mstack(strides))\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;66;03m# TODO(mdan): Instead of implicitly casting, it's better to enforce the\u001b[39;00m\n\u001b[1;32m    234\u001b[0m   \u001b[38;5;66;03m# same dtypes.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (packed_begin\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mint64 \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    236\u001b[0m       packed_end\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mint64 \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    237\u001b[0m       packed_strides\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mint64):\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/ops/array_ops_stack.py:74\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     72\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# If the input is a constant list, it can be converted to a constant op\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Input list contains non-constant tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:713\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    712\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:1294\u001b[0m, in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_autopacking_conversion_function\u001b[39m(v, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, as_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Tensor conversion function that automatically packs arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1294\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m as_ref \u001b[38;5;129;01mor\u001b[39;00m \u001b[43m_should_not_autopack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1296\u001b[0m   inferred_dtype \u001b[38;5;241m=\u001b[39m _get_dtype_from_nested_lists(v)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:1288\u001b[0m, in \u001b[0;36m_should_not_autopack\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_should_not_autopack\u001b[39m(v):\n\u001b[1;32m   1283\u001b[0m   \u001b[38;5;66;03m# The condition we really want is\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m   \u001b[38;5;66;03m#    any(isinstance(elem, core.Tensor))\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m   \u001b[38;5;66;03m# but it is >5x slower due to abc.ABCMeta.__instancecheck__.\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m   \u001b[38;5;66;03m# pylint: disable=unidiomatic-typecheck\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m   \u001b[38;5;66;03m# TODO(slebedev): add nest.all?\u001b[39;00m\n\u001b[0;32m-> 1288\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mtype\u001b[39m(elem) \u001b[38;5;129;01min\u001b[39;00m _NON_AUTOPACKABLE_TYPES \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/util/nest.py:293\u001b[0m, in \u001b[0;36mflatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnest.flatten\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflatten\u001b[39m(structure, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a flat list from a given structure.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    TypeError: The nest is or contains a dict with non-sortable keys.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCORE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:712\u001b[0m, in \u001b[0;36mflatten\u001b[0;34m(modality, structure, expand_composites)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Flattens a nested structure.\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m- For Modality.CORE: refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m  TypeError: The nest is or contains a dict with non-sortable keys.\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mCORE:\n\u001b[0;32m--> 712\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_core_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[1;32m    714\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_flatten(structure)\n",
      "File \u001b[0;32m~/Desktop/Summer Projects/.venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:726\u001b[0m, in \u001b[0;36m_tf_core_flatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    724\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    725\u001b[0m expand_composites \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(expand_composites)\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pywrap_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFlatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Forth stage of training\n",
    "'''\n",
    "# calculate the residue for the forth stage\n",
    "x_train4 = x_train - model.predict(t_train2) - model2.predict(t_train2) - model3.predict(t_train2)\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Calculate dominant frequency\n",
    "g = x_train4.numpy().flatten()\n",
    "N = len(g)\n",
    "# Total time range\n",
    "total_time_range = 2  # from -1 to 1\n",
    "\n",
    "# Calculate the sample rate\n",
    "sample_rate = N / total_time_range\n",
    "print(f\"Sample Rate: {sample_rate} Hz\")\n",
    "\n",
    "# Perform FFT\n",
    "T = 1.0 / sample_rate\n",
    "yf = fft(g)\n",
    "xf = fftfreq(N, T)[:N//2]\n",
    "\n",
    "# Identify the dominant frequency\n",
    "dominant_freq_idx = np.argmax(np.abs(yf[:N//2]))\n",
    "dominant_freq = xf[dominant_freq_idx]\n",
    "magnitude = np.abs(yf[dominant_freq_idx]) / N\n",
    "\n",
    "print(f\"Dominant Frequency: {dominant_freq} Hz\")\n",
    "kappa4f =  2 * np.pi * dominant_freq\n",
    "print(f\"kappa4f: {kappa4f}\")\n",
    "######################################################\n",
    "\n",
    "\n",
    "# get the scale factor approximately by finding the number of zeros of the residues)\n",
    "idxZero = np.where(x_train4[0:-1, 0] * x_train4[1:, 0] < 0)[0]\n",
    "NumZero3 = idxZero.shape[0]\n",
    "kappa4 = 3*NumZero3\n",
    "\n",
    "# (acts = 1 indicates selecting sin as the activation function)\n",
    "model4 = PhysicsInformedNN(t_train2, x_train4, layers2, kappa4f, lt, ut)\n",
    "# start the forth stage training\n",
    "model4.train(5000, 1)\n",
    "model4.train(40000, 2)\n",
    "x_pred4 = model4.predict(t_eval)\n",
    "# combining the result from all stages\n",
    "x_p3 = x_pred + x_pred2 + x_pred3 + x_pred4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f41a2be9-c8b1-4428-b53f-fb627cb2de93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error u: 9.072918e-05\n",
      "Error u: 1.766780e-07\n",
      "Error u: 7.735767e-10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_p3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m error_x3 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(x_eval\u001b[38;5;241m-\u001b[39mx_p2, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(x_eval, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError u: \u001b[39m\u001b[38;5;132;01m%e\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (error_x3))\n\u001b[0;32m---> 16\u001b[0m error_x4 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(x_eval\u001b[38;5;241m-\u001b[39m\u001b[43mx_p3\u001b[49m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(x_eval, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError u: \u001b[39m\u001b[38;5;132;01m%e\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (error_x4))\n\u001b[1;32m     19\u001b[0m mdic \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m: t_eval\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_g\u001b[39m\u001b[38;5;124m\"\u001b[39m: x_eval\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx0\u001b[39m\u001b[38;5;124m\"\u001b[39m: x_pred\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx1\u001b[39m\u001b[38;5;124m\"\u001b[39m: x_pred2\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx2\u001b[39m\u001b[38;5;124m\"\u001b[39m: x_pred3\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx3\u001b[39m\u001b[38;5;124m'\u001b[39m: x_pred4\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merr\u001b[39m\u001b[38;5;124m\"\u001b[39m: residue\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_p3' is not defined"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# combine the loss of all four stages of training\n",
    "loss = np.array(model.loss + model2.loss + model3.loss + model4.loss)\n",
    "\n",
    "residue = x_train4 - model4.predict(t_train2)\n",
    "\n",
    "error_x = np.linalg.norm(x_eval-x_pred, 2)/np.linalg.norm(x_eval, 2)\n",
    "print('Error u: %e' % (error_x))\n",
    "\n",
    "error_x2 = np.linalg.norm(x_eval-x_p, 2)/np.linalg.norm(x_eval, 2)\n",
    "print('Error u: %e' % (error_x2))\n",
    "\n",
    "error_x3 = np.linalg.norm(x_eval-x_p2, 2)/np.linalg.norm(x_eval, 2)\n",
    "print('Error u: %e' % (error_x3))\n",
    "\n",
    "error_x4 = np.linalg.norm(x_eval-x_p3, 2)/np.linalg.norm(x_eval, 2)\n",
    "print('Error u: %e' % (error_x4))\n",
    "\n",
    "mdic = {\"t\": t_eval.numpy(), \"x_g\": x_eval.numpy(), \"x0\": x_pred.numpy(),\n",
    "        \"x1\": x_pred2.numpy(), \"x2\": x_pred3.numpy(), 'x3': x_pred4.numpy(),\n",
    "        \"err\": residue.numpy(), 'loss': loss}\n",
    "FileName = 'Reg_mNN_1D_64bit.mat'\n",
    "savemat(FileName, mdic)\n",
    "\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe4b79a-8857-470f-8c42-49735b0b67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss, 'b-')\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1988f9-21e1-4f7a-8d68-1e5f61be3949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
